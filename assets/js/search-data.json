{
  
    
        "post0": {
            "title": "Variants of Gradient Descent which are useful to know",
            "content": "Sometimes, pure gradient descent can be too slow, or for some other reason it&#39;s not what you need. This post dicusses some alternatives. . First, we&#39;ll make some classification data and run vanilla gradient descent to create a baseline for more exotic variants . from sklearn.datasets import make_circles import autograd.numpy as np import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; plt.style.use([&#39;seaborn-colorblind&#39;, &#39;seaborn-darkgrid&#39;]) np.random.seed(0) NUM_FEATURES=6 NUM_CLASSES=2 NUM_SAMPLES=400 X, y = make_circles(n_samples=NUM_SAMPLES, factor=.3, noise=.05) plt.figure() reds = y == 0 blues = y == 1 plt.scatter(X[reds, 0], X[reds, 1], c=&quot;red&quot;, s=20, edgecolor=&#39;k&#39;) plt.scatter(X[blues, 0], X[blues, 1], c=&quot;blue&quot;, s=20, edgecolor=&#39;k&#39;) . &lt;matplotlib.collections.PathCollection at 0x1148cce48&gt; . This data is not linearly separable, so it&#39;ll be difficult to classify these points using the same method we used last time. No fear though! We can add features to X which will make the data linearly seperable - we&#39;ll transform X into a higher space. You can think of the current data set as points on a hill, and we&#39;re looking down at them. If the blue points are higher than the red, then a plane which slices the hill in half will separate the data. The next function creates the extra columns which define the new space. . def quadratic_kernal(X): &quot;&quot;&quot; Adds quadratic features. This expansion allows your linear model to make non-linear separation. For each sample (row in matrix), compute an expanded row: [feature0, feature1, feature0^2, feature1^2, feature0*feature1, 1] :param X: matrix of features, shape [n_samples,2] :returns: expanded features of shape [n_samples,6] &quot;&quot;&quot; X_expanded = np.zeros((X.shape[0], 6)) # TODO:&lt;your code here&gt; X_0_squared = X[:,0] * X[:, 0] X_1_squared = X[:,1] * X[:, 1] X_10 = X[:, 0] * X[:, 1] X_expanded[:, 0] = X[:, 0] X_expanded[:, 1] = X[:, 1] X_expanded[:, 2] = X_0_squared X_expanded[:, 3] = X_1_squared X_expanded[:, 4] = X_10 X_expanded[:, 5] = np.ones((X.shape[0],)).T return X_expanded . X_kernal = quadratic_kernal(X) . When we classified points in the last post, we used the sigmoid function to create probabioilties. . def sigmoid(x): prob = 1.0 / (1.0 + np.exp(-x)) return prob def predict_prob(w, x): return sigmoid(np.dot(x, w)) def predict(probs): return np.greater(probs, 0.5) . Let&#39;s try it out on some random data, and plot the predictions. . weights = np.random.randn(NUM_FEATURES,) y_probs = predict_prob(weights, X_kernal) y_pred = predict(y_probs) plt.figure() reds = y_pred == 0 blues = y_pred == 1 plt.scatter(X[reds, 0], X[reds, 1], c=&quot;red&quot;, s=20, edgecolor=&#39;k&#39;) plt.scatter(X[blues, 0], X[blues, 1], c=&quot;blue&quot;, s=20, edgecolor=&#39;k&#39;) . &lt;matplotlib.collections.PathCollection at 0x114a935c0&gt; . As you can see, this gets everything wrong! Let&#39;s try a better strategy. . def loss(weights, inputs, targets): num_samples = inputs.shape[0] y_pred = predict_prob(weights, inputs) label_probabilities = y_pred * targets + (1 - y_pred) * (1 - targets) return -np.sum(np.log(label_probabilities)) . loss(weights, X_kernal, y) . 357.50054021310473 . from autograd import grad gradient = grad(loss) def gradient_descent_auto(X, y, cost, num_classes=2, learning_rate=0.001, num_iters=500): from autograd import grad num_samples, num_features = X.shape weights = np.zeros((num_features,)) gradient = grad(cost) yield weights, cost(weights, X, y) for i in range(num_iters): nabla = gradient(weights, X, y) weights = weights - learning_rate * nabla yield weights, cost(weights, X, y) . weights = gradient_descent_auto(X_kernal, y, loss, learning_rate=0.0001, num_classes=NUM_CLASSES) w = list(weights) costs = [x[1] for x in w] . plt.plot(costs) . [&lt;matplotlib.lines.Line2D at 0x1164a2780&gt;] . y_probs = predict_prob(w[-1][0], X_kernal) y_pred = predict(y_probs) plt.figure() reds = y_pred == 0 blues = y_pred == 1 plt.scatter(X[reds, 0], X[reds, 1], c=&quot;red&quot;, s=20, edgecolor=&#39;k&#39;) plt.scatter(X[blues, 0], X[blues, 1], c=&quot;blue&quot;, s=20, edgecolor=&#39;k&#39;) . &lt;matplotlib.collections.PathCollection at 0x11519f080&gt; . Much better! . Gradient descent is taking a lot longer to converge in this setting. Let&#39;s try a different variant of Gradient descent - one with momentum. Momentum is a method that helps accelerate gradient descent in the relevant direction and dampens oscillations as can be seen in image below. It does this by adding a fraction $ alpha$ of the update vector of the past time step to the current update vector. . $$ nu_t = alpha nu_{t-1} + eta nabla_w L(w_t, x_{i_j}, y_{i_j}) $$ $$ w_t = w_{t-1} - nu_t$$ . . def gradient_descent_with_momentum(X, y, cost, num_classes=2, learning_rate=0.001, alpha=0.9, num_iters=500): from autograd import grad num_samples, num_features = X.shape weights = np.zeros((num_features,)) nu = np.zeros_like(weights) gradient = grad(cost) yield weights, cost(weights, X, y) for i in range(num_iters): nabla = gradient(weights, X, y) nu = alpha * nu + learning_rate * nabla weights = weights - nu yield weights, cost(weights, X, y) . weights = gradient_descent_with_momentum(X_kernal, y, loss, learning_rate=0.0001, num_classes=NUM_CLASSES) w = list(weights) costs = [x[1] for x in w] . plt.plot(costs) . [&lt;matplotlib.lines.Line2D at 0x1147e8860&gt;] . y_probs = predict_prob(w[-1][0], X_kernal) y_pred = predict(y_probs) plt.figure() reds = y_pred == 0 blues = y_pred == 1 plt.scatter(X[reds, 0], X[reds, 1], c=&quot;red&quot;, s=20, edgecolor=&#39;k&#39;) plt.scatter(X[blues, 0], X[blues, 1], c=&quot;blue&quot;, s=20, edgecolor=&#39;k&#39;) . &lt;matplotlib.collections.PathCollection at 0x116c33cf8&gt; . As you can see, this algorithm is converging faster than vanilla gradient descent! A final variant of Gradient Descent is RMSProp which uses squared gradients to adjust learning rate: . $$ G_j^t = alpha G_j^{t-1} + (1 - alpha) g_{tj}^2 $$ $$ w_j^t = w_j^{t-1} - dfrac{ eta}{ sqrt{G_j^t + varepsilon}} g_{tj} $$ . def RMSProp(X, y, cost, num_classes=2, learning_rate=0.001, alpha=0.9, num_iters=500): from autograd import grad num_samples, num_features = X.shape weights = np.zeros((num_features,)) g2 = np.zeros_like(weights) eps = 1e-8 gradient = grad(cost) yield weights, cost(weights, X, y) for i in range(num_iters): nabla = gradient(weights, X, y) g2 = alpha * g2 + (1 - alpha) * nabla**2 weights = weights - learning_rate * nabla / np.sqrt(g2 + eps) yield weights, cost(weights, X, y) . weights = RMSProp(X_kernal, y, loss, learning_rate=0.0001, num_classes=NUM_CLASSES) w = list(weights) costs = [x[1] for x in w] . plt.plot(costs) . [&lt;matplotlib.lines.Line2D at 0x116cd32b0&gt;] . y_probs = predict_prob(w[-1][0], X_kernal) y_pred = predict(y_probs) plt.figure() reds = y_pred == 0 blues = y_pred == 1 plt.scatter(X[reds, 0], X[reds, 1], c=&quot;red&quot;, s=20, edgecolor=&#39;k&#39;) plt.scatter(X[blues, 0], X[blues, 1], c=&quot;blue&quot;, s=20, edgecolor=&#39;k&#39;) . &lt;matplotlib.collections.PathCollection at 0x116c89cc0&gt; .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/08/13/Variants-of-Gradient-Descent-That-Are-Useful-To-Know.html",
            "relUrl": "/2021/08/13/Variants-of-Gradient-Descent-That-Are-Useful-To-Know.html",
            "date": " • Aug 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "This is a short post introducing Generalised Additive Models (GAMs) - not the nuts and bolts, but some things you can do with them. We will be follwoing this post: https://petolau.github.io/Analyzing-double-seasonal-time-series-with-GAM-in-R/ but we won&#39;t go so deep into the theory, all the data come from the github repository linked in the post. . GAMs are a very flexible modelling technique, but unfortunately there isn&#39;t a Python package as good as R&#39;s mgcv yet. It&#39;s something we&#39;re working on! In this post, I&#39;ll fit a simple GAM using PyGAM and in a later post I&#39;ll talk about some theory, and some extensions. . GAMs are smooth, semi-parametric models of the form: . $$ y = sum_{i=0}^{n-1} beta_i f_i left(x_i right) $$ . where $y$ is the dependent variable, $x_i$ are the independent variables, $ beta$ are the model coefficients, and $f_i$ are the feature functions. We build the $f_i$ using a type of function called a spline; splines allow us to automatically model non-linear relationships without having to manually try out many different transformations on each variable. . Nedt we&#39;ll load some data and fit a GAM! . import feather from pygam import LinearGAM from pygam.utils import generate_X_grid import matplotlib.pyplot as plt import pandas as pd import numpy as np %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; plt.style.use([&#39;seaborn-colorblind&#39;, &#39;seaborn-darkgrid&#39;]) . def load_data(file): df = feather.read_dataframe(file) weekday_map = {&#39;Monday&#39;:1, &#39;Tuesday&#39;:2, &#39;Wednesday&#39;:3, &#39;Thursday&#39;:4, &#39;Friday&#39;:5, &#39;Saturday&#39;:6, &#39;Sunday&#39;:7} df[&#39;weekday&#39;] = df[&#39;week&#39;].map(weekday_map) n_type = pd.unique(df[&#39;type&#39;]) n_date = pd.unique(df[&#39;date_time&#39;]) n_weekdays = pd.unique(df[&#39;weekday&#39;]) period = 48 begin = &quot;2012-02-27&quot; end = &quot;2012-03-12&quot; mask = (df[&#39;date_time&#39;] &gt; begin) &amp; (df[&#39;date_time&#39;] &lt;= end) data = df.loc[mask] data = data[df[&#39;type&#39;] == n_type[0]] return data data = load_data(&#39;DT_4_ind.dms&#39;) data.plot(x=&#39;date_time&#39;, y=&#39;value&#39;) . /Users/thomas.kealy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Boolean Series key will be reindexed to match DataFrame index. app.launch_new_instance() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11f0bbac8&gt; . The above code loads some data, and does a little bit of preprocessing - makes weekday names more legible to humans, and just selects a few weeks of data about &#39;Commerical Properties&#39;. You can see that the time series has a lot of structure - exhibiting daily, but also weekly periodicity. There are 48 measurements during the day and 7 days during the week so that will be our independent variables to model response variable - electricity load. Let&#39;s build it! . period = 48 N = data.shape[0] # number of observations in the train set window = N / period # number of days in the train set weekly = data[&#39;weekday&#39;] x = np.array(range(1, period+1)) daily = np.tile(x, int(window)) matrix_gam = pd.DataFrame(columns=[&#39;daily&#39;, &#39;weekly&#39;, &#39;load&#39;]) matrix_gam[&#39;load&#39;] = data[&#39;value&#39;] matrix_gam[&#39;daily&#39;] = daily matrix_gam[&#39;weekly&#39;] = weekly . gam = LinearGAM(n_splines=10).gridsearch(matrix_gam[[&#39;daily&#39;, &#39;weekly&#39;]], matrix_gam[&#39;load&#39;]) XX = generate_X_grid(gam) . 100% (11 of 11) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00 . fig, axs = plt.subplots(1, 2) fig.set_figheight(10) fig.set_figwidth(15) titles = [&#39;daily&#39;, &#39;weekly&#39;] for i, ax in enumerate(axs): pdep, confi = gam.partial_dependence(XX, feature=i+1, width=.95) confi = np.asarray(confi) confi = confi.squeeze() ax.plot(XX[:, i], pdep) ax.plot(XX[:, i], confi, c=&#39;r&#39;, ls=&#39;--&#39;) ax.set_title(titles[i]) . This is good! You can see that the electricity load follows an approximate sin pattern during the day, and that the electricity load falls off during the week! If we&#39;d tried using a linear model to do this, we&#39;d have had to build these features manually - the good thing about GAMs is that they do this for us. Let&#39;s visualise the fit. . predictions = gam.predict(matrix_gam[[&#39;daily&#39;, &#39;weekly&#39;]]) . fig = plt.figure(figsize=(15, 10)) plt.plot(data[&#39;date_time&#39;], matrix_gam[&#39;load&#39;]) plt.plot(data[&#39;date_time&#39;], predictions) plt.xticks(rotation=&#39;vertical&#39;) plt.legend([&#39;True&#39;, &#39;Predicted&#39;]) . &lt;matplotlib.legend.Legend at 0x11eb87f28&gt; . Alas, this isn&#39;t the best fit, but it&#39;ll do! .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/08/13/Time-Series-with-GAMS.html",
            "relUrl": "/2021/08/13/Time-Series-with-GAMS.html",
            "date": " • Aug 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "%matplotlib inline import matplotlib as mpl from matplotlib import pylab as plt import matplotlib.dates as mdates import seaborn as sns import numpy as np import pandas as pd import pymc3 as pm import arviz as az from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() sns.set_context(&quot;notebook&quot;, font_scale=1.) sns.set_style(&quot;darkgrid&quot;) . passengers = pd.read_csv(&#39;passengers.csv&#39;, header=0, sep=&#39;;&#39;) passengers[&#39;Passengers&#39;] = passengers[&#39;Passengers&#39;].astype(float) passengers[&#39;Month&#39;] = pd.to_datetime(passengers[&#39;Month&#39;]) passengers.set_index(&#39;Month&#39;, inplace=True) passengers.plot(figsize=(12, 6)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x134a79d50&gt; . passengers[&#39;Passengers&#39;].values[0] . 112.0 . with pm.Model(): delta = pm.GaussianRandomWalk(&#39;delta&#39;, mu=0, sd=1, shape=(144,)) mu = pm.GaussianRandomWalk(&#39;mu&#39;, mu=delta, sd=1, shape=(143,), observed=passengers[&#39;Passengers&#39;]) trace = pm.sample(5000) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [delta] . . 100.00% [24000/24000 00:11&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 20 seconds. . az.plot_trace(trace) . /Users/tomkealy/opt/anaconda3/lib/python3.7/site-packages/arviz/data/io_pymc3.py:91: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context. FutureWarning, /Users/tomkealy/opt/anaconda3/lib/python3.7/site-packages/arviz/plots/traceplot.py:195: UserWarning: rcParams[&#39;plot.max_subplots&#39;] (20) is smaller than the number of variables to plot (144), generating only 20 plots UserWarning, . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x132062690&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x13210eed0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x134d33350&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1321ceed0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x13595a9d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132154a90&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x13229f650&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132384b50&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1323c2f90&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1324bc7d0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1324fcb50&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1325eaed0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x132639bd0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132727cd0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x132773850&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132862d10&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1328a6fd0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x13299d990&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1329e0c10&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132ad8610&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x132b1ad90&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132c08f50&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x132c91a10&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132d57ad0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x132da6690&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132e96b50&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x132ed8f50&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132fcf7d0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x133014b50&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x133101ed0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x13314dbd0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x13323ccd0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1332d0850&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x133395d10&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1333d9fd0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1334cf990&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x133511c10&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x13360c610&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x13364cd90&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x133739f50&gt;]], dtype=object) . def plot_forecast(data_df, col_name, forecast_start, forecast_mean, forecast_scale, forecast_samples, title, x_locator=None, x_formatter=None): &quot;&quot;&quot;Plot a forecast distribution against the &#39;true&#39; time series.&quot;&quot;&quot; colors = sns.color_palette() c1, c2 = colors[0], colors[1] fig = plt.figure(figsize=(12, 6)) ax = fig.add_subplot(1, 1, 1) y = data_df[col_name] x = data_df.index num_steps = data_df.shape[0] num_steps_forecast = forecast_mean.shape[-1] num_steps_train = num_steps - num_steps_forecast ax.plot(x, y, lw=2, color=c1, label=&#39;ground truth&#39;) forecast_steps = data_df.loc[forecast_start:].index ax.plot(forecast_steps, forecast_samples.T, lw=1, color=c2, alpha=0.1) ax.plot(forecast_steps, forecast_mean, lw=2, ls=&#39;--&#39;, color=c2, label=&#39;forecast&#39;) ax.fill_between(forecast_steps, forecast_mean-2*forecast_scale, forecast_mean+2*forecast_scale, color=c2, alpha=0.2) ymin, ymax = min(np.min(forecast_samples), np.min(y)), max(np.max(forecast_samples), np.max(y)) yrange = ymax-ymin ax.set_ylim([ymin - yrange*0.1, ymax + yrange*0.1]) ax.legend() return fig, ax fig, ax = plot_forecast( passengers, &#39;Passengers&#39;, &#39;1959-01-01&#39;, forecast_mean, forecast_scale, forecast_samples, title=&#39;Airplane Passenger Numbers&#39;) ax.legend(loc=&quot;upper left&quot;) ax.set_ylabel(&quot;Passenger Numbers&quot;) ax.set_xlabel(&quot;Month&quot;) fig.autofmt_xdate() . NameError Traceback (most recent call last) &lt;ipython-input-4-d3135643ac66&gt; in &lt;module&gt; 54 &#39;Passengers&#39;, 55 &#39;1959-01-01&#39;, &gt; 56 forecast_mean, 57 forecast_scale, 58 forecast_samples, NameError: name &#39;forecast_mean&#39; is not defined .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/08/13/Structural-Time-Series-in-PyMC3.html",
            "relUrl": "/2021/08/13/Structural-Time-Series-in-PyMC3.html",
            "date": " • Aug 13, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Title",
            "content": "import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd plt.style.use(&#39;ggplot&#39;) . passengers = pd.read_csv(&#39;passengers.csv&#39;, sep=&#39;;&#39;) . passengers[&#39;datetime&#39;] = pd.to_datetime(passengers[&#39;Month&#39;]) . passengers = passengers.set_index(&#39;datetime&#39;) . passengers = passengers.drop(&#39;Month&#39;, axis=1) . with mpl.rc_context(): mpl.rc(&quot;figure&quot;, figsize=(15, 10)) ax = passengers.plot(label=&#39;observed&#39;) . Let&#39;s fit a simple ARIMA model, where we simply chose 1 AR order and 1 MA order. We also include seasonality of 12 months in this regression - but there&#39;s no need to worry about it! . from IPython.core.debugger import set_trace def meboot(x: pd.Series, num_replicates=1): &#39;&#39;&#39; Maximum Entropy Time Series Bootstrap as described in: https://cran.r-project.org/web/packages/meboot/vignettes/meboot.pdf The algorithm (described below) creates an (x.shape[0], num_replicates) DataFrame of replicated time series designed to mimic the properties of the given time series. Bootstrap samples are used to study the relation between the sample and the (unknown) population by a comparible relation between the sample at hand and appropriately designed (observable) resamples. The Maximum Entropy (ME) Bootstrap extends the traditional bootstrap to nonstationary dependent data. Original R source code: https://rdrr.io/cran/meboot/f/ The steps of the algorithm are: 1. Sort the original data in increasing order to create order statistics x_(t) t=1,...,T and store the ordering index vector. 2. Compute intermediate points z_t = (x_(t) + x_(t+1))/2 from the order statistics 3. Compute the trimmed mean, m_trimmed, of the deviations x_t - x_{t-1} among all consecutive observations. Compute the lower limit for the left tail as x_(1) - m_trimmed and the upper limie for the right tail as x_(T) + m_trimmed. The limits become the limiting intermediate points. 4. Compute the mean of the maximum entropy density within each interval such that the &#39;mean preserving constraint&#39; (designed to satify the ergodic theorem) is satisfied. Interval means are denoted m_t. The means for the first and last interval have simpler formulas. 5. Generate random numbers from the [0, 1] uniform interval, and compute sample quantiels of the ME density at those points and sort them. 6. Reorder the sorted sample quantiles by using the ordering index of step 1. This recovers the time dependence relationships of the originally observed data. 7. Repeat steps 2 to 6 num_replicates times. Parameters - x : pd.Series The original Time Series to create replicates for. num_replicates : int The number of replicates to create. Returns -- replicates : pd.DataFrame A (x.shape[0], num_replicates) DataFrame containing the Maximum Entropy replicates of x as columns. Examples -- x = my_series replicates = meboot(x, num_replicates=100) &#39;&#39;&#39; if not isinstance(x, pd.Series): raise TypeError(f&#39;`x` should be a pandas.Series&#39;) # 1 sorted_x = x.sort_values() xt = sorted_x.values # 3 + 2 trimmed_mean = x.diff().abs().mean() zt = np.hstack(( xt[0] - trimmed_mean, (xt[:-1] + xt[1:]) / 2, xt[-1] + trimmed_mean )) # 4 desired_means = np.hstack(( 0.75 * xt[0] + 0.25 * xt[1], 0.25 * xt[:-2] + 0.5 * xt[1:-1] + 0.25 * xt[2:], 0.75 * xt[-1] + 0.25 * xt[-2] )) # 5 xr = np.linspace(0, 1, len(x) + 1) U = np.sort(np.random.rand(num_replicates, len(x))).transpose() inds = np.searchsorted(xr, U, side=&#39;right&#39;) - 1 lin_interp = desired_means[inds] - (zt[inds] - zt[inds + 1]) / 2 y0 = zt[inds] + lin_interp y1 = zt[inds + 1] + lin_interp quantiles = (y0 + ((U - xr[inds]) * (y1 - y0)) / (xr[inds + 1] - xr[inds])) replicates = pd.DataFrame(quantiles, index=sorted_x.index) # 6 return replicates.reindex(x.index) . x = passengers col = &#39;Passengers&#39; replicates = meboot(x[col], num_replicates=100) . replicates.plot(legend=False, figsize=(15, 10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11fd92f28&gt; . replicates[50].plot(legend=False, figsize=(15, 10)) passengers.plot(legend=False, figsize=(15, 10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11fd92d68&gt; .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/08/12/Time-Series-Forecasting-With-SARIMAX.html",
            "relUrl": "/2021/08/12/Time-Series-Forecasting-With-SARIMAX.html",
            "date": " • Aug 12, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Title",
            "content": "This post will explain some of the internals of GAMs: how to estimate the feature functions. First we&#39;ll fit some simple splines on some wage data, then we&#39;ll fit more complicated splines on some accelerometer data, with a highly non-linear realtionship between in the input and the output. . import pandas as pd import patsy import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import PolynomialFeatures import statsmodels.api as sm import statsmodels.formula.api as smf %matplotlib inline . /Users/thomas.kealy/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead. from pandas.core import datetools . GAMs are smooth, semi-parametric models of the form: . $$ y = sum_{i=0}^{n-1} beta_i f_i left(x_i right) $$ . where $y$ is the dependent variable, $x_i$ are the independent variables, $ beta$ are the model coefficients, and $f_i$ are the feature functions. . We build the $f_i$ using a type of function called a spline; splines allow us to automatically model non-linear relationships without having to manually try out many different transformations on each variable. . First of all, we&#39;ll use patsy to construct a few spline bases and fit generalised linear models with statsmodels. Then, we&#39;ll dive into constructing splines ourselves; following Simon Wood&#39;s book we&#39;ll use penalised regression splines. . Firstly, we&#39;ll use patsy to create some basic pline models. The data we&#39;re using comes from https://vincentarelbundock.github.io/Rdatasets/doc/ISLR/Wage.html. It&#39;s plotted below: . df = pd.read_csv(&#39;Wage.csv&#39;) age_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1) plt.scatter(df.age, df.wage, facecolor=&#39;None&#39;, edgecolor=&#39;k&#39;, alpha=0.1) . &lt;matplotlib.collections.PathCollection at 0x11d0a5898&gt; . GAMs are essentially linear models, but in a very special (and useful!) basis made of regression splines. We can use the bs() function in patsy to create such a basis for us: . transformed_x1 = patsy.dmatrix(&quot;bs(df.age, knots=(25,40,60), degree=3, include_intercept=False)&quot;, {&quot;df.age&quot;: df.age}, return_type=&#39;dataframe&#39;) fit1 = sm.GLM(df.wage, transformed_x1).fit() . fit1.params . Intercept 60.493714 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[0] 3.980500 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[1] 44.630980 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[2] 62.838788 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[3] 55.990830 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[4] 50.688098 bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[5] 16.606142 dtype: float64 . age_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1) pred = fit1.predict(patsy.dmatrix(&quot;bs(age_grid, knots=(25,40,60), include_intercept=False)&quot;, {&quot;age_grid&quot;: age_grid}, return_type=&#39;dataframe&#39;)) plt.scatter(df.age, df.wage, facecolor=&#39;None&#39;, edgecolor=&#39;k&#39;, alpha=0.1) plt.plot(age_grid, pred, color=&#39;b&#39;, label=&#39;Specifying three knots&#39;) plt.xlim(15,85) plt.ylim(0,350) plt.xlabel(&#39;age&#39;) plt.ylabel(&#39;wage&#39;) . Text(0,0.5,&#39;wage&#39;) . Here we have prespecified knots at ages 25, 40, and 60. This produces a spline with six basis functions. A cubic spline has 7 degrees of freedom: one for the intercept, and two for each order. We could also have specified knot points at uniform quantiles of the data: . transformed_x2 = patsy.dmatrix(&quot;bs(df.age, df=6, include_intercept=False)&quot;, {&quot;df.age&quot;: df.age}, return_type=&#39;dataframe&#39;) fit2 = sm.GLM(df.wage, transformed_x2).fit() fit2.params . Intercept 56.313841 bs(df.age, df=6, include_intercept=False)[0] 27.824002 bs(df.age, df=6, include_intercept=False)[1] 54.062546 bs(df.age, df=6, include_intercept=False)[2] 65.828391 bs(df.age, df=6, include_intercept=False)[3] 55.812734 bs(df.age, df=6, include_intercept=False)[4] 72.131473 bs(df.age, df=6, include_intercept=False)[5] 14.750876 dtype: float64 . age_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1) pred = fit2.predict(patsy.dmatrix(&quot;bs(age_grid, df=6, include_intercept=False)&quot;, {&quot;age_grid&quot;: age_grid}, return_type=&#39;dataframe&#39;)) plt.scatter(df.age, df.wage, facecolor=&#39;None&#39;, edgecolor=&#39;k&#39;, alpha=0.1) plt.plot(age_grid, pred, color=&#39;b&#39;, label=&#39;Specifying three knots&#39;) plt.xlim(15,85) plt.ylim(0,350) plt.xlabel(&#39;age&#39;) plt.ylabel(&#39;wage&#39;) . Text(0,0.5,&#39;wage&#39;) . Finally, we can also fit natural splines with the cr() function: . transformed_x3 = patsy.dmatrix(&quot;cr(df.age, df=4)&quot;, {&quot;df.age&quot;: df.age}, return_type=&#39;dataframe&#39;) fit3 = sm.GLM(df.wage, transformed_x3).fit() fit3.params . Intercept -6.970341e+13 cr(df.age, df=4)[0] 6.970341e+13 cr(df.age, df=4)[1] 6.970341e+13 cr(df.age, df=4)[2] 6.970341e+13 cr(df.age, df=4)[3] 6.970341e+13 dtype: float64 . pred = fit3.predict(patsy.dmatrix(&quot;cr(age_grid, df=4)&quot;, {&quot;age_grid&quot;: age_grid}, return_type=&#39;dataframe&#39;)) plt.scatter(df.age, df.wage, facecolor=&#39;None&#39;, edgecolor=&#39;k&#39;, alpha=0.1) plt.plot(age_grid, pred, color=&#39;g&#39;, label=&#39;Natural spline df=4&#39;) plt.legend() plt.xlim(15,85) plt.ylim(0,350) plt.xlabel(&#39;age&#39;) plt.ylabel(&#39;wage&#39;) . Text(0,0.5,&#39;wage&#39;) . Let&#39;s see how these fits all stack together: . age_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1) # Make some predictions pred1 = fit1.predict(patsy.dmatrix(&quot;bs(age_grid, knots=(25,40,60), include_intercept=False)&quot;, {&quot;age_grid&quot;: age_grid}, return_type=&#39;dataframe&#39;)) pred2 = fit2.predict(patsy.dmatrix(&quot;bs(age_grid, df=6, include_intercept=False)&quot;, {&quot;age_grid&quot;: age_grid}, return_type=&#39;dataframe&#39;)) pred3 = fit3.predict(patsy.dmatrix(&quot;cr(age_grid, df=4)&quot;, {&quot;age_grid&quot;: age_grid}, return_type=&#39;dataframe&#39;)) # Plot the splines and error bands plt.scatter(df.age, df.wage, facecolor=&#39;None&#39;, edgecolor=&#39;k&#39;, alpha=0.1) plt.plot(age_grid, pred1, color=&#39;b&#39;, label=&#39;Specifying three knots&#39;) plt.plot(age_grid, pred2, color=&#39;r&#39;, label=&#39;Specifying df=6&#39;) plt.plot(age_grid, pred3, color=&#39;g&#39;, label=&#39;Natural spline df=4&#39;) plt.legend() plt.xlim(15,85) plt.ylim(0,350) plt.xlabel(&#39;age&#39;) plt.ylabel(&#39;wage&#39;) . Text(0,0.5,&#39;wage&#39;) . from matplotlib import pyplot as plt import numpy as np import pandas as pd import patsy import scipy as sp import seaborn as sns from statsmodels import api as sm %matplotlib inline . df = pd.read_csv(&#39;mcycle.csv&#39;) df = df.drop(&#39;Unnamed: 0&#39;, axis=1) . fig, ax = plt.subplots(figsize=(8, 6)) blue = sns.color_palette()[0] ax.scatter(df.times, df.accel, c=blue, alpha=0.5) ax.set_xlabel(&#39;time&#39;) ax.set_ylabel(&#39;Acceleration&#39;) . Text(0,0.5,&#39;Acceleration&#39;) . As discussed earlier: GAMs are smooth, semi-parametric models of the form: ​ $$ y = sum_{i=0}^{n-1} beta_i f_i left(x_i right) $$ ​ where $y$ is the dependent variable, $x_i$ are the independent variables, $ beta$ are the model coefficients, and $f_i$ are the feature functions. ​ We build the $f_i$ using a type of function called a spline. Since our data is 1D, we can model it as: . $$ y = beta_0 + f left( x right) + varepsilon $$ . We must also choose a basis for $ f $: . $$ f left( x right) = beta_1 B_1 left(x right) + ldots + beta_k B_k left(x right) $$ . We define . $$ X = left[1, x_1, ldots, x_k right] $$ . so we can write: . $ y = beta_0 + f left( x right) + varepsilon = X beta + varepsilon $$ . We choose to minimise the sum of squares again, this time with a regularisation term: . $$ frac{1}{2} lVert y - X beta rVert + lambda int_0^1 f&#39;&#39; left(x right)^2 dx $$ . You can show (you, not me!) that the second term can always be written: . $$ int_0^1 f&#39;&#39; left(x right)^2 dx = beta^T S beta $$ . where $ S $ is a postive (semi)-definiate matrix (i.e. all it&#39;s eigenvalues are positive or 0). Therefore our objective function becomes: . $$ frac{1}{2} lVert y - X beta rVert + lambda beta^T S beta dx $$ . and we can use the techniques we&#39;ve developed fitting linear models to fit additive models! We&#39;ll start by fitting a univariate spline, then maybe something more complicated. . def R(x, z): return ((z - 0.5)**2 - 1 / 12) * ((x - 0.5)**2 - 1 / 12) / 4 - ((np.abs(x - z) - 0.5)**4 - 0.5 * (np.abs(x - z) - 0.5)**2 + 7 / 240) / 24 R = np.frompyfunc(R, 2, 1) def R_(x): return R.outer(x, knots).astype(np.float64) . q = 20 knots = df.times.quantile(np.linspace(0, 1, q)) . y, X = patsy.dmatrices(&#39;accel ~ times + R_(times)&#39;, data=df) . S = np.zeros((q + 2, q + 2)) S[2:, 2:] = R_(knots) . B = np.zeros_like(S) B[2:, 2:] = np.real_if_close(sp.linalg.sqrtm(S[2:, 2:]), tol=10**8) . /Users/thomas.kealy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: ComplexWarning: Casting complex values to real discards the imaginary part . def fit(y, X, B, lambda_=1.0): # build the augmented matrices y_ = np.vstack((y, np.zeros((q + 2, 1)))) X_ = np.vstack((X, np.sqrt(lambda_) * B)) return sm.OLS(y_, X_).fit() . min_time = df.times.min() max_time = df.times.max() plot_x = np.linspace(min_time, max_time, 100) plot_X = patsy.dmatrix(&#39;times + R_(times)&#39;, {&#39;times&#39;: plot_x}) results = fit(y, X, B) fig, ax = plt.subplots(figsize=(8, 6)) blue = sns.color_palette()[0] ax.scatter(df.times, df.accel, c=blue, alpha=0.5) ax.plot(plot_x, results.predict(plot_X)) ax.set_xlabel(&#39;time&#39;) ax.set_ylabel(&#39;accel&#39;) ax.set_title(r&#39;$ lambda = {}$&#39;.format(1.0)) . Text(0.5,1,&#39;$ lambda = 1.0$&#39;) .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/08/10/The-Guts-Of-GAMs.html",
            "relUrl": "/2021/08/10/The-Guts-Of-GAMs.html",
            "date": " • Aug 10, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Title",
            "content": "It&#39;s useful sometimes to write simple versions of complex things, so that you understand them. In this post we write a simple neural network from scratch. . In a normal classification problem, we have some labels (y) and inputs (x) and we would like to learn a linear function . $$ y = W x $$ . to separate the classes. Neural networks add an (or many!) extra layer . $$ h = mathrm{sigmoid}(M x) $$ . between the inputs and output so that it produces is . $$ y = W h $$ . Thus we are esentially fitting a linear classifier on the basis expansion ( mathrm{sigmoid}(M x)), the difference being that w efit the basis expansion, as well as the linear classifier. That is the Network learns a data dependent basis on which to clssify. . Enough with the maths, lets do some coding. . from __future__ import print_function import numpy as np np.random.seed(42) . Neural networks are made up of Layers, the simplest just returns what it recieves as input. . class Layer(object): &quot;&quot;&quot; A building block. Each layer is capable of performing two things: - Process input to get output: output = layer.forward(input) - Propagate gradients through itself: grad_input = layer.backward(input, grad_output) Some layers also have learnable parameters which they update during layer.backward. &quot;&quot;&quot; def __init__(self): &quot;&quot;&quot;This is an identity layer so it doesn&#39;t need to do anything.&quot;&quot;&quot; pass def forward(self, input): &quot;&quot;&quot; Parameters - input : Tensor of shape [batch_size, num_input_units] Returns - output: Tensor of shape [batch_size, num_output_units] &quot;&quot;&quot; return input def backward(self, input, grad_output): &quot;&quot;&quot; Performs a backpropagation step through the layer, with respect to the given input. Parameters - input : Tensor of shape [batch_size, num_input_units] grad_output : Tensor of shape [batch_size, num_input_units] Returns - grad_output : Tensor of shape [batch_size, num_output_units] &quot;&quot;&quot; num_units = input.shape[1] d_layer_d_input = np.eye(num_units) return np.dot(grad_output, d_layer_d_input) # chain rule . Lets add some non-linearity layers: a ReLU layer, and a Sigmoid layer . class ReLU(Layer): def __init__(self): &quot;&quot;&quot;ReLU layer simply applies elementwise rectified linear unit to all inputs&quot;&quot;&quot; pass def forward(self, input): &quot;&quot;&quot; Apply elementwise ReLU to [batch, input_units] matrix Parameters - input : Tensor of shape [batch_size, num_input_units] Returns - output: Tensor of shape [batch_size, num_output_units] &quot;&quot;&quot; return np.maximum(0, input) def backward(self, input, grad_output): &quot;&quot;&quot; Compute gradient of loss w.r.t. ReLU input Parameters - input : Tensor of shape [batch_size, num_input_units] grad_output : Tensor of shape [batch_size, num_input_units] Returns - grad_output : Tensor of shape [batch_size, num_output_units] &quot;&quot;&quot; relu_grad = input &gt; 0 return grad_output*relu_grad . class Sigmoid(Layer): def __init__(self): &quot;&quot;&quot;Sigmoid layer simply applies elementwise sigmoid unit to all inputs&quot;&quot;&quot; pass def forward(self, input): &quot;&quot;&quot; Apply elementwise ReLU to [batch, input_units] matrix Parameters - input : Tensor of shape [batch_size, num_input_units] Returns - output: Tensor of shape [batch_size, num_output_units] &quot;&quot;&quot; return np.tanh(input) def backward(self, input, grad_output): &quot;&quot;&quot; Compute gradient of loss w.r.t. ReLU input Parameters - input : Tensor of shape [batch_size, num_input_units] grad_output : Tensor of shape [batch_size, num_input_units] Returns - grad_output : Tensor of shape [batch_size, num_output_units] &quot;&quot;&quot; sigmoid_grad = 1 - np.tanh(input)*np.tanh(input) return grad_output*sigmoid_grad . We can test this by evaluating the numerical gradients: . def eval_numerical_gradient(f, x, verbose=False, h=0.00001): &quot;&quot;&quot;Evaluates gradient df/dx via finite differences: df/dx ~ (f(x+h) - f(x-h)) / 2h Adopted from https://github.com/ddtm/dl-course/ (our ysda course). &quot;&quot;&quot; fx = f(x) # evaluate function value at original point grad = np.zeros_like(x) # iterate over all indexes in x it = np.nditer(x, flags=[&#39;multi_index&#39;], op_flags=[&#39;readwrite&#39;]) while not it.finished: # evaluate function at x+h ix = it.multi_index oldval = x[ix] x[ix] = oldval + h # increment by h fxph = f(x) # evalute f(x + h) x[ix] = oldval - h fxmh = f(x) # evaluate f(x - h) x[ix] = oldval # restore # compute the partial derivative with centered formula grad[ix] = (fxph - fxmh) / (2 * h) # the slope if verbose: print (ix, grad[ix]) it.iternext() # step to next dimension return grad . x = np.linspace(-1,1,10*32).reshape([10,32]) l = ReLU() grads = l.backward(x,np.ones([10,32])/(32*10)) numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).mean(), x=x) assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0), &quot;gradient returned by your layer does not match the numerically computed gradient&quot; . x = np.linspace(-1,1,10*32).reshape([10,32]) l = Sigmoid() grads = l.backward(x,np.ones([10,32])/(32*10)) numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).mean(), x=x) assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0), &quot;gradient returned by your layer does not match the numerically computed gradient&quot; . The next type of layer we will implement will be a Dense or Fully Connected layer. Unlike nonlinearity, this layer actually has something to learn. . A dense layer applies affine transformation. In a vectorized form, it can be described as: $$f(X)= W cdot X + vec b $$ . Where . X is an object-feature matrix of shape [batch_size, num_features], | W is a weight matrix [num_features, num_outputs] | and b is a vector of num_outputs biases. | . Both W and b are initialized during layer creation and updated each time backward is called. . class Dense(Layer): def __init__(self, input_units, output_units, learning_rate=0.1): &quot;&quot;&quot; A dense layer is a layer which performs a learned affine transformation: f(x) = &lt;W*x&gt; + b Weights initialised by Xavier initialisation: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf &quot;&quot;&quot; self.learning_rate = learning_rate self.weights = np.random.randn(input_units, output_units) * np.sqrt(2.0/(input_units+output_units)) self.biases = np.zeros(output_units) def forward(self,input): &quot;&quot;&quot; Perform an affine transformation: f(x) = &lt;W*x&gt; + b Parameters - input : Tensor of shape [batch_size, num_input_units] Returns - output: Tensor of shape [batch_size, num_output_units] &quot;&quot;&quot; return input @ self.weights + self.biases def backward(self, input, grad_output): &quot;&quot;&quot; Parameters - input : Tensor of shape [batch_size, num_input_units] Returns - grad_output: Tensor of shape [batch_size, num_output_units] &quot;&quot;&quot; grad_input = grad_output @ self.weights.T grad_weights = (input.T @ grad_output) grad_biases = grad_output.sum(axis=0) assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape self.weights = self.weights - self.learning_rate * grad_weights self.biases = self.biases - self.learning_rate * grad_biases return grad_input . Next, some tests: . l = Dense(128, 150) assert -0.05 &lt; l.weights.mean() &lt; 0.05 and 1e-3 &lt; l.weights.std() &lt; 1e-1, &quot;The initial weights must have zero mean and small variance. &quot; &quot;If you know what you&#39;re doing, remove this assertion.&quot; assert -0.05 &lt; l.biases.mean() &lt; 0.05, &quot;Biases must be zero mean. Ignore if you have a reason to do otherwise.&quot; # To test the outputs, we explicitly set weights with fixed values. DO NOT DO THAT IN ACTUAL NETWORK! l = Dense(3,4) x = np.linspace(-1,1,2*3).reshape([2,3]) l.weights = np.linspace(-1,1,3*4).reshape([3,4]) l.biases = np.linspace(-1,1,4) assert np.allclose(l.forward(x),np.array([[ 0.07272727, 0.41212121, 0.75151515, 1.09090909], [-0.90909091, 0.08484848, 1.07878788, 2.07272727]])) . x = np.linspace(-1,1,10*32).reshape([10,32]) l = Dense(32,64,learning_rate=0) numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).sum(),x) grads = l.backward(x,np.ones([10,64])) assert np.allclose(grads,numeric_grads,rtol=1e-3,atol=0), &quot;input gradient does not match numeric grad&quot; . def compute_out_given_wb(w,b): l = Dense(32,64,learning_rate=1) l.weights = np.array(w) l.biases = np.array(b) x = np.linspace(-1,1,10*32).reshape([10,32]) return l.forward(x) def compute_grad_by_params(w,b): l = Dense(32,64,learning_rate=1) l.weights = np.array(w) l.biases = np.array(b) x = np.linspace(-1,1,10*32).reshape([10,32]) l.backward(x,np.ones([10,64]) / 10.) return w - l.weights, b - l.biases w,b = np.random.randn(32,64), np.linspace(-1,1,64) numeric_dw = eval_numerical_gradient(lambda w: compute_out_given_wb(w,b).mean(0).sum(),w ) numeric_db = eval_numerical_gradient(lambda b: compute_out_given_wb(w,b).mean(0).sum(),b ) grad_w,grad_b = compute_grad_by_params(w,b) assert np.allclose(numeric_dw,grad_w,rtol=1e-3,atol=0), &quot;weight gradient does not match numeric weight gradient&quot; assert np.allclose(numeric_db,grad_b,rtol=1e-3,atol=0), &quot;weight gradient does not match numeric weight gradient&quot; . We will optimise the following loss, which is a more numerically stable version of logg loss (courtesy of Coursera advanced ML): . def softmax_crossentropy_with_logits(logits, reference_answers): &quot;&quot;&quot;Compute crossentropy from logits[batch,n_classes] and ids of correct answers&quot;&quot;&quot; logits_for_answers = logits[np.arange(len(logits)),reference_answers] xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1)) return xentropy def grad_softmax_crossentropy_with_logits(logits, reference_answers): &quot;&quot;&quot;Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers&quot;&quot;&quot; ones_for_answers = np.zeros_like(logits) ones_for_answers[np.arange(len(logits)),reference_answers] = 1 softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True) return (- ones_for_answers + softmax) / logits.shape[0] . logits = np.linspace(-1,1,500).reshape([50,10]) answers = np.arange(50)%10 softmax_crossentropy_with_logits(logits,answers) grads = grad_softmax_crossentropy_with_logits(logits,answers) numeric_grads = eval_numerical_gradient(lambda l: softmax_crossentropy_with_logits(l,answers).mean(),logits) assert np.allclose(numeric_grads,grads,rtol=1e-3,atol=0) . We&#39;ll use the following function to load the mnist dataset: . def load_dataset(flatten=False): import keras (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data() # normalize x X_train = X_train.astype(float) / 255. X_test = X_test.astype(float) / 255. # we reserve the last 10000 training examples for validation X_train, X_val = X_train[:-10000], X_train[-10000:] y_train, y_val = y_train[:-10000], y_train[-10000:] if flatten: X_train = X_train.reshape([X_train.shape[0], -1]) X_val = X_val.reshape([X_val.shape[0], -1]) X_test = X_test.reshape([X_test.shape[0], -1]) return X_train, y_train, X_val, y_val, X_test, y_test . X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True) . /Users/thomas.kealy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Using TensorFlow backend. . class Network(object): def __init__(self): network = [] network.append(Dense(X_train.shape[1],100)) network.append(ReLU()) network.append(Dense(100,200)) network.append(ReLU()) network.append(Dense(200,10)) self.network = network def forward(self, X): &quot;&quot;&quot; comppute activations of all network layers by applying them sequentially. Return a list of activations for each layer. Make sure last activation corresponds to network logits. &quot;&quot;&quot; activations = [] input = X for layer in self.network: activations.append(layer.forward(input)) input = activations[-1] assert len(activations) == len(self.network) return activations def predict(self,X): &quot;&quot;&quot; Compute network predictions. &quot;&quot;&quot; logits = self.forward(X)[-1] return logits.argmax(axis=-1) def fit(self,X,y): &quot;&quot;&quot; Train your network on a given batch of X and y. You first need to run forward to get all layer activations. Then you can run layer.backward going from last to first layer. After you called backward for all layers, all Dense layers have already made one gradient step. &quot;&quot;&quot; # Get the layer activations layer_activations = self.forward(X) layer_inputs = [X]+layer_activations logits = layer_activations[-1] # Compute the loss and the initial gradient loss = softmax_crossentropy_with_logits(logits,y) loss_grad = grad_softmax_crossentropy_with_logits(logits,y) for layer_i in range(len(self.network))[::-1]: layer = self.network[layer_i] loss_grad = layer.backward(layer_inputs[layer_i],loss_grad) return np.mean(loss) . Finally, we can train our nework! . from tqdm import trange def iterate_minibatches(inputs, targets, batchsize, shuffle=False): assert len(inputs) == len(targets) if shuffle: indices = np.random.permutation(len(inputs)) for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize): if shuffle: excerpt = indices[start_idx:start_idx + batchsize] else: excerpt = slice(start_idx, start_idx + batchsize) yield inputs[excerpt], targets[excerpt] . from IPython.display import clear_output import matplotlib.pyplot as plt %matplotlib inline network = Network() train_log = [] val_log = [] for epoch in range(25): for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True): network.fit(x_batch,y_batch) train_log.append(np.mean(network.predict(X_train)==y_train)) val_log.append(np.mean(network.predict(X_val)==y_val)) clear_output() print(&quot;Epoch&quot;,epoch) print(&quot;Train accuracy:&quot;,train_log[-1]) print(&quot;Val accuracy:&quot;,val_log[-1]) plt.plot(train_log,label=&#39;train accuracy&#39;) plt.plot(val_log,label=&#39;val accuracy&#39;) plt.legend(loc=&#39;best&#39;) plt.grid() plt.show() . Epoch 24 Train accuracy: 1.0 Val accuracy: 0.9819 .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/06/21/Neural-Netwroks-From-Scratch.html",
            "relUrl": "/2021/06/21/Neural-Netwroks-From-Scratch.html",
            "date": " • Jun 21, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Title",
            "content": "Let $y_t$ denote observation $t$ in a real-valued time series. A structural time series model can be described by a pair of equations relating yt to a vector of latent state variables $u_t$ . $$ y_t = Z^T_tu_{t} + varepsilon_t$$ . $$ u_T = Tu_{t_1} + R eta_t $$ . Where $ varepsilon_t sim N left(0, H_t right) $ and $ eta_t sim N left(0, Q_t right) $. . One usefuel extension of thsi model is the &#39;basic&#39; structural time series, which can be described in the following equations: . $$ y_t = u_{t} + tau_t + beta^T textbf{x} + varepsilon_t$$ . $$ u_T = u_{t_1} + delta_{t-1} + eta_t $$ . $$ delta_t = delta_{t-1} + nu_t $$ . $$ tau_t = - sum_{s=1}^{S-1} tau_{t-s} + w_t $$ . We&#39;ll model this in sections, firsly by excluding the seasonal component. . import pystan import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd plt.style.use(&#39;ggplot&#39;) %matplotlib inline stan_code = &quot;&quot;&quot;data { int &lt;lower=0&gt; T; vector[T] x; vector[T] y; } parameters { vector[T] u_err; //Slope innovation vector[T] v_err; //Level innovation real beta; real &lt;lower=0&gt; s_obs; real &lt;lower=0&gt; s_slope; real &lt;lower=0&gt; s_level; } transformed parameters { vector[T] u; //Level vector[T] v; //Slope u[1] = u_err[1]; v[1] = v_err[1]; for (t in 2:T) { u[t] = u[t-1] + v[t-1] + s_level * u_err[t]; v[t] = v[t-1] + s_slope * v_err[t]; } } model { u_err ~ normal(0,1); v_err ~ normal(0,1); y ~ normal (u + beta*x, s_obs); }&quot;&quot;&quot; . data = pd.read_csv(&#39;unemployment.csv&#39;) . data[&#39;unemployment.office&#39;].plot(figsize=(30, 10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x10ab7c048&gt; . data_feed = {&#39;y&#39;: data[&#39;unemployment.office&#39;].values, &#39;x&#39;: np.zeros((data.shape[0], )), &#39;T&#39;: data.shape[0]} sm = pystan.StanModel(model_code=stan_code) fit = sm.sampling(data=data_feed, iter=1000) . INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_a216d1830c917dccb932a956782a2c63 NOW. /Users/thomas.kealy/anaconda3/lib/python3.7/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. elif np.issubdtype(np.asarray(v).dtype, float): . with mpl.rc_context(): mpl.rc(&#39;figure&#39;, figsize=(30, 10)) fit.plot() . samples = fit.extract(permuted=True) u = samples[&#39;u&#39;].mean(axis=0) v = samples[&#39;v&#39;].mean(axis=0) . data[&#39;pred&#39;] = u + v data[[&#39;unemployment.office&#39;, &#39;pred&#39;]].plot(figsize=(30, 10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a25b3e9b0&gt; . The fit looks ok, but it can definately be improved by adding a seasonal component. .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/06/07/bayesian-structural-time-series-in-pystan.html",
            "relUrl": "/2021/06/07/bayesian-structural-time-series-in-pystan.html",
            "date": " • Jun 7, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Gradient Descent Without Gradients",
            "content": "In the last post I introduced Gradient Descent, and used it to a simple linear regression in 1 dimension. The function that did most of the work was: . def gradient_descent(X, y, cost, learning_rate=0.01, num_iters=250): m, n = X.shape theta = np.zeros((n, 1)) yhat = theta.T @ X.T yield theta, cost(y.reshape(-1, 1), yhat.T) for i in range(num_iters): yhat = theta.T @ X.T yhatt = yhat.T nabla = np.sum(X.T @ (y.reshape(-1, 1) - yhatt), axis=1).reshape(-1, 1) assert(nabla.shape == theta.shape) theta += (2 * learning_rate / m) * nabla yield theta, cost(y.reshape(-1, 1), yhat.T) . However, this function has a drawback - it only works for linear regression. In this post we&#39;ll modify the function to take other losses and perform gradient descent automatically. Let&#39;s first generate some toy data: . import autograd.numpy as np import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; plt.style.use([&#39;seaborn-colorblind&#39;, &#39;seaborn-darkgrid&#39;]) def make_blobs(num_samples=1000, num_features=2, num_classes=2): mu = np.random.rand(num_classes, num_features) sigma = np.ones((num_classes, num_features)) * 0.1 samples_per_class = num_samples // num_classes x = np.zeros((num_samples, num_features)) y = np.zeros((num_samples, num_classes)) for i in range(num_classes): class_samples = np.random.normal(mu[i, :], sigma[i, :], (samples_per_class, num_features)) x[i * samples_per_class:(i+1) * samples_per_class] = class_samples y[i * samples_per_class:(i+1) * samples_per_class, i] = 1 return x, y def plot_clusters(x, y, num_classes=2): temp = np.argmax(y, axis=1) colours = [&#39;r&#39;, &#39;g&#39;, &#39;b&#39;] for i in range(num_classes): x_class = x[temp == i] plt.scatter(x_class[:, 0], x_class[:, 1], color=colours[i], s=1) plt.show() NUM_FEATURES=50 NUM_CLASSES=2 NUM_SAMPLES=1000 X, y, = make_blobs(num_samples=NUM_SAMPLES, num_features=NUM_FEATURES, num_classes=NUM_CLASSES) plot_clusters(X, y, num_classes=NUM_CLASSES) . X . array([[-0.05461888, 0.93787102, -0.09551936, ..., 0.02657511, 0.72509201, 0.68530672], [-0.09366208, 0.7994963 , 0.008594 , ..., 0.12660328, 0.60329672, 0.66772192], [ 0.13367689, 0.81014452, -0.10304008, ..., 0.15666234, 0.71245837, 0.57504779], ..., [ 0.9828324 , 0.82414756, 0.55084061, ..., 0.23759356, 1.06799085, 0.56271078], [ 0.77171086, 0.93970304, 0.37411522, ..., 0.04376771, 0.95470468, 0.43622007], [ 0.8846701 , 0.89806374, 0.35203692, ..., 0.09546763, 1.09566046, 0.5202908 ]]) . We&#39;ll predict the class of each point using softmax (multinomial logistic) regression. The model has a matrix $ W $ of weights, which measures for each feature how likely that feature is to be in a particular. It is of size $ mathrm{n_{features}} times mathrm{n_{classes}} $. The goal of softmax regression is to learn such a matrix. Given a matrix of weights, $ W $, and matrix of points, $ X $, it predicts the probability od each class given the samples . $$ p left( y_i | x_i ; w right) = frac{e^{w_j^T x_i}}{ sum_j e^{w_j^T x_i}}$$ . This prediction is encapsulated in the following function: . def sigmoid(x): prob = 1.0 / (1.0 + np.exp(-x)) return prob def predict(w, x): return sigmoid(np.dot(x, w)) . To get a feel we&#39;ll make a random guess: . weights = np.random.randn(NUM_FEATURES, NUM_CLASSES) y_pred = predict(weights, X) . plot_clusters(X, y_pred, NUM_CLASSES) . As you can see, that looks nothing like the real clusters! . Logistic regression minimises the following loss function: . $$ J left(w right) = y * p left( y_i | x_i ; w right) + (1-y) * (1 - p left( y_i | x_i ; w right)) $$ . There is a mathematical justification for why this is the right loss to use, but heuristically, this loss minimises the probability error between the predicition classes and the true classes. . In python, the loss can be written: . def loss(weights, inputs, targets): num_samples = inputs.shape[0] y_pred = predict(weights, inputs) label_probabilities = y_pred * targets + (1 - y_pred) * (1 - targets) return -np.sum(np.log(label_probabilities)) . loss(weights, X, y) . 4750.573533547572 . We&#39;re now in a position to do gradient descent! . from autograd import grad gradient = grad(loss) . def gradient_descent_auto(X, y, cost, num_classes=2, learning_rate=0.001, num_iters=20): from autograd import grad num_samples, num_features = X.shape weights = np.zeros((num_features, num_classes)) gradient = grad(cost) yield weights, cost(weights, X, y) for i in range(num_iters): nabla = gradient(weights, X, y) weights = weights - learning_rate * nabla yield weights, cost(weights, X, y) . weights = gradient_descent_auto(X, y, loss, learning_rate=0.001, num_classes=NUM_CLASSES) w = list(weights) costs = [x[1] for x in w] . predictions = predict(w[-1][0], X) . plot_clusters(X, predictions, NUM_CLASSES) . plt.plot(costs) . [&lt;matplotlib.lines.Line2D at 0x11e28a9b0&gt;] .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/05/05/gradient-free-gradient-descent.html",
            "relUrl": "/2021/05/05/gradient-free-gradient-descent.html",
            "date": " • May 5, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Title",
            "content": "The last part of moddeling with (univariate spline) GAMs is choosing the smoothing parameter $ lambda $. This post will elaborate on this, using the scikit-learn GriddSearchCV functionality to do this. We&#39;ll use pyGAM to do the heavy lifting, and we&#39;ll use the same data as the last post . from matplotlib import pyplot as plt import numpy as np import pandas as pd import patsy import scipy as sp import seaborn as sns from statsmodels import api as sm %matplotlib inline df = pd.read_csv(&#39;mcycle.csv&#39;) df = df.drop(&#39;Unnamed: 0&#39;, axis=1) min_time = df.times.min() max_time = df.times.max() fig, ax = plt.subplots(figsize=(8, 6)) blue = sns.color_palette()[0] ax.scatter(df.times, df.accel, c=blue, alpha=0.5) ax.set_xlabel(&#39;time&#39;) ax.set_ylabel(&#39;Acceleration&#39;) . Text(0,0.5,&#39;Acceleration&#39;) . def splines(df): def R(x, z): return ((z - 0.5)**2 - 1 / 12) * ((x - 0.5)**2 - 1 / 12) / 4 - ((np.abs(x - z) - 0.5)**4 - 0.5 * (np.abs(x - z) - 0.5)**2 + 7 / 240) / 24 R = np.frompyfunc(R, 2, 1) def R_(x): return R.outer(x, knots).astype(np.float64) y, X = patsy.dmatrices(&#39;accel ~ times + R_(times)&#39;, data=df) knots = df.times.quantile(np.linspace(0, 1, q)) def GAM(df, q=20, gamma=1.0): S = np.zeros((q + 2, q + 2)) S[2:, 2:] = R_(knots) B = np.zeros_like(S) B[2:, 2:] = np.real_if_close(sp.linalg.sqrtm(S[2:, 2:]), tol=10**8) def fit(y, X, B, lambda_=gamma): # build the augmented matrices y_ = np.vstack((y, np.zeros((q + 2, 1)))) X_ = np.vstack((X, np.sqrt(lambda_) * B)) return sm.OLS(y_, X_).fit() return fit(X, y, b, lambda_) . min_time = df.times.min() max_time = df.times.max() plot_x = np.linspace(min_time, max_time, 100) plot_X = patsy.dmatrix(&#39;times + R_(times)&#39;, {&#39;times&#39;: plot_x}) results = GAM(df) fig, ax = plt.subplots(figsize=(8, 6)) blue = sns.color_palette()[0] ax.scatter(df.times, df.accel, c=blue, alpha=0.5) ax.plot(plot_x, results.predict(plot_X)) ax.set_xlabel(&#39;time&#39;) ax.set_ylabel(&#39;accel&#39;) ax.set_title(r&#39;$ lambda = {}$&#39;.format(1.0)) . NameError Traceback (most recent call last) ~/anaconda3/lib/python3.6/site-packages/patsy/compat.py in call_and_wrap_exc(msg, origin, f, *args, **kwargs) 35 try: &gt; 36 return f(*args, **kwargs) 37 except Exception as e: ~/anaconda3/lib/python3.6/site-packages/patsy/eval.py in eval(self, expr, source_name, inner_namespace) 165 return eval(code, {}, VarLookupDict([inner_namespace] --&gt; 166 + self._namespaces)) 167 &lt;string&gt; in &lt;module&gt;() NameError: name &#39;R_&#39; is not defined The above exception was the direct cause of the following exception: PatsyError Traceback (most recent call last) &lt;ipython-input-3-9eb1425fd693&gt; in &lt;module&gt;() 3 4 plot_x = np.linspace(min_time, max_time, 100) -&gt; 5 plot_X = patsy.dmatrix(&#39;times + R_(times)&#39;, {&#39;times&#39;: plot_x}) 6 7 results = GAM(df) ~/anaconda3/lib/python3.6/site-packages/patsy/highlevel.py in dmatrix(formula_like, data, eval_env, NA_action, return_type) 289 eval_env = EvalEnvironment.capture(eval_env, reference=1) 290 (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env, --&gt; 291 NA_action, return_type) 292 if lhs.shape[1] != 0: 293 raise PatsyError(&#34;encountered outcome variables for a model &#34; ~/anaconda3/lib/python3.6/site-packages/patsy/highlevel.py in _do_highlevel_design(formula_like, data, eval_env, NA_action, return_type) 163 return iter([data]) 164 design_infos = _try_incr_builders(formula_like, data_iter_maker, eval_env, --&gt; 165 NA_action) 166 if design_infos is not None: 167 return build_design_matrices(design_infos, data, ~/anaconda3/lib/python3.6/site-packages/patsy/highlevel.py in _try_incr_builders(formula_like, data_iter_maker, eval_env, NA_action) 68 data_iter_maker, 69 eval_env, &gt; 70 NA_action) 71 else: 72 return None ~/anaconda3/lib/python3.6/site-packages/patsy/build.py in design_matrix_builders(termlists, data_iter_maker, eval_env, NA_action) 694 factor_states, 695 data_iter_maker, --&gt; 696 NA_action) 697 # Now we need the factor infos, which encapsulate the knowledge of 698 # how to turn any given factor into a chunk of data: ~/anaconda3/lib/python3.6/site-packages/patsy/build.py in _examine_factor_types(factors, factor_states, data_iter_maker, NA_action) 441 for data in data_iter_maker(): 442 for factor in list(examine_needed): --&gt; 443 value = factor.eval(factor_states[factor], data) 444 if factor in cat_sniffers or guess_categorical(value): 445 if factor not in cat_sniffers: ~/anaconda3/lib/python3.6/site-packages/patsy/eval.py in eval(self, memorize_state, data) 564 return self._eval(memorize_state[&#34;eval_code&#34;], 565 memorize_state, --&gt; 566 data) 567 568 __getstate__ = no_pickling ~/anaconda3/lib/python3.6/site-packages/patsy/eval.py in _eval(self, code, memorize_state, data) 549 memorize_state[&#34;eval_env&#34;].eval, 550 code, --&gt; 551 inner_namespace=inner_namespace) 552 553 def memorize_chunk(self, state, which_pass, data): ~/anaconda3/lib/python3.6/site-packages/patsy/compat.py in call_and_wrap_exc(msg, origin, f, *args, **kwargs) 41 origin) 42 # Use &#39;exec&#39; to hide this syntax from the Python 2 parser: &gt; 43 exec(&#34;raise new_exc from e&#34;) 44 else: 45 # In python 2, we just let the original exception escape -- better ~/anaconda3/lib/python3.6/site-packages/patsy/compat.py in &lt;module&gt;() PatsyError: Error evaluating factor: NameError: name &#39;R_&#39; is not defined times + R_(times) ^^^^^^^^^ .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/04/08/Model-Selection-wiht-GAMs.html",
            "relUrl": "/2021/04/08/Model-Selection-wiht-GAMs.html",
            "date": " • Apr 8, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Pipelines and Pandas",
            "content": "import pandas as pd import numpy as np from sklearn.base import BaseEstimator, TransformerMixin . ames = pd.read_csv(&#39;ames.csv&#39;) . ames.head(10) . Order PID MS.SubClass MS.Zoning Lot.Frontage Lot.Area Street Alley Lot.Shape Land.Contour ... Pool.Area Pool.QC Fence Misc.Feature Misc.Val Mo.Sold Yr.Sold Sale.Type Sale.Condition SalePrice . 0 1 | 526301100 | 20 | RL | 141.0 | 31770 | Pave | NaN | IR1 | Lvl | ... | 0 | NaN | NaN | NaN | 0 | 5 | 2010 | WD | Normal | 215000 | . 1 2 | 526350040 | 20 | RH | 80.0 | 11622 | Pave | NaN | Reg | Lvl | ... | 0 | NaN | MnPrv | NaN | 0 | 6 | 2010 | WD | Normal | 105000 | . 2 3 | 526351010 | 20 | RL | 81.0 | 14267 | Pave | NaN | IR1 | Lvl | ... | 0 | NaN | NaN | Gar2 | 12500 | 6 | 2010 | WD | Normal | 172000 | . 3 4 | 526353030 | 20 | RL | 93.0 | 11160 | Pave | NaN | Reg | Lvl | ... | 0 | NaN | NaN | NaN | 0 | 4 | 2010 | WD | Normal | 244000 | . 4 5 | 527105010 | 60 | RL | 74.0 | 13830 | Pave | NaN | IR1 | Lvl | ... | 0 | NaN | MnPrv | NaN | 0 | 3 | 2010 | WD | Normal | 189900 | . 5 6 | 527105030 | 60 | RL | 78.0 | 9978 | Pave | NaN | IR1 | Lvl | ... | 0 | NaN | NaN | NaN | 0 | 6 | 2010 | WD | Normal | 195500 | . 6 7 | 527127150 | 120 | RL | 41.0 | 4920 | Pave | NaN | Reg | Lvl | ... | 0 | NaN | NaN | NaN | 0 | 4 | 2010 | WD | Normal | 213500 | . 7 8 | 527145080 | 120 | RL | 43.0 | 5005 | Pave | NaN | IR1 | HLS | ... | 0 | NaN | NaN | NaN | 0 | 1 | 2010 | WD | Normal | 191500 | . 8 9 | 527146030 | 120 | RL | 39.0 | 5389 | Pave | NaN | IR1 | Lvl | ... | 0 | NaN | NaN | NaN | 0 | 3 | 2010 | WD | Normal | 236500 | . 9 10 | 527162130 | 60 | RL | 60.0 | 7500 | Pave | NaN | Reg | Lvl | ... | 0 | NaN | NaN | NaN | 0 | 6 | 2010 | WD | Normal | 189000 | . 10 rows × 82 columns . It&#39;s always a good idea to set an index on a DataFrame if you have one. In this case, the PID column is a unique identifier. . ames = ames.set_index(&#39;PID&#39;).copy() . ames.describe().T . count mean std min 25% 50% 75% max . Order 2930.0 | 1465.500000 | 845.962470 | 1.0 | 733.25 | 1465.5 | 2197.75 | 2930.0 | . MS.SubClass 2930.0 | 57.387372 | 42.638025 | 20.0 | 20.00 | 50.0 | 70.00 | 190.0 | . Lot.Frontage 2440.0 | 69.224590 | 23.365335 | 21.0 | 58.00 | 68.0 | 80.00 | 313.0 | . Lot.Area 2930.0 | 10147.921843 | 7880.017759 | 1300.0 | 7440.25 | 9436.5 | 11555.25 | 215245.0 | . Overall.Qual 2930.0 | 6.094881 | 1.411026 | 1.0 | 5.00 | 6.0 | 7.00 | 10.0 | . Overall.Cond 2930.0 | 5.563140 | 1.111537 | 1.0 | 5.00 | 5.0 | 6.00 | 9.0 | . Year.Built 2930.0 | 1971.356314 | 30.245361 | 1872.0 | 1954.00 | 1973.0 | 2001.00 | 2010.0 | . Year.Remod.Add 2930.0 | 1984.266553 | 20.860286 | 1950.0 | 1965.00 | 1993.0 | 2004.00 | 2010.0 | . Mas.Vnr.Area 2907.0 | 101.896801 | 179.112611 | 0.0 | 0.00 | 0.0 | 164.00 | 1600.0 | . BsmtFin.SF.1 2929.0 | 442.629566 | 455.590839 | 0.0 | 0.00 | 370.0 | 734.00 | 5644.0 | . BsmtFin.SF.2 2929.0 | 49.722431 | 169.168476 | 0.0 | 0.00 | 0.0 | 0.00 | 1526.0 | . Bsmt.Unf.SF 2929.0 | 559.262547 | 439.494153 | 0.0 | 219.00 | 466.0 | 802.00 | 2336.0 | . Total.Bsmt.SF 2929.0 | 1051.614544 | 440.615067 | 0.0 | 793.00 | 990.0 | 1302.00 | 6110.0 | . X1st.Flr.SF 2930.0 | 1159.557679 | 391.890885 | 334.0 | 876.25 | 1084.0 | 1384.00 | 5095.0 | . X2nd.Flr.SF 2930.0 | 335.455973 | 428.395715 | 0.0 | 0.00 | 0.0 | 703.75 | 2065.0 | . Low.Qual.Fin.SF 2930.0 | 4.676792 | 46.310510 | 0.0 | 0.00 | 0.0 | 0.00 | 1064.0 | . Gr.Liv.Area 2930.0 | 1499.690444 | 505.508887 | 334.0 | 1126.00 | 1442.0 | 1742.75 | 5642.0 | . Bsmt.Full.Bath 2928.0 | 0.431352 | 0.524820 | 0.0 | 0.00 | 0.0 | 1.00 | 3.0 | . Bsmt.Half.Bath 2928.0 | 0.061134 | 0.245254 | 0.0 | 0.00 | 0.0 | 0.00 | 2.0 | . Full.Bath 2930.0 | 1.566553 | 0.552941 | 0.0 | 1.00 | 2.0 | 2.00 | 4.0 | . Half.Bath 2930.0 | 0.379522 | 0.502629 | 0.0 | 0.00 | 0.0 | 1.00 | 2.0 | . Bedroom.AbvGr 2930.0 | 2.854266 | 0.827731 | 0.0 | 2.00 | 3.0 | 3.00 | 8.0 | . Kitchen.AbvGr 2930.0 | 1.044369 | 0.214076 | 0.0 | 1.00 | 1.0 | 1.00 | 3.0 | . TotRms.AbvGrd 2930.0 | 6.443003 | 1.572964 | 2.0 | 5.00 | 6.0 | 7.00 | 15.0 | . Fireplaces 2930.0 | 0.599317 | 0.647921 | 0.0 | 0.00 | 1.0 | 1.00 | 4.0 | . Garage.Yr.Blt 2771.0 | 1978.132443 | 25.528411 | 1895.0 | 1960.00 | 1979.0 | 2002.00 | 2207.0 | . Garage.Cars 2929.0 | 1.766815 | 0.760566 | 0.0 | 1.00 | 2.0 | 2.00 | 5.0 | . Garage.Area 2929.0 | 472.819734 | 215.046549 | 0.0 | 320.00 | 480.0 | 576.00 | 1488.0 | . Wood.Deck.SF 2930.0 | 93.751877 | 126.361562 | 0.0 | 0.00 | 0.0 | 168.00 | 1424.0 | . Open.Porch.SF 2930.0 | 47.533447 | 67.483400 | 0.0 | 0.00 | 27.0 | 70.00 | 742.0 | . Enclosed.Porch 2930.0 | 23.011604 | 64.139059 | 0.0 | 0.00 | 0.0 | 0.00 | 1012.0 | . X3Ssn.Porch 2930.0 | 2.592491 | 25.141331 | 0.0 | 0.00 | 0.0 | 0.00 | 508.0 | . Screen.Porch 2930.0 | 16.002048 | 56.087370 | 0.0 | 0.00 | 0.0 | 0.00 | 576.0 | . Pool.Area 2930.0 | 2.243345 | 35.597181 | 0.0 | 0.00 | 0.0 | 0.00 | 800.0 | . Misc.Val 2930.0 | 50.635154 | 566.344288 | 0.0 | 0.00 | 0.0 | 0.00 | 17000.0 | . Mo.Sold 2930.0 | 6.216041 | 2.714492 | 1.0 | 4.00 | 6.0 | 8.00 | 12.0 | . Yr.Sold 2930.0 | 2007.790444 | 1.316613 | 2006.0 | 2007.00 | 2008.0 | 2009.00 | 2010.0 | . SalePrice 2930.0 | 180796.060068 | 79886.692357 | 12789.0 | 129500.00 | 160000.0 | 213500.00 | 755000.0 | . The main comment is that there&#39;s a lof of missing data, and that some columns should be dropped entirely (in particular Alley, Pool QC, Fence, Misc Feature, Fireplace QC) - or the dataset documentation needs to be checked to see that N/A isn&#39;t a default category in these cases. There&#39;s also a mixture of categorical and numerical features, which is a little tricky to handle. Luckily sklearn has the FeatureUnion and Pipeline objects to help us. . ames.isnull().sum() . Order 0 MS.SubClass 0 MS.Zoning 0 Lot.Frontage 490 Lot.Area 0 Street 0 Alley 2732 Lot.Shape 0 Land.Contour 0 Utilities 0 Lot.Config 0 Land.Slope 0 Neighborhood 0 Condition.1 0 Condition.2 0 Bldg.Type 0 House.Style 0 Overall.Qual 0 Overall.Cond 0 Year.Built 0 Year.Remod.Add 0 Roof.Style 0 Roof.Matl 0 Exterior.1st 0 Exterior.2nd 0 Mas.Vnr.Type 23 Mas.Vnr.Area 23 Exter.Qual 0 Exter.Cond 0 Foundation 0 ... Bedroom.AbvGr 0 Kitchen.AbvGr 0 Kitchen.Qual 0 TotRms.AbvGrd 0 Functional 0 Fireplaces 0 Fireplace.Qu 1422 Garage.Type 157 Garage.Yr.Blt 159 Garage.Finish 159 Garage.Cars 1 Garage.Area 1 Garage.Qual 159 Garage.Cond 159 Paved.Drive 0 Wood.Deck.SF 0 Open.Porch.SF 0 Enclosed.Porch 0 X3Ssn.Porch 0 Screen.Porch 0 Pool.Area 0 Pool.QC 2917 Fence 2358 Misc.Feature 2824 Misc.Val 0 Mo.Sold 0 Yr.Sold 0 Sale.Type 0 Sale.Condition 0 SalePrice 0 Length: 81, dtype: int64 . First of all, let&#39;s build a transformer which drops the columns we suggest. A sklearn compatible transformer is a class which has to have two methods fit (which returns self), and transform (which can return whatever you want). It&#39;s a good idea to inherit from sklearn.base.TransformerMixin and sklearn.base.BaseEstimator. The general pattern of a Transformer is: . from sklearn.base import BaseEstimator, TransformerMixin class ExampleTransformer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y=None): return self def transform(self, X): return do_something_to(self, X) . These are actually quite simple, and can be quite flexible. Later on, we might see a transformer that uses the fit method. Anyway, here&#39;s a column dropper. . from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.linear_model import LarsCV from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, mean_squared_log_error . class ColumnDropper(BaseEstimator, TransformerMixin): &#39;&#39;&#39; Transformer to drop a list of cols &#39;&#39;&#39; def __init__(self, drop_cols): self._drop_cols = drop_cols def fit(self, X, y=None): return self def transform(self, X): df = X.copy() df = df.drop(self._drop_cols, axis=1) return df . y = ames[&#39;SalePrice&#39;].copy() X = ames.drop(&#39;SalePrice&#39;, axis=1).copy() X.columns . Index([&#39;Order&#39;, &#39;MS.SubClass&#39;, &#39;MS.Zoning&#39;, &#39;Lot.Frontage&#39;, &#39;Lot.Area&#39;, &#39;Street&#39;, &#39;Alley&#39;, &#39;Lot.Shape&#39;, &#39;Land.Contour&#39;, &#39;Utilities&#39;, &#39;Lot.Config&#39;, &#39;Land.Slope&#39;, &#39;Neighborhood&#39;, &#39;Condition.1&#39;, &#39;Condition.2&#39;, &#39;Bldg.Type&#39;, &#39;House.Style&#39;, &#39;Overall.Qual&#39;, &#39;Overall.Cond&#39;, &#39;Year.Built&#39;, &#39;Year.Remod.Add&#39;, &#39;Roof.Style&#39;, &#39;Roof.Matl&#39;, &#39;Exterior.1st&#39;, &#39;Exterior.2nd&#39;, &#39;Mas.Vnr.Type&#39;, &#39;Mas.Vnr.Area&#39;, &#39;Exter.Qual&#39;, &#39;Exter.Cond&#39;, &#39;Foundation&#39;, &#39;Bsmt.Qual&#39;, &#39;Bsmt.Cond&#39;, &#39;Bsmt.Exposure&#39;, &#39;BsmtFin.Type.1&#39;, &#39;BsmtFin.SF.1&#39;, &#39;BsmtFin.Type.2&#39;, &#39;BsmtFin.SF.2&#39;, &#39;Bsmt.Unf.SF&#39;, &#39;Total.Bsmt.SF&#39;, &#39;Heating&#39;, &#39;Heating.QC&#39;, &#39;Central.Air&#39;, &#39;Electrical&#39;, &#39;X1st.Flr.SF&#39;, &#39;X2nd.Flr.SF&#39;, &#39;Low.Qual.Fin.SF&#39;, &#39;Gr.Liv.Area&#39;, &#39;Bsmt.Full.Bath&#39;, &#39;Bsmt.Half.Bath&#39;, &#39;Full.Bath&#39;, &#39;Half.Bath&#39;, &#39;Bedroom.AbvGr&#39;, &#39;Kitchen.AbvGr&#39;, &#39;Kitchen.Qual&#39;, &#39;TotRms.AbvGrd&#39;, &#39;Functional&#39;, &#39;Fireplaces&#39;, &#39;Fireplace.Qu&#39;, &#39;Garage.Type&#39;, &#39;Garage.Yr.Blt&#39;, &#39;Garage.Finish&#39;, &#39;Garage.Cars&#39;, &#39;Garage.Area&#39;, &#39;Garage.Qual&#39;, &#39;Garage.Cond&#39;, &#39;Paved.Drive&#39;, &#39;Wood.Deck.SF&#39;, &#39;Open.Porch.SF&#39;, &#39;Enclosed.Porch&#39;, &#39;X3Ssn.Porch&#39;, &#39;Screen.Porch&#39;, &#39;Pool.Area&#39;, &#39;Pool.QC&#39;, &#39;Fence&#39;, &#39;Misc.Feature&#39;, &#39;Misc.Val&#39;, &#39;Mo.Sold&#39;, &#39;Yr.Sold&#39;, &#39;Sale.Type&#39;, &#39;Sale.Condition&#39;], dtype=&#39;object&#39;) . pipe = Pipeline([(&#39;dropper&#39;, ColumnDropper([&#39;Alley&#39;, &#39;Pool.QC&#39;, &#39;Fence&#39;, &#39;Misc.Feature&#39;, &#39;Fireplace.Qu&#39;, &#39;Order&#39;]))]) X_trans = pipe.fit_transform(X) X_trans.columns . Index([&#39;MS.SubClass&#39;, &#39;MS.Zoning&#39;, &#39;Lot.Frontage&#39;, &#39;Lot.Area&#39;, &#39;Street&#39;, &#39;Lot.Shape&#39;, &#39;Land.Contour&#39;, &#39;Utilities&#39;, &#39;Lot.Config&#39;, &#39;Land.Slope&#39;, &#39;Neighborhood&#39;, &#39;Condition.1&#39;, &#39;Condition.2&#39;, &#39;Bldg.Type&#39;, &#39;House.Style&#39;, &#39;Overall.Qual&#39;, &#39;Overall.Cond&#39;, &#39;Year.Built&#39;, &#39;Year.Remod.Add&#39;, &#39;Roof.Style&#39;, &#39;Roof.Matl&#39;, &#39;Exterior.1st&#39;, &#39;Exterior.2nd&#39;, &#39;Mas.Vnr.Type&#39;, &#39;Mas.Vnr.Area&#39;, &#39;Exter.Qual&#39;, &#39;Exter.Cond&#39;, &#39;Foundation&#39;, &#39;Bsmt.Qual&#39;, &#39;Bsmt.Cond&#39;, &#39;Bsmt.Exposure&#39;, &#39;BsmtFin.Type.1&#39;, &#39;BsmtFin.SF.1&#39;, &#39;BsmtFin.Type.2&#39;, &#39;BsmtFin.SF.2&#39;, &#39;Bsmt.Unf.SF&#39;, &#39;Total.Bsmt.SF&#39;, &#39;Heating&#39;, &#39;Heating.QC&#39;, &#39;Central.Air&#39;, &#39;Electrical&#39;, &#39;X1st.Flr.SF&#39;, &#39;X2nd.Flr.SF&#39;, &#39;Low.Qual.Fin.SF&#39;, &#39;Gr.Liv.Area&#39;, &#39;Bsmt.Full.Bath&#39;, &#39;Bsmt.Half.Bath&#39;, &#39;Full.Bath&#39;, &#39;Half.Bath&#39;, &#39;Bedroom.AbvGr&#39;, &#39;Kitchen.AbvGr&#39;, &#39;Kitchen.Qual&#39;, &#39;TotRms.AbvGrd&#39;, &#39;Functional&#39;, &#39;Fireplaces&#39;, &#39;Garage.Type&#39;, &#39;Garage.Yr.Blt&#39;, &#39;Garage.Finish&#39;, &#39;Garage.Cars&#39;, &#39;Garage.Area&#39;, &#39;Garage.Qual&#39;, &#39;Garage.Cond&#39;, &#39;Paved.Drive&#39;, &#39;Wood.Deck.SF&#39;, &#39;Open.Porch.SF&#39;, &#39;Enclosed.Porch&#39;, &#39;X3Ssn.Porch&#39;, &#39;Screen.Porch&#39;, &#39;Pool.Area&#39;, &#39;Misc.Val&#39;, &#39;Mo.Sold&#39;, &#39;Yr.Sold&#39;, &#39;Sale.Type&#39;, &#39;Sale.Condition&#39;], dtype=&#39;object&#39;) . Another option, especially with columns with missing values, is to impute the value but to include a column telling the model where the imputed values are. . class ImputeWithDummy(BaseEstimator, TransformerMixin): def __init__(self, cols_to_impute, strategy, fill=&#39;NA&#39;): self.cols_to_impute = cols_to_impute self.strategy = strategy self.fill = fill def fit(self, X, y=None, **kwargs): if self.strategy == &#39;mean&#39;: self.fill = X.mean() elif self.strategy == &#39;median&#39;: self.fill = X.median() elif self.strategy == &#39;mode&#39;: self.fill = X.mode().iloc[0] elif self.strategy == &#39;fill&#39;: if type(self.fill) is list and type(X) is pd.DataFrame: self.fill = dict([(cname, v) for cname,v in zip(X.columns, self.fill)]) return self def transform(self, X): df = X.copy() for col in self.cols_to_impute: df[&#39;{}_missing&#39;.format(col)] = df[col].isnull().astype(int) df[col] = df[col].fillna(self.fill[col]) return df X = pd.read_csv(&#39;ames.csv&#39;) imputer = ImputeWithDummy([&#39;Alley&#39;], strategy=&#39;mode&#39;) X_transformed = imputer.fit_transform(X) X_transformed[[&#39;Alley&#39;, &#39;Alley_missing&#39;]].head() . Alley Alley_missing . 0 Grvl | 1 | . 1 Grvl | 1 | . 2 Grvl | 1 | . 3 Grvl | 1 | . 4 Grvl | 1 | . Of course, you should always read the data documentation (https://ww2.amstat.org/publications/jse/v19n3/decock/datadocumentation.txt), and there you&#39;ll see for Alley that NaN means No Alley Access, and that we don&#39;t need to any imputation at all! . class NaNImpute(BaseEstimator, TransformerMixin): def __init__(self, cols, fill_vals): self.cols = cols self.fill_vals = fill_vals def fit(self, X, y=None, **kwargs): return self def transform(self, X): df = X.copy() for i, col in enumerate(self.cols): df[col].fillna(self.fill_vals[i]) . The other thing we&#39;ll need to consider is that some columns will need to be converted to numeric features first, before an estimator can be fitted. First we&#39;ll fit an imputer, and then encode. . class DummyEncoding(BaseEstimator, TransformerMixin): def __init__(self, columns=None): self.columns = columns def fit(self, X, y=None, **kwargs): return self def transform(self, X, y=None, **kwargs): return pd.get_dummies(X, columns=self.columns, drop_first=True) . impute_cols = [&#39;Alley&#39;, &#39;Pool.QC&#39;, &#39;Fence&#39;, &#39;Misc.Feature&#39;, &#39;Fireplace.Qu&#39;] pipe = Pipeline([(&#39;impute&#39;, ImputeWithDummy(impute_cols, strategy=&#39;mode&#39;)), (&#39;encode&#39;, DummyEncoding(impute_cols))]) X = pd.read_csv(&#39;ames.csv&#39;) X_trans = pipe.fit_transform(X) X_trans.columns . Index([&#39;Order&#39;, &#39;PID&#39;, &#39;MS.SubClass&#39;, &#39;MS.Zoning&#39;, &#39;Lot.Frontage&#39;, &#39;Lot.Area&#39;, &#39;Street&#39;, &#39;Lot.Shape&#39;, &#39;Land.Contour&#39;, &#39;Utilities&#39;, &#39;Lot.Config&#39;, &#39;Land.Slope&#39;, &#39;Neighborhood&#39;, &#39;Condition.1&#39;, &#39;Condition.2&#39;, &#39;Bldg.Type&#39;, &#39;House.Style&#39;, &#39;Overall.Qual&#39;, &#39;Overall.Cond&#39;, &#39;Year.Built&#39;, &#39;Year.Remod.Add&#39;, &#39;Roof.Style&#39;, &#39;Roof.Matl&#39;, &#39;Exterior.1st&#39;, &#39;Exterior.2nd&#39;, &#39;Mas.Vnr.Type&#39;, &#39;Mas.Vnr.Area&#39;, &#39;Exter.Qual&#39;, &#39;Exter.Cond&#39;, &#39;Foundation&#39;, &#39;Bsmt.Qual&#39;, &#39;Bsmt.Cond&#39;, &#39;Bsmt.Exposure&#39;, &#39;BsmtFin.Type.1&#39;, &#39;BsmtFin.SF.1&#39;, &#39;BsmtFin.Type.2&#39;, &#39;BsmtFin.SF.2&#39;, &#39;Bsmt.Unf.SF&#39;, &#39;Total.Bsmt.SF&#39;, &#39;Heating&#39;, &#39;Heating.QC&#39;, &#39;Central.Air&#39;, &#39;Electrical&#39;, &#39;X1st.Flr.SF&#39;, &#39;X2nd.Flr.SF&#39;, &#39;Low.Qual.Fin.SF&#39;, &#39;Gr.Liv.Area&#39;, &#39;Bsmt.Full.Bath&#39;, &#39;Bsmt.Half.Bath&#39;, &#39;Full.Bath&#39;, &#39;Half.Bath&#39;, &#39;Bedroom.AbvGr&#39;, &#39;Kitchen.AbvGr&#39;, &#39;Kitchen.Qual&#39;, &#39;TotRms.AbvGrd&#39;, &#39;Functional&#39;, &#39;Fireplaces&#39;, &#39;Garage.Type&#39;, &#39;Garage.Yr.Blt&#39;, &#39;Garage.Finish&#39;, &#39;Garage.Cars&#39;, &#39;Garage.Area&#39;, &#39;Garage.Qual&#39;, &#39;Garage.Cond&#39;, &#39;Paved.Drive&#39;, &#39;Wood.Deck.SF&#39;, &#39;Open.Porch.SF&#39;, &#39;Enclosed.Porch&#39;, &#39;X3Ssn.Porch&#39;, &#39;Screen.Porch&#39;, &#39;Pool.Area&#39;, &#39;Misc.Val&#39;, &#39;Mo.Sold&#39;, &#39;Yr.Sold&#39;, &#39;Sale.Type&#39;, &#39;Sale.Condition&#39;, &#39;SalePrice&#39;, &#39;Alley_missing&#39;, &#39;Pool.QC_missing&#39;, &#39;Fence_missing&#39;, &#39;Misc.Feature_missing&#39;, &#39;Fireplace.Qu_missing&#39;, &#39;Alley_Pave&#39;, &#39;Pool.QC_Fa&#39;, &#39;Pool.QC_Gd&#39;, &#39;Pool.QC_TA&#39;, &#39;Fence_GdWo&#39;, &#39;Fence_MnPrv&#39;, &#39;Fence_MnWw&#39;, &#39;Misc.Feature_Gar2&#39;, &#39;Misc.Feature_Othr&#39;, &#39;Misc.Feature_Shed&#39;, &#39;Misc.Feature_TenC&#39;, &#39;Fireplace.Qu_Fa&#39;, &#39;Fireplace.Qu_Gd&#39;, &#39;Fireplace.Qu_Po&#39;, &#39;Fireplace.Qu_TA&#39;], dtype=&#39;object&#39;) . This dataset has several types of columns - continuous features encoded as ints and floats, but also some ordinal variables have snick in as ints. We&#39;ll properly define all the int and float colukmns first: . float_cols = [&#39;Lot.Frontage&#39;, &#39;Mas.Vnr.Area&#39;, &#39;BsmtFin.SF.1&#39;, &#39;BsmtFin.SF.2&#39;, &#39;Bsmt.Unf.SF&#39;, &#39;Total.Bsmt.SF&#39;, &#39;Garage.Cars&#39;, &#39;Garage.Area&#39; ] int_cols = [&#39;MS.SubClass&#39;, &#39;Lot.Area&#39;, &#39;X1st.Flr.SF&#39;, &#39;X2nd.Flr.SF&#39;, &#39;Low.Qual.Fin.SF&#39;, &#39;Gr.Liv.Area&#39;, &#39;Full.Bath&#39;, &#39;Half.Bath&#39;, &#39;Bedroom.AbvGr&#39;, &#39;Kitchen.AbvGr&#39;, &#39;TotRms.AbvGrd&#39;, &#39;Fireplaces&#39;, &#39;Wood.Deck.SF&#39;, &#39;Open.Porch.SF&#39;, &#39;Enclosed.Porch&#39;, &#39;X3Ssn.Porch&#39;, &#39;Screen.Porch&#39;, &#39;Pool.Area&#39;, &#39;Misc.Val&#39; ] . Finally, we need some way to deal with ordinal features: . Lot Shape (Ordinal): General shape of property | Utilities (Ordinal): Type of utilities available | Land Slope (Ordinal): Slope of property | Overall Qual (Ordinal): Rates the overall material and finish of the house | Overall Cond (Ordinal): Rates the overall condition of the house | Exter Qual (Ordinal): Evaluates the quality of the material on the exterior | Exter Cond (Ordinal): Evaluates the present condition of the material on the exterior | Bsmt Qual (Ordinal): Evaluates the height of the basement | Bsmt Cond (Ordinal): Evaluates the general condition of the basement | Bsmt Exposure (Ordinal): Refers to walkout or garden level walls | BsmtFin Type 1 (Ordinal): Rating of basement finished area | BsmtFin Type 2 (Ordinal): Rating of basement finished area (if multiple types) | HeatingQC (Ordinal): Heating quality and condition | Electrical (Ordinal): Electrical system | FireplaceQu (Ordinal): Fireplace quality | Garage Finish (Ordinal) : Interior finish of the garage | Garage Qual (Ordinal): Garage quality | Garage Cond (Ordinal): Garage condition | Paved Drive (Ordinal): Paved driveway | Pool QC (Ordinal): Pool quality | Fence (Ordinal): Fence quality | . To do this we could use the OrdinalEncoder from http://contrib.scikit-learn.org/categorical-encoding/, which will be included in sklearn in a future release - but I have trouble getting this to work with Pandas. Another choice is just to write our own. What we&#39;ll do instead is to mix ordinal and categorical variables, and use the OneHotEncoder from the category_encoders package. . ord_cols = [&#39;Lot.Shape&#39;, &#39;Utilities&#39;, &#39;Land.Slope&#39;, &#39;Overall.Qual&#39;, &#39;Overall.Cond&#39;, &#39;Exter.Qual&#39;, &#39;Exter.Cond&#39;, &#39;Bsmt.Qual&#39;, &#39;Bsmt.Cond&#39;, &#39;Bsmt.Exposure&#39;, &#39;BsmtFin.Type.1&#39;, &#39;BsmtFin.SF.1&#39;, &#39;Heating.QC&#39;, &#39;Electrical&#39;, &#39;Fireplace.Qu&#39;, &#39;Garage.Finish&#39;, &#39;Garage.Qual&#39;, &#39;Garage.Cond&#39;, &#39;Paved.Drive&#39;, &#39;Pool.QC&#39;, &#39;Fence&#39;, ] cat_cols = [&#39;MS.SubClass&#39;, &#39;MS.Zoning&#39;, &#39;Street&#39;, &#39;Alley&#39;, &#39;Land.Contour&#39;, &#39;Lot.Config&#39;, &#39;Neighborhood&#39;, &#39;Condition.1&#39;, &#39;Condition.2&#39;, &#39;Bldg.Type&#39;, &#39;House.Style&#39;, &#39;Roof.Style&#39;, &#39;Exterior.1st&#39;, &#39;Exterior.2nd&#39;, &#39;Mas.Vnr.Type&#39;, &#39;Foundation&#39;, &#39;Heating&#39;, &#39;Central.Air&#39;, &#39;Garage.Type&#39;, &#39;Misc.Feature&#39;, &#39;Sale.Type&#39;, &#39;Sale.Condition&#39; ] . Finally, we define a few useful transforms and put together our first pipeline. . class DataFrameSelector(BaseEstimator, TransformerMixin): &#39;&#39;&#39; Select columns from pandas dataframe by specifying a list of column names &#39;&#39;&#39; def __init__(self, col_names): self.col_names = col_names def fit(self, X, y=None): return self def transform(self, X): return X[self.col_names] . class Scale(BaseEstimator, TransformerMixin): def __init__(self, cols): self.scaler = StandardScaler() self.cols = cols self.index = [] def fit(self, X, y=None, **kwargs): self.scaler.fit(X) self.cols = X.columns self.index = X.index return self def transform(self, X): df = X.copy() df = self.scaler.transform(df) df = pd.DataFrame(df, columns=self.cols, index=self.index) return df . from sklearn.externals.joblib import Parallel, delayed from sklearn.pipeline import FeatureUnion, _fit_transform_one, _transform_one from scipy import sparse class FeatureUnion(FeatureUnion): def fit_transform(self, X, y=None, **fit_params): self._validate_transformers() result = Parallel(n_jobs=self.n_jobs)( delayed(_fit_transform_one)(trans, weight, X, y, **fit_params) for name, trans, weight in self._iter()) if not result: # All transformers are None return np.zeros((X.shape[0], 0)) Xs, transformers = zip(*result) self._update_transformer_list(transformers) if any(sparse.issparse(f) for f in Xs): Xs = sparse.hstack(Xs).tocsr() else: Xs = self.merge_dataframes_by_column(Xs) return Xs def merge_dataframes_by_column(self, Xs): return pd.concat(Xs, axis=&quot;columns&quot;, copy=False) def transform(self, X): Xs = Parallel(n_jobs=self.n_jobs)( delayed(_transform_one)(trans, weight, X) for name, trans, weight in self._iter()) if not Xs: # All transformers are None return np.zeros((X.shape[0], 0)) if any(sparse.issparse(f) for f in Xs): Xs = sparse.hstack(Xs).tocsr() else: Xs = self.merge_dataframes_by_column(Xs) return Xs . from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer from category_encoders import OneHotEncoder numerical_cols = int_cols + float_cols pipe = Pipeline([ (&#39;features&#39;, FeatureUnion(n_jobs=1, transformer_list=[ (&#39;numericals&#39;, Pipeline([ (&#39;selector&#39;, DataFrameSelector(numerical_cols)), (&#39;imputer&#39;, ImputeWithDummy(numerical_cols, strategy=&#39;mean&#39;)), #(&#39;scaling&#39;, Scale(numerical_cols)) ])), #(&#39;categoricals&#39;, Pipeline([ # (&#39;selector&#39;, DataFrameSelector(cat_cols)), # (&#39;encode&#39;, OneHotEncoder(cat_cols, return_df=True)), #])), #(&#39;NanImpute&#39;, Pipeline([ ## (&#39;selector&#39;, DataFrameSelector([&#39;Alley&#39;, &#39;Pool.QC&#39;, &#39;Fence&#39;, &#39;Misc.Feature&#39;, &#39;Fireplace.Qu&#39;])), # (&#39;nan_impute&#39;, NaNImpute([&#39;Alley&#39;, &#39;Pool.QC&#39;, &#39;Fence&#39;, &#39;Misc.Feature&#39;, &#39;Fireplace.Qu&#39;], &#39;Not Applicable&#39;)) #])), ])), ]) . X = pd.read_csv(&#39;ames.csv&#39;) X = X.set_index(&#39;PID&#39;).copy() . X_trans = pipe.fit_transform(X) X_trans . MS.SubClass Lot.Area X1st.Flr.SF X2nd.Flr.SF Low.Qual.Fin.SF Gr.Liv.Area Full.Bath Half.Bath Bedroom.AbvGr Kitchen.AbvGr ... Pool.Area_missing Misc.Val_missing Lot.Frontage_missing Mas.Vnr.Area_missing BsmtFin.SF.1_missing BsmtFin.SF.2_missing Bsmt.Unf.SF_missing Total.Bsmt.SF_missing Garage.Cars_missing Garage.Area_missing . PID . 526301100 20 | 31770 | 1656 | 0 | 0 | 1656 | 1 | 0 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 526350040 20 | 11622 | 896 | 0 | 0 | 896 | 1 | 0 | 2 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 526351010 20 | 14267 | 1329 | 0 | 0 | 1329 | 1 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 526353030 20 | 11160 | 2110 | 0 | 0 | 2110 | 2 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527105010 60 | 13830 | 928 | 701 | 0 | 1629 | 2 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527105030 60 | 9978 | 926 | 678 | 0 | 1604 | 2 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527127150 120 | 4920 | 1338 | 0 | 0 | 1338 | 2 | 0 | 2 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527145080 120 | 5005 | 1280 | 0 | 0 | 1280 | 2 | 0 | 2 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527146030 120 | 5389 | 1616 | 0 | 0 | 1616 | 2 | 0 | 2 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527162130 60 | 7500 | 1028 | 776 | 0 | 1804 | 2 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527163010 60 | 10000 | 763 | 892 | 0 | 1655 | 2 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527165230 20 | 7980 | 1187 | 0 | 0 | 1187 | 2 | 0 | 3 | 1 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527166040 60 | 8402 | 789 | 676 | 0 | 1465 | 2 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527180040 20 | 10176 | 1341 | 0 | 0 | 1341 | 1 | 1 | 2 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527182190 120 | 6820 | 1502 | 0 | 0 | 1502 | 1 | 1 | 1 | 1 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527216070 60 | 53504 | 1690 | 1589 | 0 | 3279 | 3 | 1 | 4 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527225035 50 | 12134 | 1080 | 672 | 0 | 1752 | 2 | 0 | 4 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527258010 20 | 11394 | 1856 | 0 | 0 | 1856 | 1 | 1 | 1 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527276150 20 | 19138 | 864 | 0 | 0 | 864 | 1 | 0 | 2 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527302110 20 | 13175 | 2073 | 0 | 0 | 2073 | 2 | 0 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527358140 20 | 11751 | 1844 | 0 | 0 | 1844 | 2 | 0 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527358200 85 | 10625 | 1173 | 0 | 0 | 1173 | 2 | 0 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527368020 60 | 7500 | 814 | 860 | 0 | 1674 | 2 | 1 | 3 | 1 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527402200 20 | 11241 | 1004 | 0 | 0 | 1004 | 1 | 0 | 2 | 1 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527402250 20 | 12537 | 1078 | 0 | 0 | 1078 | 1 | 1 | 3 | 1 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527403020 20 | 8450 | 1056 | 0 | 0 | 1056 | 1 | 0 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527404120 20 | 8400 | 882 | 0 | 0 | 882 | 1 | 0 | 2 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527425090 20 | 10500 | 864 | 0 | 0 | 864 | 1 | 0 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527427230 120 | 5858 | 1337 | 0 | 0 | 1337 | 2 | 0 | 2 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 527451180 160 | 1680 | 483 | 504 | 0 | 987 | 1 | 1 | 2 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 916477010 20 | 13618 | 1960 | 0 | 0 | 1960 | 2 | 0 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 921205030 20 | 11443 | 2028 | 0 | 0 | 2028 | 2 | 0 | 2 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 921205050 20 | 11577 | 1838 | 0 | 0 | 1838 | 2 | 0 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923125030 20 | 31250 | 1600 | 0 | 0 | 1600 | 1 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923202025 90 | 7020 | 1368 | 0 | 0 | 1368 | 2 | 0 | 2 | 2 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923203090 120 | 4500 | 1216 | 0 | 0 | 1216 | 2 | 0 | 2 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923203100 120 | 4500 | 1337 | 0 | 0 | 1337 | 2 | 0 | 2 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923205120 20 | 17217 | 1140 | 0 | 0 | 1140 | 1 | 0 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923225190 160 | 2665 | 616 | 688 | 0 | 1304 | 1 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923225240 160 | 2665 | 925 | 550 | 0 | 1475 | 2 | 0 | 4 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923225260 160 | 3964 | 1291 | 1230 | 0 | 2521 | 2 | 1 | 5 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923225510 20 | 10172 | 874 | 0 | 0 | 874 | 1 | 0 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923226150 90 | 11836 | 1652 | 0 | 0 | 1652 | 2 | 0 | 4 | 2 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923226180 180 | 1470 | 630 | 0 | 0 | 630 | 1 | 0 | 1 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923226290 160 | 1484 | 546 | 546 | 0 | 1092 | 1 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923227100 20 | 13384 | 1360 | 0 | 0 | 1360 | 1 | 0 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923228130 180 | 1533 | 630 | 0 | 0 | 630 | 1 | 0 | 1 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923228180 160 | 1533 | 546 | 546 | 0 | 1092 | 1 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923228210 160 | 1526 | 546 | 546 | 0 | 1092 | 1 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923228260 160 | 1936 | 546 | 546 | 0 | 1092 | 1 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923228310 160 | 1894 | 546 | 546 | 0 | 1092 | 1 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923229110 90 | 12640 | 1728 | 0 | 0 | 1728 | 2 | 0 | 4 | 2 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923230040 90 | 9297 | 1728 | 0 | 0 | 1728 | 2 | 0 | 4 | 2 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923250060 20 | 17400 | 1126 | 0 | 0 | 1126 | 2 | 0 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923251180 20 | 20000 | 1224 | 0 | 0 | 1224 | 1 | 0 | 4 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923275080 80 | 7937 | 1003 | 0 | 0 | 1003 | 1 | 0 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923276100 20 | 8885 | 902 | 0 | 0 | 902 | 1 | 0 | 2 | 1 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 923400125 85 | 10441 | 970 | 0 | 0 | 970 | 1 | 0 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 924100070 20 | 10010 | 1389 | 0 | 0 | 1389 | 1 | 0 | 2 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 924151050 60 | 9627 | 996 | 1004 | 0 | 2000 | 2 | 1 | 3 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2930 rows × 54 columns . l = list(X_trans.isnull().sum()) . import warnings warnings.filterwarnings(&quot;ignore&quot;) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42) model = Pipeline([(&#39;pipeline&#39;, pipe), (&#39;clf&#39;, LarsCV())]) model.fit(X_train, y_train) . Pipeline(memory=None, steps=[(&#39;pipeline&#39;, Pipeline(memory=None, steps=[(&#39;features&#39;, FeatureUnion(n_jobs=1, transformer_list=[(&#39;numericals&#39;, Pipeline(memory=None, steps=[(&#39;selector&#39;, DataFrameSelector(col_names=[&#39;MS.SubClass&#39;, &#39;Lot.Area&#39;, &#39;X1st.Flr.SF&#39;, &#39;X2nd.Flr.SF&#39;, &#39;Low.Qual.Fin.SF&#39;, &#39;Gr.Liv.Area&#39;, &#39;Fu...max_n_alphas=1000, n_jobs=1, normalize=True, positive=False, precompute=&#39;auto&#39;, verbose=False))]) . pipe.fit_transform(X_test).columns . Index([&#39;MS.SubClass&#39;, &#39;Lot.Area&#39;, &#39;X1st.Flr.SF&#39;, &#39;X2nd.Flr.SF&#39;, &#39;Low.Qual.Fin.SF&#39;, &#39;Gr.Liv.Area&#39;, &#39;Full.Bath&#39;, &#39;Half.Bath&#39;, &#39;Bedroom.AbvGr&#39;, &#39;Kitchen.AbvGr&#39;, &#39;TotRms.AbvGrd&#39;, &#39;Fireplaces&#39;, &#39;Wood.Deck.SF&#39;, &#39;Open.Porch.SF&#39;, &#39;Enclosed.Porch&#39;, &#39;X3Ssn.Porch&#39;, &#39;Screen.Porch&#39;, &#39;Pool.Area&#39;, &#39;Misc.Val&#39;, &#39;Lot.Frontage&#39;, &#39;Mas.Vnr.Area&#39;, &#39;BsmtFin.SF.1&#39;, &#39;BsmtFin.SF.2&#39;, &#39;Bsmt.Unf.SF&#39;, &#39;Total.Bsmt.SF&#39;, &#39;Garage.Cars&#39;, &#39;Garage.Area&#39;, &#39;MS.SubClass_missing&#39;, &#39;Lot.Area_missing&#39;, &#39;X1st.Flr.SF_missing&#39;, &#39;X2nd.Flr.SF_missing&#39;, &#39;Low.Qual.Fin.SF_missing&#39;, &#39;Gr.Liv.Area_missing&#39;, &#39;Full.Bath_missing&#39;, &#39;Half.Bath_missing&#39;, &#39;Bedroom.AbvGr_missing&#39;, &#39;Kitchen.AbvGr_missing&#39;, &#39;TotRms.AbvGrd_missing&#39;, &#39;Fireplaces_missing&#39;, &#39;Wood.Deck.SF_missing&#39;, &#39;Open.Porch.SF_missing&#39;, &#39;Enclosed.Porch_missing&#39;, &#39;X3Ssn.Porch_missing&#39;, &#39;Screen.Porch_missing&#39;, &#39;Pool.Area_missing&#39;, &#39;Misc.Val_missing&#39;, &#39;Lot.Frontage_missing&#39;, &#39;Mas.Vnr.Area_missing&#39;, &#39;BsmtFin.SF.1_missing&#39;, &#39;BsmtFin.SF.2_missing&#39;, &#39;Bsmt.Unf.SF_missing&#39;, &#39;Total.Bsmt.SF_missing&#39;, &#39;Garage.Cars_missing&#39;, &#39;Garage.Area_missing&#39;], dtype=&#39;object&#39;) . preds = model.predict(X_test) . from sklearn.model_selection import cross_val_score from sklearn.model_selection import KFold def rmse_cv(model, X, y): rmse= np.sqrt(-cross_val_score(model, X, y, scoring=&quot;neg_mean_squared_error&quot;, cv=2)) return(rmse) . rmse_cv(model, X, y) . array([37647.53964453, 71123.27879506]) . from sklearn.metrics import r2_score def get_score(prediction, labels): print(&#39;R2: {}&#39;.format(r2_score(prediction, labels))) . get_score(preds, y_test) . R2: 0.6208433062823728 .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/04/01/ames-iowa.html",
            "relUrl": "/2021/04/01/ames-iowa.html",
            "date": " • Apr 1, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Title",
            "content": "We have previously considered models of the form: . $$ hat{y} = beta X + w $$ . where we have measured how well the model is doing by minimising the function: . $$ J left( beta right) = frac{1}{n} lVert y - hat{y} rVert $$ . However, this method doesn&#39;t allow us to encode some of the ideas we may have about $ beta$. . In least squares regression we are (essentially) solving a series of equations: . $$ y = X beta $$ . but the problem may be ill posed: there may be no $ beta$, or many, which satisfy the above equation. Also, many systems we are interested in moddeling act like low-pass filters going in the direction $X beta$, so inverting the system naively will act like a high-pass filter and will amplify noise. We can give preference to particular solutions by instead minimising: . $$ J left( theta right) = frac{1}{n} lVert y - hat{y} rVert_2^2 + lVert Gamma beta rVert_2^2 $$ . Luckily, this equation has a closed form solution: . $$ hat{ beta} = left(X^T X + Gamma^T Gamma right)^{-1} X^T y $$ . which can be found the same way as the closed form solution for Linear Regression. A particularly important case is $ Gamma = lambda 1$ (a constant times the identity matrix), which is known by the name of Ridge Regression. . Sometimes we have more complex priors about which solutions we require from any particular optimisation problem, and many cannot be solved by simply taking the gradient. For example . $$ J left( theta right) = frac{1}{n} lVert y - hat{y} rVert_2^2 + lVert beta rVert_1 $$ . this optimisation problem is non differentiable! Or consider . $$ J left( theta right) = frac{1}{n} lVert y - hat{y} rVert_2^2 + lVert nabla beta rVert_1 $$ . or . $$ J left( theta right) = frac{1}{n} lVert y - hat{y} rVert_2^2 + lVert beta rVert_0 $$ . where . $$ lVert beta rVert_0 = { beta neq 0 } $$ . None of these optimisation problems can be solved in the straightforward way that we solved Ridge regression. . These optimisation problem can be solved by using the following trick, set . $$ z = beta $$ . in the second term, and then optimise the following function (the last term is to enforce the constraint we introduced): . $$ J left( beta right) = frac{1}{n} lVert y - beta^T X rVert_2^2 + lambda lVert z rVert_2^2 + nu^T left( beta - z right) + frac{ rho}{2} lVert beta -z rVert_2^2 $$ . This is cleverer than it looks, because . $$ frac{ partial J}{ partial beta} = -X^T left(y - X beta right) + rho left( beta - z right) + nu^T $$ . and . $$ frac{ partial J}{ partial z} = lambda - nu^T - rho left( beta - z right) $$ . for $ z &gt; 0 $, and . $$ frac{ partial J}{ partial z} = - lambda - nu^T + rho left( beta - z right) $$ . for $ z &lt; 0 $, and . $$ - frac{ lambda}{ rho} leq x + frac{ nu}{ rho} leq frac{ lambda}{ rho} $$ . combining these we find: . $$ z = mathrm{sign} left(X + frac{ nu}{ rho} right) mathrm{max} left( mid X + frac{ nu}{ rho} mid - frac{ lambda}{ rho}, 0 right) $$ . we can then update our weights by the following set of iterates: . $$ X^{k+1} = left(X^T X + rho I right)^{-1} left(X^t y + rho left(z^{k} - nu^{k} right) right)$$ . $$ z^{k+1} = S_{ frac{ lambda}{ rho}} left(X^{k+1} + nu^{k}/ rho right) $$ . $$ nu^{k+1} = n^{k} + rho left(x^{k+1} - z^{k+1} right) $$ . This is implemented in the code below: . import numpy as np import matplotlib.pyplot as plt %matplotlib inline def l2prox(y, mu): return (1.0/(1.0 + mu)) * y def l1prox(y, mu): return np.sign(y)*np.maximum(0, np.absolute(y)-mu/2.0) def ADMM(A, y, rho, mu, prox): &quot;&quot;&quot;Alternating Direction Method of Multipliers This is a python implementation of the Alternating Direction Method of Multipliers - a method of constrained optimisation that is used widely in statistics (http://stanford.edu/~boyd/admm.html). &quot;&quot;&quot; m, n = A.shape A_t_A = A.T.dot(A) w, v = np.linalg.eig(A_t_A) MAX_ITER = 10000 #Function to caluculate min 1/2(y - Ax) + l||x|| #via alternating direction methods x_hat = np.zeros([n, 1]) z_hat = np.zeros([n, 1]) u = np.zeros([n, 1]) #Calculate regression co-efficient and stepsize # r = np.amax(np.absolute(w)) # l_over_rho = np.sqrt(2*np.log(n)) * r / 2.0 # I might be wrong here # rho = mu/r #Pre-compute to save some multiplications A_t_y = A.T.dot(y) Q = A_t_A + rho * np.identity(n) Q = np.linalg.inv(Q) Q_dot = Q.dot for _ in range(MAX_ITER): #x minimisation step via posterier OLS x_hat = Q_dot(A_t_y + rho*(z_hat - u)) z_hat = prox(x_hat + u, mu) #mulitplier update u = u + rho*(x_hat - z_hat) return z_hat def plot(original, computed): &quot;&quot;&quot;Plot two vectors to compare their values&quot;&quot;&quot; plt.figure(1) plt.subplot(211) plt.plot(original, label=&#39;Original&#39;) plt.plot(computed, label=&#39;Estimate&#39;) plt.subplot(212) plt.plot(original - computed) plt.legend(loc=&#39;upper right&#39;) plt.show() def test(m=50, n=200): &quot;&quot;&quot;Test the ADMM method with randomly generated matrices and vectors&quot;&quot;&quot; A = np.random.randn(m, n) num_non_zeros = 10 positions = np.random.randint(0, n, num_non_zeros) amplitudes = 100*np.random.randn(num_non_zeros, 1) x = np.zeros((n, 1)) x[positions] = amplitudes y = A.dot(x) #+ np.random.randn(m, 1) plot(x, ADMM(A, y, 1.0, 1.0, l1prox)) test() . No handles with labels found to put in legend. .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/03/05/More-Complex-Regularised-Linear-Regressions.html",
            "relUrl": "/2021/03/05/More-Complex-Regularised-Linear-Regressions.html",
            "date": " • Mar 5, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Title",
            "content": "Local Linear Trend models are one of the simplest time series models, and can be expressed by the following equations: . $$ v_t sim N left(v_t, sigma_v^2 right) $$ . $$ x_t sim N left(x_{t-1} + v_{t-1}, sigma_x^2 right) $$ . $$ y_t sim N left(x_t, sigma_y^2 right) $$ . Where $ sigma_x^2$ is the observation error, $ sigma_y^2$ is the level disturbance, and $ sigma_v^2$ is the slope distrubance . We will model this in pystan, using the air passengers dataset. . import pystan import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd plt.style.use(&#39;ggplot&#39;) %matplotlib inline passengers = pd.read_csv(&#39;passengers.csv&#39;, header=0, sep=&#39;;&#39;) passengers[&#39;Month&#39;] = pd.to_datetime(passengers[&#39;Month&#39;]) passengers.set_index(&#39;Month&#39;, inplace=True) passengers.plot(figsize=(15, 10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1eb1f828&gt; . In stan we can write out model as follows: . stan_code = &quot;&quot;&quot;data { int N; vector[N] X; } parameters { vector[N] u; vector[N] v; real&lt;lower=0&gt; s_u; real&lt;lower=0&gt; s_v; real&lt;lower=0&gt; s_x; } model { v[2:N] ~ normal(v[1:N-1], s_v); u[2:N] ~ normal(u[1:N-1] + v[1:N-1], s_u); X ~ normal(u, s_x); }&quot;&quot;&quot; . data_feed = {&#39;X&#39;: passengers[&#39;Passengers&#39;].values, &#39;N&#39;: passengers.shape[0]} sm = pystan.StanModel(model_code=stan_code) fit = sm.sampling(data=data_feed, iter=1000) . INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_ae1b8f06975ee0f66c2a6bd10f156f5b NOW. . We can visually check the fit and the parameters with: . with mpl.rc_context(): mpl.rc(&#39;figure&#39;, figsize=(30, 10)) fit.plot() . And we can also check the in sample fit visually: . samples = fit.extract(permuted=True) u_mean = samples[&#39;u&#39;].mean(axis=0) . passengers[&#39;pred&#39;] = u_mean passengers.plot(figsize=(30, 10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1ff665c0&gt; . To predict future points, we have to include the extra points in the original stan code . stan_code = &quot;&quot;&quot;data { int N; vector[N] X; int pred_num; } parameters { vector[N] u; vector[N] v; real&lt;lower=0&gt; s_u; real&lt;lower=0&gt; s_v; real&lt;lower=0&gt; s_x; } model { v[2:N] ~ normal(v[1:N-1], s_v); u[2:N] ~ normal(u[1:N-1] + v[1:N-1], s_u); X ~ normal(u, s_x); } generated quantities { vector[N + pred_num] u_pred; vector[pred_num] x_pred; u_pred[1:N] = u; for (i in 1:pred_num) { u_pred[N+i] = normal_rng(u_pred[N+i-1], s_u); x_pred[i] = normal_rng(u_pred[N+i], s_x); } } &quot;&quot;&quot; . data_feed = {&#39;X&#39;: passengers[&#39;Passengers&#39;].values, &#39;N&#39;: passengers.shape[0], &#39;pred_num&#39;:10} sm = pystan.StanModel(model_code=stan_code) fit = sm.sampling(data=data_feed, iter=1000) . INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_cc645429411ff4903a697d8562e07c6d NOW. . samples = fit.extract(permuted=True) u_mean = samples[&#39;u&#39;].mean(axis=0) u_pred = samples[&#39;u_pred&#39;][:] pred_df = pd.DataFrame(data=u_pred).T . passengers[&#39;pred&#39;] = u_mean passengers.plot(figsize=(30, 10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a204ff0b8&gt; . index = pd.date_range(&#39;1961-01&#39;, periods=10, freq=&#39;MS&#39;) . df_ = pd.DataFrame(index=passengers.index.append(index), columns=passengers.columns) . df_[&#39;Passengers&#39;] = passengers[&#39;Passengers&#39;] . pred_df.set_index(passengers.index.append(index), inplace=True) df_ = pd.concat([df_, pred_df], axis=1) . df_[[&#39;Passengers&#39;, &#39;pred&#39;, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]].plot(figsize=(30, 10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1f9f33c8&gt; . So, even though our model has a good in-sample fit, the out of sample predictions are very poor. To solve this, we can add a seasonal component: . $$ u_t sim N left(u_{t-1}, sigma_v^2 right) $$ . $$ s_t sim N left(- sum^n_{l=1}s_{t-l}, sigma_s right) $$ . $$ y_t sim N left(u_t + s_t, sigma_y^2 right) $$ . stan_code = &quot;&quot;&quot;data { int N; int pred_num; vector[N] y; } parameters { vector[N] s; vector[N] u; real&lt;lower=0&gt; s_s; real&lt;lower=0&gt; s_u; real&lt;lower=0&gt; s_y; } model { s[12:N] ~ normal(-s[1:N-11]-s[2:N-10]-s[3:N-9]-s[4:N-8]-s[5:N-7]-s[6:N-6]-s[7:N-5]-s[8:N-4]-s[9:N-3]-s[10:N-2]-s[11:N-1], s_s); u[2:N] ~ normal(u[1:N-1], s_u); y ~ normal(u+s, s_y); } generated quantities { vector[N+pred_num] s_pred; vector[N+pred_num] u_pred; vector[N+pred_num] y_pred; s_pred[1:N] = s; u_pred[1:N] = u; y_pred[1:N] = y; for (t in (N+1):(N+pred_num)){ s_pred[t] = normal_rng(-s_pred[t-11]-s_pred[t-10]-s_pred[t-9]-s_pred[t-8]-s_pred[t-7]-s_pred[t-6]-s_pred[t-5]-s_pred[t-4]-s_pred[t-3]-s_pred[t-2]-s_pred[t-1], s_s); u_pred[t] = normal_rng(u_pred[t-1], s_u); y_pred[t] = normal_rng(u_pred[t]+s_pred[t], s_y); } } &quot;&quot;&quot; . data_feed = {&#39;y&#39;: passengers[&#39;Passengers&#39;].values, &#39;N&#39;: passengers.shape[0], &#39;pred_num&#39;:10} sm = pystan.StanModel(model_code=stan_code) fit = sm.sampling(data=data_feed, iter=1000) . INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_65ffdf38c841c93de59a3d4d247dc640 NOW. . samples = fit.extract(permuted=True) u_mean = samples[&#39;u&#39;].mean(axis=0) u_pred = samples[&#39;y_pred&#39;][:] pred_df = pd.DataFrame(data=u_pred).T df_ = pd.DataFrame(index=passengers.index.append(index), columns=passengers.columns) df_[&#39;Passengers&#39;] = passengers[&#39;Passengers&#39;] pred_df.set_index(passengers.index.append(index), inplace=True) df_ = pd.concat([df_, pred_df], axis=1) df_[[&#39;Passengers&#39;, &#39;pred&#39;, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]].plot(figsize=(30, 10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a2118a278&gt; . s_pred = samples[&#39;s_pred&#39;].mean(axis=0) plt.plot(list(range(0, s_pred.shape[0])), s_pred) . [&lt;matplotlib.lines.Line2D at 0x1a25972710&gt;] . These out of sample predicitons look much better! .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/03/02/Local-Linear-trend-models-for-time-series.html",
            "relUrl": "/2021/03/02/Local-Linear-trend-models-for-time-series.html",
            "date": " • Mar 2, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Title",
            "content": "In this post we&#39;ll discuss some ways of doing feature selection within a Bayesian framework . Let $y$ be a set of real-valued observations. The basic linear regression model can be decribed as . $$ y_t = beta^Tx + varepsilon_t$$ . Where $ varepsilon sim N left(0, sigma^2 right) $. This can be equivalently written: . $$ y sim N left( beta^Tx, sigma^2 right) $$. . We will first place a prior on $ beta$, like: . $$ beta_i sim left( frac{ tau}{2} right)^p mathrm{exp} left(- tau mid beta mid right)$$ . Then we will use the horseshoe prior: . $$ beta_i sim N left(0, lambda_i right) $$ . $$ lambda_i sim mathrm{Cauchy}^+ left(0, tau right)$$ . $$ tau sim mathrm{Cauchy}^+ left(0, 1 right)$$ . import numpy as np import pymc3 as pm from sklearn.metrics import r2_score import theano import theano.tensor as T import matplotlib.pyplot as plt import matplotlib as mpl plt.style.use(&#39;ggplot&#39;) %matplotlib inline from pymc3_models.exc import PyMC3ModelsError from pymc3_models.models import BayesianModel . class BayesianLassoRegression(BayesianModel): &quot;&quot;&quot; Linear Regression built using PyMC3. &quot;&quot;&quot; def __init__(self): super(BayesianLassoRegression, self).__init__() self.ppc = None def create_model(self): &quot;&quot;&quot; Creates and returns the PyMC3 model. Note: The size of the shared variables must match the size of the training data. Otherwise, setting the shared variables later will raise an error. See http://docs.pymc.io/advanced_theano.html Returns - the PyMC3 model &quot;&quot;&quot; model_input = theano.shared(np.zeros([self.num_training_samples, self.num_pred])) model_output = theano.shared(np.zeros(self.num_training_samples)) self.shared_vars = { &#39;model_input&#39;: model_input, &#39;model_output&#39;: model_output, } model = pm.Model() with model: beta = pm.Laplace(&#39;beta&#39;, mu = 0.0, b = 10.0, shape = (1, self.num_pred)) sigma = pm.HalfNormal(&#39;sigma&#39;, sd=10.0) ll = pm.math.dot(model_input, beta.T) mu = T.sum(ll) y = pm.Normal(&#39;y&#39;, mu=mu, sd=sigma, observed=model_output) return model def fit(self, X, y, inference_type=&#39;nuts&#39;, minibatch_size=None, inference_args={&#39;draws&#39;: 1000}): &quot;&quot;&quot; Train the Linear Regression model Parameters - X : numpy array, shape [n_samples, n_features] y : numpy array, shape [n_samples, ] inference_type : string, specifies which inference method to call. Defaults to &#39;advi&#39;. Currently, only &#39;advi&#39; and &#39;nuts&#39; are supported minibatch_size : number of samples to include in each minibatch for ADVI, defaults to None, so minibatch is not run by default inference_args : dict, arguments to be passed to the inference methods. Check the PyMC3 docs for permissable values. If no arguments are specified, default values will be set. &quot;&quot;&quot; self.num_training_samples, self.num_pred = X.shape self.inference_type = inference_type if y.ndim != 1: y = np.squeeze(y) if not inference_args: inference_args = self._set_default_inference_args() if self.cached_model is None: self.cached_model = self.create_model() if minibatch_size: with self.cached_model: minibatches = { self.shared_vars[&#39;model_input&#39;]: pm.Minibatch(X, batch_size=minibatch_size), self.shared_vars[&#39;model_output&#39;]: pm.Minibatch(y, batch_size=minibatch_size), } inference_args[&#39;more_replacements&#39;] = minibatches else: self._set_shared_vars({&#39;model_input&#39;: X, &#39;model_output&#39;: y}) self._inference(inference_type, inference_args) return self def predict(self, X, return_std=False): &quot;&quot;&quot; Predicts values of new data with a trained Linear Regression model Parameters - X : numpy array, shape [n_samples, n_features] return_std : Boolean flag of whether to return standard deviations with mean values. Defaults to False. &quot;&quot;&quot; if self.trace is None: raise PyMC3ModelsError(&#39;Run fit on the model before predict.&#39;) num_samples = X.shape[0] if self.cached_model is None: self.cached_model = self.create_model() self._set_shared_vars({&#39;model_input&#39;: X, &#39;model_output&#39;: np.zeros(num_samples)}) self.ppc = pm.sample_ppc(self.trace, model=self.cached_model, samples=2000) if return_std: return self.ppc[&#39;y&#39;].mean(axis=0), ppc[&#39;y&#39;].std(axis=0) else: return self.ppc[&#39;y&#39;].mean(axis=0) def score(self, X, y): &quot;&quot;&quot; Scores new data with a trained model. Parameters - X : numpy array, shape [n_samples, n_features] y : numpy array, shape [n_samples, ] &quot;&quot;&quot; return r2_score(y, self.predict(X)) def save(self, file_prefix): params = { &#39;inference_type&#39;: self.inference_type, &#39;num_pred&#39;: self.num_pred, &#39;num_training_samples&#39;: self.num_training_samples } super(LinearRegression, self).save(file_prefix, params) def load(self, file_prefix): params = super(LinearRegression, self).load(file_prefix, load_custom_params=True) self.inference_type = params[&#39;inference_type&#39;] self.num_pred = params[&#39;num_pred&#39;] self.num_training_samples = params[&#39;num_training_samples&#39;] . class HorseshoeRegression(BayesianModel): &quot;&quot;&quot; Linear Regression built using PyMC3. &quot;&quot;&quot; def __init__(self): super(HorseshoeRegression, self).__init__() self.ppc = None def create_model(self): &quot;&quot;&quot; Creates and returns the PyMC3 model. Note: The size of the shared variables must match the size of the training data. Otherwise, setting the shared variables later will raise an error. See http://docs.pymc.io/advanced_theano.html Returns - the PyMC3 model &quot;&quot;&quot; model_input = theano.shared(np.zeros([self.num_training_samples, self.num_pred])) model_output = theano.shared(np.zeros(self.num_training_samples)) self.shared_vars = { &#39;model_input&#39;: model_input, &#39;model_output&#39;: model_output, } model = pm.Model() m = 10 ss = 3 dof = 25 with model: sigma = pm.HalfNormal(&#39;sigma&#39;, 2) tau_0 = m / (self.num_training_samples - m) * sigma / T.sqrt(self.num_training_samples) tau = pm.HalfCauchy(&#39;tau&#39;, tau_0) c2 = pm.InverseGamma(&#39;c2&#39;, dof/2, dof/2 * ss**2) lam = pm.HalfCauchy(&#39;lam&#39;, 1) l1 = lam * T.sqrt(c2) l2 = T.sqrt(c2 + tau * tau * lam * lam) lam_d = l1 / l2 beta = pm.Normal(&#39;beta&#39;, 0, tau * lam_d, shape=self.num_pred) y_hat = T.dot(model_input, beta) y = pm.Normal(&#39;y&#39;, mu=y_hat, observed=model_output) return model def fit(self, X, y, inference_type=&#39;nuts&#39;, minibatch_size=None, inference_args={&#39;draws&#39;: 1000}): &quot;&quot;&quot; Train the Linear Regression model Parameters - X : numpy array, shape [n_samples, n_features] y : numpy array, shape [n_samples, ] inference_type : string, specifies which inference method to call. Defaults to &#39;advi&#39;. Currently, only &#39;advi&#39; and &#39;nuts&#39; are supported minibatch_size : number of samples to include in each minibatch for ADVI, defaults to None, so minibatch is not run by default inference_args : dict, arguments to be passed to the inference methods. Check the PyMC3 docs for permissable values. If no arguments are specified, default values will be set. &quot;&quot;&quot; self.num_training_samples, self.num_pred = X.shape self.inference_type = inference_type if y.ndim != 1: y = np.squeeze(y) if not inference_args: inference_args = self._set_default_inference_args() if self.cached_model is None: self.cached_model = self.create_model() if minibatch_size: with self.cached_model: minibatches = { self.shared_vars[&#39;model_input&#39;]: pm.Minibatch(X, batch_size=minibatch_size), self.shared_vars[&#39;model_output&#39;]: pm.Minibatch(y, batch_size=minibatch_size), } inference_args[&#39;more_replacements&#39;] = minibatches else: self._set_shared_vars({&#39;model_input&#39;: X, &#39;model_output&#39;: y}) self._inference(inference_type, inference_args) return self def predict(self, X, return_std=False): &quot;&quot;&quot; Predicts values of new data with a trained Linear Regression model Parameters - X : numpy array, shape [n_samples, n_features] return_std : Boolean flag of whether to return standard deviations with mean values. Defaults to False. &quot;&quot;&quot; if self.trace is None: raise PyMC3ModelsError(&#39;Run fit on the model before predict.&#39;) num_samples = X.shape[0] if self.cached_model is None: self.cached_model = self.create_model() self._set_shared_vars({&#39;model_input&#39;: X, &#39;model_output&#39;: np.zeros(num_samples)}) self.ppc = pm.sample_ppc(self.trace, model=self.cached_model, samples=2000) if return_std: return selfl.ppc[&#39;y&#39;].mean(axis=0), ppc[&#39;y&#39;].std(axis=0) else: return self.ppc[&#39;y&#39;].mean(axis=0) def score(self, X, y): &quot;&quot;&quot; Scores new data with a trained model. Parameters - X : numpy array, shape [n_samples, n_features] y : numpy array, shape [n_samples, ] &quot;&quot;&quot; return r2_score(y, self.predict(X)) def save(self, file_prefix): params = { &#39;inference_type&#39;: self.inference_type, &#39;num_pred&#39;: self.num_pred, &#39;num_training_samples&#39;: self.num_training_samples } super(LinearRegression, self).save(file_prefix, params) def load(self, file_prefix): params = super(LinearRegression, self).load(file_prefix, load_custom_params=True) self.inference_type = params[&#39;inference_type&#39;] self.num_pred = params[&#39;num_pred&#39;] self.num_training_samples = params[&#39;num_training_samples&#39;] . np.random.seed(42) n_samples, n_features = 50, 200 X = np.random.randn(n_samples, n_features) coef = 3 * np.random.randn(n_features) inds = np.arange(n_features) np.random.shuffle(inds) coef[inds[10:]] = 0 # sparsify coef y = np.dot(X, coef) # add noise y += 0.01 * np.random.normal(size=n_samples) # Split data in train set and test set n_samples = X.shape[0] X_train, y_train = X[:n_samples // 2], y[:n_samples // 2] X_test, y_test = X[n_samples // 2:], y[n_samples // 2:] . def plot_beta(b, b_hat, std=None): x = range(len(b)) plt.plot(x, b, &#39;k&#39;, alpha=0.5) plt.plot(x, b_hat, alpha=0.5) if std is not None: plt.fill_between(x, b_hat + std, b_hat - std, alpha=0.3) plt.show() def make_data(n, m): from scipy.stats import bernoulli alpha = 3 sigma = 1 sig_p = 0.05 beta = np.zeros(m) f = np.zeros(m) for i in range(m): if bernoulli(sig_p).rvs(): if bernoulli(0.5).rvs(): beta[i] = np.random.normal(10, 1) else: beta[i] = np.random.normal(-10, 1) f[i] = 1 else: beta[i] = np.random.normal(0, 0.25) X = np.random.normal(0, 1, (n, m)) y = np.random.normal(X.dot(beta) + alpha, sigma) return X, y, beta, f X, y, beta, f = make_data(100, 200) with mpl.rc_context(): mpl.rc(&#39;figure&#39;, figsize=(30, 10)) plt.plot(beta, &#39;--&#39;, color=&#39;navy&#39;, label=&#39;original coefficients&#39;) . lasso = BayesianLassoRegression() lasso.fit(X, y) y_pred_lasso = lasso.predict(X) r2_score_lasso = lasso.score(X, y) print(lasso) print(&quot;r^2 on test data : %f&quot; % r2_score_lasso) . Multiprocess sampling (2 chains in 2 jobs) NUTS: [sigma, beta] Sampling 2 chains: 100%|██████████| 3000/3000 [04:20&lt;00:00, 10.81draws/s] The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize. The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize. The estimated number of effective samples is smaller than 200 for some parameters. 100%|██████████| 2000/2000 [00:15&lt;00:00, 125.78it/s] . BayesianLassoRegression() r^2 on test data : -0.003733 . . horse_lasso = HorseshoeRegression() horse_lasso.fit(X, y) y_pred_hlasso = horse_lasso.predict(X) r2_score_hlasso = r2_score(y, y_pred_hlasso) print(&quot;r^2 on test data : %f&quot; % r2_score_hlasso) . Multiprocess sampling (2 chains in 2 jobs) NUTS: [beta, lam, c2, tau, sigma] Sampling 2 chains: 100%|██████████| 3000/3000 [01:37&lt;00:00, 30.67draws/s] 47%|████▋ | 934/2000 [00:07&lt;00:08, 120.36it/s] . with mpl.rc_context(): mpl.rc(&#39;figure&#39;, figsize=(30, 10)) plt.plot(coef, &#39;--&#39;, color=&#39;navy&#39;, label=&#39;original coefficients&#39;) plt.plot(lasso.trace.get_values(&#39;beta&#39;).mean(0).T, color=&#39;gold&#39;, label=&#39;original coefficients&#39;) plt.plot(horse_lasso.trace.get_values(&#39;beta&#39;).mean(0).T, color=&#39;lightgreen&#39;, label=&#39;original coefficients&#39;) plt.legend(loc=&#39;best&#39;) . from pymc3 import summary, traceplot .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/03/01/bayesian-feature-selection.html",
            "relUrl": "/2021/03/01/bayesian-feature-selection.html",
            "date": " • Mar 1, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Gradient Descent",
            "content": "Gradient Descent is the simplest learning algorithm. Suppose you have a set of observations of some process you wanted to model, for example the size of a house labelled as $ x_i in mathrm{R}^n $, and the house price labeleld as $ y_i in mathrm{R} $, $ i = 1 ldots m$ (i.e. you have $m$ examples). One good choice for a model is linear: . $$ hat{y} left(x right) = Ax + b $$ . The goal is to find some suitable $ A in mathcal{R}^n $ and $ b in mathcal{R} $, to model this process coreectly. An example is shown below (the data is taken from the Coursera Machine Learning class). . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; plt.style.use([&#39;seaborn-colorblind&#39;, &#39;seaborn-darkgrid&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x109cba630&gt; . We&#39;ll generate some data that we&#39;ll use for the rest of this post: . def generate_data(n, m=2.25, b=6.0, stddev=1.5): &quot;&quot;&quot;Generate n data points approximating given line. m, b: line slope and intercept. stddev: standard deviation of added error. Returns pair x, y: arrays of length n. &quot;&quot;&quot; x = np.linspace(-2.0, 2.0, n) y = x * m + b + np.random.normal(loc=0, scale=stddev, size=n) return x.reshape(-1, 1), y . N = 500 X, y = generate_data(N) plt.scatter(X, y) . &lt;matplotlib.collections.PathCollection at 0x10ee65438&gt; . For convenience, let&#39;s define $ Theta = [A: b]$ be a $ (n+1) times 1 $ vector, let $ X = [x_1: ldots : x_m :1] in mathrm{R}^{(n+1) times m} $, and let $Y = [y_1: ldots : y_m :1]$ (i.e. we expanded both to include the intercept, and we concatenate all the examples into a single matrix of x&#39;s and y&#39;s respectively). Now out hypothesis can be written as $ hat{Y} = Theta^T X $ . Say you also had good reason to believe that the best reconstruction of $ x $ you could possilby hope to achieve was to minimise the following (mean squared) error measure: . $$ J left( Theta right) = frac{1}{n} lVert hat{Y} - Y rVert_2^2 $$ . A function to compute the cost is below: . def MSE(y, yhat): &quot;&quot;&quot;Function to compute MSE between true values and estimate y: true values yhat: estimate &quot;&quot;&quot; assert(y.shape == yhat.shape) return np.square(np.subtract(y, yhat)).mean() . We could also seek to minimise the least absolute deviations of our predictions from the data: . $$ J left( Theta right) = frac{1}{n} lVert hat{Y} - Y rVert_1 $$ . a function to do this is included below: . def MAE(y, yhat): &quot;&quot;&quot;Function to compute LAE between true values and estimate y: true values yhat: estimate &quot;&quot;&quot; assert(y.shape == yhat.shape) return np.absolute(y - yhat).mean() . There are a couple of ways you could find such an $ hat{Y} $, given a cost function. The most straigtforward is to start with some initital value, and then move in the direction of the negative gradient of the cost funtion: . $$ Theta_{k+1} = Theta_{k} - alpha nabla_{ Theta} J left( Theta right) $$ . with $ Theta_0 = 0 $. Here. $ alpha$ is the learning rate - a tuneable parameter. . The following function does exactly this, using autograd to avoid mathematically computing the gradients. . def gradient_descent(X, y, cost, learning_rate=0.01, num_iters=250): m, n = X.shape theta = np.zeros((n, 1)) yhat = theta.T @ X.T yield theta, cost(y.reshape(-1, 1), yhat.T) for i in range(num_iters): yhat = theta.T @ X.T yhatt = yhat.T nabla = np.sum(X.T @ (y.reshape(-1, 1) - yhatt), axis=1).reshape(-1, 1) assert(nabla.shape == theta.shape) theta += (2 * learning_rate / m) * nabla yield theta, cost(y.reshape(-1, 1), yhat.T) . ones = np.ones_like(X) X = np.concatenate([X, ones], axis=1) thetas = gradient_descent(X, y, MSE) . final = [(t, c) for t,c in thetas] costs = [x[1] for x in final] theta = final[-1][0] plt.plot(costs) . [&lt;matplotlib.lines.Line2D at 0x10eedc898&gt;] . theta . array([[ 2.27769915], [ 5.90213934]]) . x = np.linspace(-2.0, 2.0) yhat = x * theta[0] + theta[1] . plt.scatter(X[:,0], y) plt.plot(x, yhat, &quot;r-&quot;) . [&lt;matplotlib.lines.Line2D at 0x10eebad30&gt;] . Firstly, if you are minimising the MSE you can compute it analytically via . $$ (A^T A)^{-1} A^Ty $$ .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/02/01/Gradietnt-Descent.html",
            "relUrl": "/2021/02/01/Gradietnt-Descent.html",
            "date": " • Feb 1, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Title",
            "content": "Often, we have data which has a group structure. For example, in the dataset we use in this post, radon measurements were taken in ~900 houses in 85 counties. It&#39;s unreasonable to expect that radon levels do not vary by state as well as house, and so we will integrate this into our analysis. . Typically, in linear regression we assume that each data point is independent and regresses with a constant slope amongst each other: . $$ y = X^T beta + varepsilon $$ . where . $$ varepsilon sim N left(0, I right) $$ . and $X$ are known as fixed effects coefficients. To define a mixed model, we include a term $Z eta$, which corresponds to random effects. The model is now: . $$ y = X^T beta + Z^T eta + varepsilon $$ . where . $$ varepsilon sim N left(0, sigma right) $$ . and . $$ eta sim N left(0, sigma^2I right) $$ . We wish to infer $ beta, eta, sigma$. Given the random effects have mean 0, the term $X^T beta$ captures the data&#39;s mean amd the term $Z^T eta$ captures variations in the data. . %matplotlib inline import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns; sns.set_context(&#39;notebook&#39;) import pystan import statsmodels.formula.api as smf from patsy import dmatrices # Import radon data radon = pd.read_csv(&#39;radon.csv&#39;) radon.columns = radon.columns.map(str.strip) radon_mn = radon.assign(fips=radon.stfips*1000 + radon.cntyfips)[radon.state==&#39;MN&#39;] . radon_mn.county = radon_mn.county.str.strip() n_county = radon_mn.county.unique() county_lookup = dict(zip(mn_counties, range(len(mn_counties)))) county = radon_mn[&#39;county_code&#39;] = radon_mn.county.replace(county_lookup).values radon = radon_mn.activity log_radon = radon_mn[&#39;log_radon&#39;] floor_measure = radon_mn.floor.values u = np.log(radon_mn.Uppm) . radon_mn[&#39;fips&#39;] = radon_mn.stfips*1000 + radon_mn.cntyfips . radon_mn.activity.apply(lambda x: np.log(x+0.1)).hist(bins=25, figsize=(20, 10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a229295c0&gt; . The simplest qway to fit a mixed model is to use StatsModels: which is what we&#39;ll do! . data = radon_mn[[&#39;county&#39;, &#39;log_radon&#39;, &#39;floor&#39;]] . formula = &quot;log_radon ~ floor + county&quot; . md = smf.mixedlm(formula, data, groups=data[&quot;county&quot;]) mdf = md.fit() . /Users/thomas.kealy/anaconda3/lib/python3.7/site-packages/statsmodels/regression/mixed_linear_model.py:2066: ConvergenceWarning: The Hessian matrix at the estimated parameter values is not positive definite. warnings.warn(msg, ConvergenceWarning) . print(mdf.summary()) . Mixed Linear Model Regression Results ====================================================================== Model: MixedLM Dependent Variable: log_radon No. Observations: 919 Method: REML No. Groups: 85 Scale: 0.5279 Min. group size: 1 Likelihood: -994.6192 Max. group size: 116 Converged: Yes Mean group size: 10.8 - Coef. Std.Err. z P&gt;|z| [0.025 0.975] - Intercept 0.887 0.810 1.095 0.274 -0.701 2.475 county[T.ANOKA] 0.043 1.088 0.040 0.968 -2.090 2.177 county[T.BECKER] 0.662 1.167 0.568 0.570 -1.625 2.949 county[T.BELTRAMI] 0.700 1.123 0.623 0.533 -1.501 2.901 county[T.BENTON] 0.567 1.147 0.494 0.621 -1.682 2.816 county[T.BIG STONE] 0.650 1.167 0.557 0.578 -1.637 2.936 county[T.BLUE EARTH] 1.138 1.104 1.031 0.303 -1.026 3.301 county[T.BROWN] 1.109 1.148 0.966 0.334 -1.140 3.358 county[T.CARLTON] 0.158 1.113 0.142 0.887 -2.022 2.339 county[T.CARVER] 0.679 1.128 0.602 0.547 -1.533 2.890 county[T.CASS] 0.541 1.136 0.476 0.634 -1.685 2.767 county[T.CHIPPEWA] 0.869 1.148 0.757 0.449 -1.381 3.118 county[T.CHISAGO] 0.197 1.128 0.175 0.861 -2.013 2.408 county[T.CLAY] 1.119 1.104 1.014 0.311 -1.044 3.282 county[T.CLEARWATER] 0.481 1.148 0.419 0.675 -1.768 2.730 county[T.COOK] -0.170 1.204 -0.141 0.888 -2.529 2.189 county[T.COTTONWOOD] 0.380 1.148 0.331 0.741 -1.870 2.630 county[T.CROW WING] 0.273 1.108 0.246 0.805 -1.898 2.444 county[T.DAKOTA] 0.485 1.091 0.444 0.657 -1.654 2.623 county[T.DODGE] 0.930 1.167 0.797 0.426 -1.357 3.216 county[T.DOUGLAS] 0.863 1.115 0.774 0.439 -1.322 3.049 county[T.FARIBAULT] -0.109 1.128 -0.096 0.923 -2.320 2.102 county[T.FILLMORE] 0.532 1.204 0.442 0.658 -1.827 2.891 county[T.FREEBORN] 1.226 1.114 1.100 0.271 -0.958 3.409 county[T.GOODHUE] 1.078 1.104 0.976 0.329 -1.087 3.242 county[T.HENNEPIN] 0.506 1.040 0.486 0.627 -1.533 2.544 county[T.HOUSTON] 0.900 1.128 0.798 0.425 -1.311 3.111 county[T.HUBBARD] 0.389 1.136 0.342 0.732 -1.838 2.616 county[T.ISANTI] 0.204 1.167 0.175 0.861 -2.083 2.490 county[T.ITASCA] 0.083 1.111 0.074 0.941 -2.095 2.260 county[T.JACKSON] 1.149 1.136 1.012 0.312 -1.077 3.375 county[T.KANABEC] 0.382 1.148 0.333 0.739 -1.868 2.631 county[T.KANDIYOHI] 1.189 1.147 1.036 0.300 -1.060 3.437 county[T.KITTSON] 0.730 1.167 0.625 0.532 -1.557 3.017 county[T.KOOCHICHING] -0.018 1.123 -0.016 0.987 -2.218 2.182 county[T.LAC QUI PARLE] 2.064 1.204 1.714 0.086 -0.296 4.423 county[T.LAKE] -0.401 1.115 -0.359 0.719 -2.586 1.785 county[T.LAKE OF THE WOODS] 0.989 1.148 0.862 0.389 -1.260 3.238 county[T.LE SUEUR] 0.876 1.136 0.772 0.440 -1.349 3.102 county[T.LINCOLN] 1.435 1.147 1.250 0.211 -0.814 3.684 county[T.LYON] 1.092 1.119 0.975 0.329 -1.102 3.286 county[T.MAHNOMEN] 0.499 1.309 0.381 0.703 -2.066 3.064 county[T.MARSHALL] 0.751 1.115 0.674 0.500 -1.433 2.936 county[T.MARTIN] 0.220 1.123 0.196 0.845 -1.981 2.420 county[T.MCLEOD] 0.432 1.106 0.391 0.696 -1.735 2.599 county[T.MEEKER] 0.359 1.136 0.316 0.752 -1.868 2.586 county[T.MILLE LACS] 0.081 1.204 0.067 0.946 -2.278 2.440 county[T.MORRISON] 0.294 1.115 0.263 0.792 -1.891 2.478 county[T.MOWER] 0.840 1.105 0.760 0.447 -1.327 3.006 county[T.MURRAY] 1.614 1.309 1.233 0.217 -0.951 4.179 county[T.NICOLLET] 1.291 1.148 1.125 0.261 -0.959 3.540 county[T.NOBLES] 1.055 1.167 0.905 0.366 -1.231 3.342 county[T.NORMAN] 0.398 1.166 0.341 0.733 -1.889 2.684 county[T.OLMSTED] 0.454 1.091 0.416 0.677 -1.685 2.592 county[T.OTTER TAIL] 0.752 1.119 0.672 0.501 -1.441 2.945 county[T.PENNINGTON] 0.303 1.167 0.260 0.795 -1.983 2.590 county[T.PINE] -0.074 1.128 -0.066 0.947 -2.285 2.137 county[T.PIPESTONE] 0.991 1.147 0.863 0.388 -1.258 3.239 county[T.POLK] 0.852 1.148 0.743 0.458 -1.397 3.101 county[T.POPE] 0.420 1.204 0.349 0.727 -1.939 2.779 county[T.RAMSEY] 0.309 1.077 0.287 0.774 -1.802 2.421 county[T.REDWOOD] 1.111 1.136 0.978 0.328 -1.115 3.337 county[T.RENVILLE] 0.800 1.166 0.686 0.493 -1.486 3.086 county[T.RICE] 0.977 1.111 0.879 0.379 -1.201 3.155 county[T.ROCK] 0.448 1.204 0.372 0.710 -1.911 2.807 county[T.ROSEAU] 0.795 1.105 0.719 0.472 -1.371 2.961 county[T.SCOTT] 0.935 1.106 0.846 0.398 -1.232 3.102 county[T.SHERBURNE] 0.238 1.119 0.213 0.832 -1.956 2.431 county[T.SIBLEY] 0.388 1.148 0.338 0.736 -1.862 2.637 county[T.ST LOUIS] 0.036 0.857 0.042 0.967 -1.644 1.715 county[T.STEARNS] 0.631 1.089 0.579 0.562 -1.503 2.765 county[T.STEELE] 0.717 1.113 0.644 0.519 -1.464 2.897 county[T.STEVENS] 0.922 1.204 0.766 0.444 -1.437 3.281 county[T.SWIFT] 0.139 1.148 0.122 0.903 -2.110 2.389 county[T.TODD] 0.852 1.166 0.730 0.465 -1.434 3.138 county[T.TRAVERSE] 1.133 1.148 0.987 0.323 -1.116 3.382 county[T.WABASHA] 0.955 1.122 0.851 0.395 -1.244 3.155 county[T.WADENA] 0.434 1.136 0.382 0.702 -1.792 2.660 county[T.WASECA] -0.181 1.147 -0.158 0.875 -2.430 2.068 county[T.WASHINGTON] 0.476 1.095 0.435 0.664 -1.670 2.622 county[T.WATONWAN] 1.813 1.167 1.553 0.120 -0.475 4.100 county[T.WILKIN] 1.353 1.309 1.034 0.301 -1.212 3.919 county[T.WINONA] 0.769 1.105 0.696 0.487 -1.397 2.935 county[T.WRIGHT] 0.779 1.106 0.705 0.481 -1.388 2.947 county[T.YELLOW MEDICINE] 0.330 1.204 0.274 0.784 -2.030 2.689 floor -0.689 0.071 -9.760 0.000 -0.828 -0.551 Group Var 0.528 ====================================================================== . /Users/thomas.kealy/anaconda3/lib/python3.7/site-packages/statsmodels/base/model.py:1092: RuntimeWarning: invalid value encountered in sqrt bse_ = np.sqrt(np.diag(self.cov_params())) . fe_params = pd.DataFrame(mdf.fe_params, columns=[&#39;LMM&#39;]) random_effects = pd.DataFrame(mdf.random_effects) random_effects = random_effects.transpose() random_effects = random_effects.rename(index=str, columns={&#39;Group&#39;: &#39;LMM&#39;}) #%% Generate Design Matrix for later use Y, X = dmatrices(formula, data=data, return_type=&#39;matrix&#39;) Terms = X.design_info.column_names _, Z = dmatrices(&quot;log_radon ~ county&quot;, data=data, return_type=&#39;matrix&#39;) X = np.asarray(X) # fixed effect Z = np.asarray(Z) # mixed effect Y = np.asarray(Y).flatten() nfixed = np.shape(X) nrandm = np.shape(Z) . def plotfitted(fe_params=fe_params,random_effects=random_effects,X=X,Z=Z,Y=Y): plt.figure(figsize=(18,9)) ax1 = plt.subplot2grid((2,2), (0, 0)) ax2 = plt.subplot2grid((2,2), (0, 1)) ax3 = plt.subplot2grid((2,2), (1, 0), colspan=2) fe_params.plot(ax=ax1) random_effects.plot(ax=ax2) ax3.plot(Y.flatten(), &#39;o&#39;, color=&#39;k&#39;, label=&#39;Observed&#39;, alpha=.25) for iname in fe_params.columns.get_values(): fitted = np.dot(X, fe_params[iname]) + np.dot(Z, random_effects[iname]).flatten() print(&quot;The MSE of &quot;+ iname + &quot; is &quot; + str(np.mean(np.square(Y.flatten()-fitted)))) ax3.plot(fitted, lw=1, label=iname, alpha=.5) ax3.legend(loc=0) #plt.ylim([0,5]) plt.show() plotfitted(fe_params=fe_params, random_effects=random_effects, X=X, Z=Z, Y=Y) . The MSE of LMM is 0.4784635589205236 . xbar = radon_mn.groupby(&#39;county&#39;)[&#39;floor&#39;].mean().rename(county_lookup).values x_mean = xbar[county] . stan_code = &quot;&quot;&quot; data { int&lt;lower=0&gt; J; int&lt;lower=0&gt; N; int&lt;lower=1,upper=J&gt; county[N]; vector[N] u; vector[N] x; vector[N] x_mean; vector[N] y; } parameters { vector[J] a; vector[3] b; real mu_a; real&lt;lower=0,upper=100&gt; sigma_a; real&lt;lower=0,upper=100&gt; sigma_y; } transformed parameters { vector[N] y_hat; for (i in 1:N) y_hat[i] &lt;- a[county[i]] + u[i]*b[1] + x[i]*b[2] + x_mean[i]*b[3]; } model { mu_a ~ normal(0, 1); a ~ normal(mu_a, sigma_a); b ~ normal(0, 1); y ~ normal(y_hat, sigma_y); } &quot;&quot;&quot; . stan_datadict = {&#39;N&#39;: len(log_radon), &#39;J&#39;: len(n_county), &#39;county&#39;: county+1, # Stan counts starting at 1 &#39;u&#39;: u, &#39;x_mean&#39;: x_mean, &#39;x&#39;: floor_measure, &#39;y&#39;: log_radon} stan_datadict[&#39;prior_only&#39;] = 0 sm = pystan.StanModel(model_code=stan_code) fit = sm.sampling(data=stan_datadict, iter=1000) . INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_b7928e9396770558600f70afccda0012 NOW. /Users/thomas.kealy/anaconda3/lib/python3.7/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. elif np.issubdtype(np.asarray(v).dtype, float): . fit[&#39;b&#39;].mean(0) . array([ 0.68677081, -0.68513895, 0.3876573 ]) . with mpl.rc_context(): mpl.rc(&#39;figure&#39;, figsize=(30, 10)) fit.plot(&#39;b&#39;) .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/2021/01/01/Mixed-Models.html",
            "relUrl": "/2021/01/01/Mixed-Models.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://tomkealy.github.io/TomKealy.github.io/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://tomkealy.github.io/TomKealy.github.io/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://tomkealy.github.io/TomKealy.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tomkealy.github.io/TomKealy.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}