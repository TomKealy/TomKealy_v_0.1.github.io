<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Title | fastpages</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Title" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An easy to use blogging platform with support for Jupyter Notebooks." />
<meta property="og:description" content="An easy to use blogging platform with support for Jupyter Notebooks." />
<link rel="canonical" href="https://tomkealy.github.io/TomKealy.github.io/2021/06/21/Neural-Netwroks-From-Scratch.html" />
<meta property="og:url" content="https://tomkealy.github.io/TomKealy.github.io/2021/06/21/Neural-Netwroks-From-Scratch.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-21T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2021-06-21T00:00:00-05:00","url":"https://tomkealy.github.io/TomKealy.github.io/2021/06/21/Neural-Netwroks-From-Scratch.html","@type":"BlogPosting","headline":"Title","dateModified":"2021-06-21T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomkealy.github.io/TomKealy.github.io/2021/06/21/Neural-Netwroks-From-Scratch.html"},"description":"An easy to use blogging platform with support for Jupyter Notebooks.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/TomKealy.github.io/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://tomkealy.github.io/TomKealy.github.io/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/TomKealy.github.io/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/TomKealy.github.io/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/TomKealy.github.io/about/">About Me</a><a class="page-link" href="/TomKealy.github.io/search/">Search</a><a class="page-link" href="/TomKealy.github.io/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Title</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-06-21T00:00:00-05:00" itemprop="datePublished">
        Jun 21, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/TomKealy/TomKealy.github.io/tree/master/_notebooks/2021-06-21-Neural-Netwroks-From-Scratch.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/TomKealy.github.io/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/TomKealy/TomKealy.github.io/master?filepath=_notebooks%2F2021-06-21-Neural-Netwroks-From-Scratch.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/TomKealy.github.io/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/TomKealy/TomKealy.github.io/blob/master/_notebooks/2021-06-21-Neural-Netwroks-From-Scratch.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/TomKealy.github.io/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-06-21-Neural-Netwroks-From-Scratch.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's useful sometimes to write simple versions of complex things, so that you understand them. In this post we write a simple neural network from scratch.</p>
<p>In a normal classification problem, we have some labels (y) and inputs (x) and we would like to learn a linear function</p>
$$ y = W x $$<p></p>
<p>to separate the classes. Neural networks add an (or many!) extra layer</p>
<p>
$$ h = \mathrm{sigmoid}(M x) $$
</p>
<p>between the inputs and output so that it produces is</p>
<p>
$$ y = W h $$
</p>
<p>Thus we are esentially fitting a linear classifier on the basis expansion (\mathrm{sigmoid}(M x)), the difference being that w efit the basis expansion, as well as the linear classifier. That is the Network learns a data dependent basis on which to clssify.</p>
<p>Enough with the maths, lets do some coding.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Neural networks are made up of Layers, the simplest just returns what it recieves as input.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A building block. Each layer is capable of performing two things:</span>
<span class="sd">    </span>
<span class="sd">    - Process input to get output:           output = layer.forward(input)</span>
<span class="sd">    </span>
<span class="sd">    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)</span>
<span class="sd">    </span>
<span class="sd">    Some layers also have learnable parameters which they update during layer.backward.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;This is an identity layer so it doesn&#39;t need to do anything.&quot;&quot;&quot;</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input : Tensor of shape [batch_size, num_input_units]</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        output: Tensor of shape [batch_size, num_output_units]</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">input</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs a backpropagation step through the layer, with respect to the given input.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input : Tensor of shape [batch_size, num_input_units]</span>
<span class="sd">        </span>
<span class="sd">        grad_output : Tensor of shape  [batch_size, num_input_units]</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        </span>
<span class="sd">        grad_output : Tensor of shape [batch_size, num_output_units]</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">num_units</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="n">d_layer_d_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">num_units</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">d_layer_d_input</span><span class="p">)</span> <span class="c1"># chain rule</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lets add some non-linearity layers: a ReLU layer, and a Sigmoid layer</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;ReLU layer simply applies elementwise rectified linear unit to all inputs&quot;&quot;&quot;</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply elementwise ReLU to [batch, input_units] matrix</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input : Tensor of shape [batch_size, num_input_units]</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        output: Tensor of shape [batch_size, num_output_units]</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
        
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute gradient of loss w.r.t. ReLU input</span>
<span class="sd">                </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input : Tensor of shape [batch_size, num_input_units]</span>
<span class="sd">        </span>
<span class="sd">        grad_output : Tensor of shape  [batch_size, num_input_units]</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        </span>
<span class="sd">        grad_output : Tensor of shape [batch_size, num_output_units]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relu_grad</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">grad_output</span><span class="o">*</span><span class="n">relu_grad</span>        
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sigmoid layer simply applies elementwise sigmoid unit to all inputs&quot;&quot;&quot;</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply elementwise ReLU to [batch, input_units] matrix</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input : Tensor of shape [batch_size, num_input_units]</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        output: Tensor of shape [batch_size, num_output_units]</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute gradient of loss w.r.t. ReLU input</span>
<span class="sd">                </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input : Tensor of shape [batch_size, num_input_units]</span>
<span class="sd">        </span>
<span class="sd">        grad_output : Tensor of shape  [batch_size, num_input_units]</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        </span>
<span class="sd">        grad_output : Tensor of shape [batch_size, num_output_units]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sigmoid_grad</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_output</span><span class="o">*</span><span class="n">sigmoid_grad</span>        
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can test this by evaluating the numerical gradients:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">eval_numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluates gradient df/dx via finite differences:</span>
<span class="sd">    df/dx ~ (f(x+h) - f(x-h)) / 2h</span>
<span class="sd">    Adopted from https://github.com/ddtm/dl-course/ (our ysda course).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fx</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># evaluate function value at original point</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># iterate over all indexes in x</span>
    <span class="n">it</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;multi_index&#39;</span><span class="p">],</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;readwrite&#39;</span><span class="p">])</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>

        <span class="c1"># evaluate function at x+h</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="n">it</span><span class="o">.</span><span class="n">multi_index</span>
        <span class="n">oldval</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
        <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">oldval</span> <span class="o">+</span> <span class="n">h</span> <span class="c1"># increment by h</span>
        <span class="n">fxph</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># evalute f(x + h)</span>
        <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">oldval</span> <span class="o">-</span> <span class="n">h</span>
        <span class="n">fxmh</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># evaluate f(x - h)</span>
        <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">oldval</span> <span class="c1"># restore</span>

        <span class="c1"># compute the partial derivative with centered formula</span>
        <span class="n">grad</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxph</span> <span class="o">-</span> <span class="n">fxmh</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span> <span class="c1"># the slope</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span> <span class="p">(</span><span class="n">ix</span><span class="p">,</span> <span class="n">grad</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
        <span class="n">it</span><span class="o">.</span><span class="n">iternext</span><span class="p">()</span> <span class="c1"># step to next dimension</span>

    <span class="k">return</span> <span class="n">grad</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="o">*</span><span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">32</span><span class="p">])</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>
<span class="n">grads</span> <span class="o">=</span> <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">32</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="mi">32</span><span class="o">*</span><span class="mi">10</span><span class="p">))</span>
<span class="n">numeric_grads</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">l</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">numeric_grads</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>\
    <span class="s2">&quot;gradient returned by your layer does not match the numerically computed gradient&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="o">*</span><span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">32</span><span class="p">])</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">Sigmoid</span><span class="p">()</span>
<span class="n">grads</span> <span class="o">=</span> <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">32</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="mi">32</span><span class="o">*</span><span class="mi">10</span><span class="p">))</span>
<span class="n">numeric_grads</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">l</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">numeric_grads</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>\
    <span class="s2">&quot;gradient returned by your layer does not match the numerically computed gradient&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The next type of layer we will implement will be a Dense or Fully Connected layer. Unlike nonlinearity, this layer actually has something to learn.</p>
<p>A dense layer applies affine transformation. In a vectorized form, it can be described as:

$$f(X)= W \cdot X + \vec b $$
</p>
<p>Where</p>
<ul>
<li>X is an object-feature matrix of shape [batch_size, num_features],</li>
<li>W is a weight matrix [num_features, num_outputs] </li>
<li>and b is a vector of num_outputs biases.</li>
</ul>
<p>Both W and b are initialized during layer creation and updated each time backward is called.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_units</span><span class="p">,</span> <span class="n">output_units</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A dense layer is a layer which performs a learned affine transformation:</span>
<span class="sd">        f(x) = &lt;W*x&gt; + b</span>
<span class="sd">        </span>
<span class="sd">        Weights initialised by Xavier initialisation: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_units</span><span class="p">,</span> <span class="n">output_units</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="o">/</span><span class="p">(</span><span class="n">input_units</span><span class="o">+</span><span class="n">output_units</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_units</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="nb">input</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform an affine transformation:</span>
<span class="sd">        f(x) = &lt;W*x&gt; + b</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input : Tensor of shape [batch_size, num_input_units]</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        output: Tensor of shape [batch_size, num_output_units]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">input</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>  
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input : Tensor of shape [batch_size, num_input_units]</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        grad_output: Tensor of shape [batch_size, num_output_units]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">T</span>
        
        <span class="n">grad_weights</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">grad_output</span><span class="p">)</span>
        <span class="n">grad_biases</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  
        <span class="k">assert</span> <span class="n">grad_weights</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span> <span class="ow">and</span> <span class="n">grad_biases</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">shape</span>
    
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_biases</span>
        
        <span class="k">return</span> <span class="n">grad_input</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, some tests:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">l</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">150</span><span class="p">)</span>

<span class="k">assert</span> <span class="o">-</span><span class="mf">0.05</span> <span class="o">&lt;</span> <span class="n">l</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.05</span> <span class="ow">and</span> <span class="mf">1e-3</span> <span class="o">&lt;</span> <span class="n">l</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">1e-1</span><span class="p">,</span>\
    <span class="s2">&quot;The initial weights must have zero mean and small variance. &quot;</span>\
    <span class="s2">&quot;If you know what you&#39;re doing, remove this assertion.&quot;</span>
<span class="k">assert</span> <span class="o">-</span><span class="mf">0.05</span> <span class="o">&lt;</span> <span class="n">l</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">,</span> <span class="s2">&quot;Biases must be zero mean. Ignore if you have a reason to do otherwise.&quot;</span>

<span class="c1"># To test the outputs, we explicitly set weights with fixed values. DO NOT DO THAT IN ACTUAL NETWORK!</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">l</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">l</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.07272727</span><span class="p">,</span>  <span class="mf">0.41212121</span><span class="p">,</span>  <span class="mf">0.75151515</span><span class="p">,</span>  <span class="mf">1.09090909</span><span class="p">],</span>
                                          <span class="p">[</span><span class="o">-</span><span class="mf">0.90909091</span><span class="p">,</span>  <span class="mf">0.08484848</span><span class="p">,</span>  <span class="mf">1.07878788</span><span class="p">,</span>  <span class="mf">2.07272727</span><span class="p">]]))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="o">*</span><span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">32</span><span class="p">])</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">numeric_grads</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">l</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span><span class="n">x</span><span class="p">)</span>
<span class="n">grads</span> <span class="o">=</span> <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">64</span><span class="p">]))</span>

<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span><span class="n">numeric_grads</span><span class="p">,</span><span class="n">rtol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span><span class="n">atol</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="s2">&quot;input gradient does not match numeric grad&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">compute_out_given_wb</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="o">*</span><span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">32</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">l</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">compute_grad_by_params</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="o">*</span><span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">32</span><span class="p">])</span>
    <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">64</span><span class="p">])</span> <span class="o">/</span> <span class="mf">10.</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w</span> <span class="o">-</span> <span class="n">l</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">b</span> <span class="o">-</span> <span class="n">l</span><span class="o">.</span><span class="n">biases</span>
    
<span class="n">w</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">)</span>

<span class="n">numeric_dw</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">compute_out_given_wb</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span><span class="n">w</span> <span class="p">)</span>
<span class="n">numeric_db</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="k">lambda</span> <span class="n">b</span><span class="p">:</span> <span class="n">compute_out_given_wb</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span><span class="n">b</span> <span class="p">)</span>
<span class="n">grad_w</span><span class="p">,</span><span class="n">grad_b</span> <span class="o">=</span> <span class="n">compute_grad_by_params</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">numeric_dw</span><span class="p">,</span><span class="n">grad_w</span><span class="p">,</span><span class="n">rtol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span><span class="n">atol</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="s2">&quot;weight gradient does not match numeric weight gradient&quot;</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">numeric_db</span><span class="p">,</span><span class="n">grad_b</span><span class="p">,</span><span class="n">rtol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span><span class="n">atol</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="s2">&quot;weight gradient does not match numeric weight gradient&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will optimise the following loss, which is a more numerically stable version of logg loss (courtesy of Coursera advanced ML):</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">softmax_crossentropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">reference_answers</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute crossentropy from logits[batch,n_classes] and ids of correct answers&quot;&quot;&quot;</span>
    <span class="n">logits_for_answers</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="p">)),</span><span class="n">reference_answers</span><span class="p">]</span>
    
    <span class="n">xentropy</span> <span class="o">=</span> <span class="o">-</span> <span class="n">logits_for_answers</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">xentropy</span>

<span class="k">def</span> <span class="nf">grad_softmax_crossentropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">reference_answers</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers&quot;&quot;&quot;</span>
    <span class="n">ones_for_answers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">ones_for_answers</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="p">)),</span><span class="n">reference_answers</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    
    <span class="n">softmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span><span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">(</span><span class="o">-</span> <span class="n">ones_for_answers</span> <span class="o">+</span> <span class="n">softmax</span><span class="p">)</span> <span class="o">/</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">500</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">50</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
<span class="n">answers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span><span class="o">%</span><span class="k">10</span>

<span class="n">softmax_crossentropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="n">answers</span><span class="p">)</span>
<span class="n">grads</span> <span class="o">=</span> <span class="n">grad_softmax_crossentropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="n">answers</span><span class="p">)</span>
<span class="n">numeric_grads</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="n">softmax_crossentropy_with_logits</span><span class="p">(</span><span class="n">l</span><span class="p">,</span><span class="n">answers</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span><span class="n">logits</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">numeric_grads</span><span class="p">,</span><span class="n">grads</span><span class="p">,</span><span class="n">rtol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span><span class="n">atol</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll use the following function to load the mnist dataset:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="n">flatten</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">keras</span>
    <span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

    <span class="c1"># normalize x</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>

    <span class="c1"># we reserve the last 10000 training examples for validation</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:</span><span class="o">-</span><span class="mi">10000</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="o">-</span><span class="mi">10000</span><span class="p">:]</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[:</span><span class="o">-</span><span class="mi">10000</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="o">-</span><span class="mi">10000</span><span class="p">:]</span>

    <span class="k">if</span> <span class="n">flatten</span><span class="p">:</span>
        <span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">X_val</span> <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">flatten</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/thomas.kealy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">network</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">network</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">100</span><span class="p">))</span>
        <span class="n">network</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="n">network</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">))</span>
        <span class="n">network</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="n">network</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        comppute activations of all network layers by applying them sequentially.</span>
<span class="sd">        Return a list of activations for each layer. </span>
<span class="sd">        Make sure last activation corresponds to network logits.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">X</span>

        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">:</span>
            <span class="n">activations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">activations</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">activations</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute network predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Train your network on a given batch of X and y.</span>
<span class="sd">        You first need to run forward to get all layer activations.</span>
<span class="sd">        Then you can run layer.backward going from last to first layer.</span>
<span class="sd">    </span>
<span class="sd">        After you called backward for all layers, all Dense layers have already made one gradient step.</span>
<span class="sd">        &quot;&quot;&quot;</span>
    
        <span class="c1"># Get the layer activations</span>
        <span class="n">layer_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">layer_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="p">]</span><span class="o">+</span><span class="n">layer_activations</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">layer_activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
        <span class="c1"># Compute the loss and the initial gradient</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">softmax_crossentropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">loss_grad</span> <span class="o">=</span> <span class="n">grad_softmax_crossentropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">layer_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">))[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">[</span><span class="n">layer_i</span><span class="p">]</span>
        
            <span class="n">loss_grad</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">layer_inputs</span><span class="p">[</span><span class="n">layer_i</span><span class="p">],</span><span class="n">loss_grad</span><span class="p">)</span> 
            
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we can train our nework!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">trange</span>
<span class="k">def</span> <span class="nf">iterate_minibatches</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">start_idx</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">-</span> <span class="n">batchsize</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
            <span class="n">excerpt</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">start_idx</span> <span class="o">+</span> <span class="n">batchsize</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">excerpt</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">start_idx</span><span class="p">,</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="n">batchsize</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">inputs</span><span class="p">[</span><span class="n">excerpt</span><span class="p">],</span> <span class="n">targets</span><span class="p">[</span><span class="n">excerpt</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">network</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
<span class="n">train_log</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_log</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">25</span><span class="p">):</span>

    <span class="k">for</span> <span class="n">x_batch</span><span class="p">,</span><span class="n">y_batch</span> <span class="ow">in</span> <span class="n">iterate_minibatches</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">batchsize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">network</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span><span class="n">y_batch</span><span class="p">)</span>
    
    <span class="n">train_log</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="o">==</span><span class="n">y_train</span><span class="p">))</span>
    <span class="n">val_log</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span><span class="o">==</span><span class="n">y_val</span><span class="p">))</span>
    
    <span class="n">clear_output</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">,</span><span class="n">epoch</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train accuracy:&quot;</span><span class="p">,</span><span class="n">train_log</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Val accuracy:&quot;</span><span class="p">,</span><span class="n">val_log</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_log</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;train accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_log</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;val accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch 24
Train accuracy: 1.0
Val accuracy: 0.9819
</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl8VNX5+PHPyWRPIGSBAGEJYZUdwo7a4ApqFVkE3OnXtWrt19q61Fat+tWf1da61aJFpS6IWFwRFE1AAWVHAgRIWCSEkA3IvszM+f1xJyGEJDMJsyRzn/frNa+ZuetzcuG5d8499xyltUYIIYR5BPg6ACGEEN4liV8IIUxGEr8QQpiMJH4hhDAZSfxCCGEykviFEMJkJPELIYTJSOIXQgiTkcQvhBAmE+jrABqKi4vTiYmJrV6/rKyMiIgI9wXUjkjZzVl2MHf5zVx2OFX+zZs3F2itO7uyTptL/ImJiWzatKnV66elpZGSkuK+gNoRKXuKr8PwGTOX38xlh1PlV0odcnUdqeoRQgiTkcQvhBAmI4lfCCFMps3V8TempqaG7OxsKisrnS4bFRXF7t27vRBV29MWyh4aGkqPHj0ICgryaRxCiKa1i8SfnZ1Nhw4dSExMRCnV7LIlJSV06NDBS5G1Lb4uu9aawsJCsrOz6dOnj8/iEEI0z2lVj1JqoVIqTymV3sR8pZR6USmVqZT6SSk1ut68m5RS+xyvm1obZGVlJbGxsU6TvvAtpRSxsbEu/TITQviOK3X8bwFTm5k/DejveN0G/BNAKRUDPAqMB8YBjyqlolsbqCT99kGOkxBtn9OqHq31GqVUYjOLXAUs0sYYjj8opToppboBKcDXWusiAKXU1xgnkPfPNmghxCknyqvJK7ezP78Um11jtWtsjpe17t1e993umBYaZCEixEJYUCARIRbCgwMJD7YQFmQhIODsTuBaG/uosWmqbXZqbHasNk2NzV73vcZqzLPa7NTUm3cqbmOd08thTLfZMebbNfsPVLOleo+b/pq+1TUqjGvH9/L4ftxRx58AHK73PdsxranpZ1BK3Ybxa4H4+HjS0tJOmx8VFUVJSYlLwdhsNpeXddWJEyf48MMPufXWW1u87syZM/n3v/9Np06d3BpTYzxR9taorKw84xh6Wmlpqdf36U1WuyavXHO0zE5umZ3cslOfS2scC61Z7bb9hVhqX4rQQEWIBQIDwGYHq65918a7Y5qRlE/N9+Zo3ior04t785ykqAC6V+xv0Tqt+bfvjsTf2KWBbmb6mRO1XgAsABgzZoxu+BTe7t27Xb5p6YkbnIWFhSxcuJD77rvvjHk2mw2LxdLkul999ZVbY2lOS8qutUZrTUCA+1v0hoaGMmrUKLdvtzlt7enN8mory7Ye4csduQCEB1uICAkkLNhCRPCpq+vwkEDCHVfetdNqbJr9BaXszy9jf34p+wvKOFxUjr3e/564yBCSOndk7IAIkuIiyTucxZDBg7EEKAIDlPFuUVgCAk59D1AE1L4rRWWNjfJqG+XVVsqqbJTX2CivslJWbaOi2ngvr7I6lrFRbbUTFKgIsgQQGBBAsOPzqdep78EWRaAlgOBAY//BgacvF2wJILDe5yDH/MAAYxtnlkMRGGBMrz/PohRr1qxuU8fe21rzb98diT8b6Fnvew8gxzE9pcH0NDfsz+sefPBBsrKyGDlyJBdffDGXX345jz/+ON26dWPbtm3s2rWL6dOnc/jwYSorK7n33nu57bbbgFNdUJSWljJt2jTOPfdc1q1bR0JCAp988glhYWGn7euzzz7jySefpLq6mtjYWN59913i4+MpLS3lnnvuYdOmTSilePTRR5k5cyYrVqzg4YcfxmazER0dTVpaGo899hiRkZHcf//9AAwdOpTPP/8cgGnTpjFlyhTWr1/Pxx9/zDPPPMPGjRupqKhg1qxZPP744wBs3LiRe++9l7KyMkJCQvjmm2+47LLLeOmllxg5ciQAkydP5p///CfDhw/31qFo834uLOc/Pxzkg42HKa600q9LJB1DAykoraKs2kpFtY2yKhsVNTan2woJDKBPXARDu0dx5YjuJHWOoE9cJEmdI+gYenpz2bS0n0kZ1egPaiHO4I7E/ylwt1JqMcaN3JNa66NKqZXA/9W7oXsJ8NDZ7uzxz3ayK6e4yfnOrsAbM7h7Rx795ZAm5z/zzDOkp6ezbds2wDjDbtiwgfT09LpmiwsXLiQmJoaKigrGjh3LzJkziY2NPW07+/bt4/333+f111/nmmuu4aOPPuL6668/bZlzzz2XH374AaUUb7zxBs8++yzPP/88TzzxBFFRUezYsQOA48ePk5+fz6233sqaNWvo06cPhw4576pjz549vPnmm7z66qsAPPXUU8TExGCz2bjwwgv56aefGDRoEHPmzOGDDz5g7NixFBcXExYWxi233MJbb73FCy+8wN69e6mqqpKkj/Hr6fvMAt5ed5BvMvIIUIqpQ7ty86RExvSObvSGt92uqaixUVZtpbyq3pV3tY0ABX3iIugeFXbWde1CNMZp4ldKvY9x5R6nlMrGaKkTBKC1fg1YDlwGZALlwHzHvCKl1BPARsem/lJ7o9cfjBs37rS26i+++CLLli0D4PDhw+zbt++MxN+nT5+6q+Xk5GQOHjx4xnazs7OZM2cOR48epbq6um4fq1atYvHixXXLRUdH89lnn3H++efXLRMTE+M07t69ezNhwoS670uWLGHBggVYrVaOHj3Krl27UErRrVs3xo4dC0DHjh0BmD17Nk888QR//etfWbhwITfffLPT/fmz0ior/92SzdvrDpKVX0ZsRDB3T+nHdeN70zUqtNl1AwIUESGBRIQEgjkfOxE+5EqrnnlO5mvgribmLQQWti60xjV3ZQ7ee4ipfjewaWlprFq1ivXr1xMeHk5KSkqjbdlDQkLqPlssFioqKs5Y5p577uG+++7jyiuvrKu2AeOqsuGVY2PTAAIDA7Hb7XXf68dSP+4DBw7w3HPPsXHjRqKjo7n55puprKxscrvh4eFcfPHFfPLJJyxZsuSselFtzw4UlPH2uoN8tDmbkiorI3pE8bdrRnD58G6EBLbs16YQviB99bigQ4cOzbaWOXnyJNHR0YSHh5ORkcEPP/zQ6n2dPHmShASjrvbtt9+um37JJZfw8ssv130/fvw4EydOZPXq1Rw4cACAoiLjB1ViYiJbtmwBYMuWLXXzGyouLiYiIoKoqCiOHTvGl19+CcCgQYPIyclh40bjx1pJSQlWqxWAW265hd/85jeMHTvWpV8Y/kJrzZq9+dz85gamPJfGuz8e4sJzurDs15P45O5zmTG6hyR90W60iy4bfC02NpbJkyczdOhQpk2bxuWXX37a/KlTp/Laa68xfPhwBg4ceFpVSks99thjzJ49m4SEBCZMmFCXtB955BHuuusuhg4disVi4dFHH2XGjBksWLCAGTNmYLfbiY2N5dtvv2XmzJksWrSIkSNHMnbsWAYMGNDovkaMGMGoUaMYMmQISUlJTJ48GYDg4GA++OAD7rnnHioqKggLC2PVqlVERkaSnJxMx44dmT9/fqvL2J5orVm9N58XVu1j2+ETdOkQwv9eNIB543vSpUPz1TlCtFXKqKlpO8aMGaMbViHs3r2bc845x6X1fd1fjS95o+w5OTmkpKSQkZHRZFPQlhwvd3F3c06tNWv2FfDCqr1s/fkECZ3CuPuCfswc3YPgwLb3Q7mtNWf1JjOXHU4biGWz1nqMK+vIFb9w2aJFi/jjH//I3/72N4+0/28LtNZ850j4WxwJ//+uHsas5LaZ8IVoDUn8wmU33ngjN954o6/D8IjaJpkvrNrH5kPH6R4VylNXD2V2ck9J+MLvSOIX7d7Gg0U8+F058enfk9Q5kqS4COO9cwR94iIIDWr6pqvWmrWZhbywai+bDh2nW1QoT04fyuwxcrNW+C9J/KJdO1ZcyZ3vbEHbIDI0kB/2F7Js65G6+UpB96gwkjpH0NdxMkhyPP16sKCMv6/ay8aDx+naMZQnrhrCNWN7SsIXfk8Sv2i3qq127nxnM+XVVv44NpTrfmm0piqvthr93BQ4+rrJL2N/QSkfbjpMWfXpXSXEdwzhL1cNYY4kfGEikvhFu/XkF7vY8vMJXrl2NBFFp7rlDQ8OZGhCFEMTok5bXmvNseKqug7Qgi0BXDmye7NVQUL4I0n8HhIZGUlpaamvw/BbSzdns2j9IW47P4nLh3cjLc15f+xKKbpGhdI1KpRJfeO8EKUQbZM0V/BTtU/a+qP0Iyf547IdTEyK5Q+XDvR1OEK0O5L4XfDAAw/U9WYJxtO1zz//PKWlpVx44YWMHj2aYcOG8cknnzjd1vTp00lOTmbIkCEsWLCgbvqKFSsYPXo0I0aM4MILLwSMARbmz5/PsGHDGD58OB999BFg/JqotXTp0rrO0u644w7uu+8+pkyZwgMPPMCGDRuYNGkSo0aNYtKkSezZY1wV22w27r///rrtvvTSS3zzzTdcffXVddv9+uuvmTFjRuv/aB5yvKyaO97ZTExEMC9dO4pAi/wTFqKl2l9Vz5cPQu6OJmeH2axgaWGxug6Dac80OXvu3Ln89re/5de//jVg9Gi5YsUKQkNDWbZsGR07dqSgoIAJEyZw5ZVXNjvubGPdN9vt9tO6V67tc6exrpid2bt3L6tWrcJisVBcXMyaNWsIDAxk1apVPPzww3z00UcsWLCAAwcOsHXrVgIDAykqKiI6Opq77rqL/Px8OnfuzJtvvtnmumWw2TW/WbyVvOIqltwxkbjIEOcrCSHO0P4Svw+MGjWKvLw8cnJyyM/PJzo6ml69elFTU8PDDz/MmjVrCAgI4MiRIxw7doyuXbs2ua3Gum/Oz89vtHvlxrpidmb27Nl14xGcPHmSm266iX379qGUoqampm67d9xxB4GBgaft74YbbuCdd95h/vz5rF+/nkWLFrX0T+VRf/96L9/tK+DpGcMY2dPzQ1kK4a/aX+Jv5socoMJD/dXMmjWLpUuXkpuby9y5cwF49913yc/PZ/PmzQQFBZGYmNhod8y1muq+ualukJuaXn9aw/3V73b5T3/6E1OmTGHZsmUcPHiwrj+TprY7f/58fvnLXxIaGsrs2bPrTgxtwVc7c3k5NZO5Y3syb5znB6MWwp9JBamL5s6dy+LFi1m6dCmzZs0CjCvqLl26EBQURGpqqtMRsJrqvrmp7pUb64oZjAHpd+/ejd1ur/v10NT+art4fuutt+qmX3LJJbz22mt1N4Br99e9e3e6d+/Ok08+2aYGWcnKL+W+JdsZ3iOKx65sfjwGIYRzkvhdNGTIEEpKSkhISKBbt24AXHfddWzatIkxY8bw7rvvMmjQoGa3MXXqVKxWK8OHD+dPf/pTXffNnTt3ruteecSIEcyZMwcwumI+fvw4Q4cOZcSIEaSmpgLGUJBXXHEFF1xwQV0sjfnDH/7AQw89xOTJk7HZTj24dMstt9CrVy+GDx/OiBEjeO+99+rmXXfddfTs2ZPBgwe37g/lZmVVVu74z2aCAwP45/XJ0uZeCHfQWrepV3Jysm5o165dZ0xrSnFxscvL+ht3lP2uu+7Sb7zxxlltoyXHqzl2u13/+p3Nus+Dn+u1+/KbXTY1NdUt+2yvzFx+M5dd61PlBzZpF/Ns26nEFT6XnJxMREQEzz//vK9DAeD17/bzxY6jPDRtEJP6yQNXQriLJH5RZ/Pmzb4Ooc66zAKe+TKDy4Z15bbzk3wdjhB+pd3U8es2NlKYaJw7jlPOiQrufn8rSZ0jeXbWiGafixBCtFy7SPyhoaEUFhZK8m/jtNYUFhYSGtr6sWhrbHZ+/e4Wqq12/nVDMpEh8qNUCHdrF/+revToQXZ2Nvn5+U6XraysPKvE0561hbKHhobSo0ePVq//r9VZbDts9LjZt3Ok8xWEEC3WLhJ/UFBQ3VOtzqSlpTFq1CgPR9Q2ebLsNTY7b609yGc/5fC3a0bSr4v7k3JGbjH/+GYfVwzvxuXDm26mKoQ4O+2iqkf41oYDRVzx4vc8tXw3u48Wc/d7W6issTlfsQVqbHbu/3A7UWFB/OWqoW7dthDidJL4RZMKSqv43ZLtXPOv9ZRWWVlwQzILbhxDRm4JT3y+y637ei0ti/QjxTw5fSgxEcFu3bYQ4nTtoqpHeJfNrnl/w888uyKDihobv07py90X9CM82PjncvsvkvjX6v1M7BvLFcO7n/X+MnKLefFbo4pn6lCp4hHC0yTxi9PsyD7JIx/vYHv2SSYmxfLE9CH063J6p3f3XzKQDQeKePCjHQxLiKJ3bEQTW3NOqniE8D6p6hEAnKyo4c+fpHPlK99z5EQl/5g7kvduHX9G0gcIsgTw0rxRBCi4+72tVFlbX98vVTxCeJ8kfpPTWvPfLdlc+Hwa7/xwiJsmJvLt/b/gqpEJzT441SM6nL/OHsGOIyd55suMVu1791GjiueXI7pLFY8QXiRVPSa271gJj3yczo8HihjZsxNvzR/H0IQol9e/dEhX5k9O5M21B5mQFMulQ5oegKah+lU8j0tXy0J4lSR+k9p9tJgZr64jODCAp2cMY86YngQEtLxrhAenDWLTweP8/sPtDOnekR7R4S6t98+0LHbmFPPa9aOlikcIL5OqHhM6WVHDHe9spkNoICt/ez7zxvVqVdIHCAm08PK1o7Br+M37W6mx2Z2us/toMS9JFY8QPiOJ32Tsds3vlmzjyPEKXr1uNF2jzr6Lh96xETwzcxhbfj7B81/tbXZZqeIRwvck8ZvMq2mZrNqdxyOXn8OYxBi3bfeK4d25dnwvXludReqevCaXq63ieXL6MKniEcJHJPGbyOq9+Tz/9V6mj+zOTZMS3b79P18xmEFdO/C7JdvJPXnmoPO1VTxXjujO1KGu3wgWQriXJH6TOFxUzr2LtzIwvgP/N2OYR/q4Dw2y8PK1o6motvGbxVux1qvvr1/FIwOmC+FbkvhNoLLGxp3vbsZm17x2fXJd1wue0K9LJE9OH8qGA0W8+G1m3fRXU6WKR4i2Qppz+jmtNX/+JJ30I8W8ceMYEuNa372Cq2Ym92BdViEvfbuPCX1i6BQeLFU8QrQhLl3xK6WmKqX2KKUylVIPNjK/t1LqG6XUT0qpNKVUj3rz/p9SKt3xmuPO4IVzizceZsmmbO65oB8XDY732n6fmD6EpLgI7v1gG/ct2Uan8GBpxSNEG+E08SulLMArwDRgMDBPKTW4wWLPAYu01sOBvwBPO9a9HBgNjATGA79XSnV0X/iiOdsPn+DRT3ZyXv84fnvRAK/uOzw4kFeuG01xRQ0ZuSU8dfVQoqWKR4g2wZWqnnFAptZ6P4BSajFwFVC/Q/bBwP86PqcCH9ebvlprbQWsSqntwFRgiRtiF80oLK3iznc207lDCC/OHYWllQ9onY1BXTvy6nWj2Z9f1qLuHIQQnqWcDWCulJoFTNVa3+L4fgMwXmt9d71l3gN+1Fr/Qyk1A/gIiAOSgUeBi4FwYAPwitb6+Qb7uA24DSA+Pj558eLFrS5QaWkpkZHmHKu1tux2rXluUyV7j9t5ZHwoiVEWX4fmcWY+7mDu8pu57HCq/FOmTNmstR7jyjquXPE3dqnY8GxxP/CyUupmYA1wBLBqrb9SSo0F1gH5wHrAesbGtF4ALAAYM2aMTklJcSX2RqWlpXE267dntWV/dkUGuwqzeHbmcK4Z29PXYXmFmY87mLv8Zi47tK78rtzczQbqZ48eQE79BbTWOVrrGVrrUcAfHdNOOt6f0lqP1FpfjHES2deiCE2otMrKf9Yf5Lt9+Rwvq27Ruit35vJqWhbzxvU0TdIXQrSMK1f8G4H+Sqk+GFfyc4Fr6y+glIoDirTWduAhYKFjugXopLUuVEoNB4YDX7kxfr+0aP1Bnl2xp+57QqcwhiVEMTShI0MTohiWEEVsZMgZ6+WW2XkydTsjekTJQ1JCiCY5Tfxaa6tS6m5gJWABFmqtdyql/gJs0lp/CqQATyulNEZVz12O1YOA7xxPiRYD1ztu9IpmrEzPZUj3jjx82TnsOHKSdMdrxc7cumW6RYUyNCGKod2jGNajI/06d+ClrZUEWQJ59fpkQgL9v15fCNE6Lj3ApbVeDixvMO3P9T4vBZY2sl4lRsse4aIjJyrYnn2SP0wdyOR+cUzuF1c3r7iyhp1Hio0TQc5Jdhw5yardx6i9P6+A//zPaBI6hfkmeCFEuyBP7rYxXzmu6qc20vyxY2gQE/vGMrFvbN200ioru3KK2XHkJCeOZHFu/7gz1hNCiPok8bcxK9JzGRAfSVJn15qnRYYEMq5PDOP6xJCWdsjD0Qkh/IF00taGFJRWsfFgUaNX+0II4S6S+NuQVbuOYddwqXRkJoTwIEn8bciKnbn0jAljcDfpzkgI4TmS+NuI4soa1mYWMHVIV48MkiKEELUk8bcRqRl51Ni09FcvhPA4SfxtxIr0XDp3CGFUz2hfhyKE8HOS+NuAimobaXvyuXRIPAE+6D5ZCGEukvjbgDX78qmosTF1SDdfhyKEMAFJ/G3AivRcosKCGJ8U4+tQhBAmIInfx6qtdlbtPsbFg+MJssjhEEJ4nmQaH1u/v5CSSqs8rSuE8BpJ/D62Ij2X8GCLdK4mhPAaSfw+ZLNrvt6Vy5RBXQgNkv7zhRDeIYnfhzYfOk5BabVU8wghvEoSvw+tSM8l2BLAlEFdfB2KEMJEJPH7iNaalTtzOa9/HJEhMiyCEMJ7JPH7SPqRYo6cqJAumIUQXieJ30dW7DyKJUBx0Tnxvg5FCGEykvh9ZEV6LuP7xBATEezrUIQQJiOJ3wcy80rIyi+TLpiFED4hid8HVqTnAnDJYEn8Qgjvk8TvAyt25jKqVye6RoX6OhQhhAlJ4veyw0XlpB8ploe2hBA+I4nfy1buNKp5pH5fCOErkvi9bOXOXM7p1pHesRG+DkUIYVKS+L0or6SSTYeOSzWPEMKnJPF70Vc7j6G1VPMIIXxLEr8XrdyZS5+4CAbER/o6FCGEiUni95IT5dWszyrk0iFdUUr5OhwhhIlJt5Be8s3uPKx2LdU8QjRFaziZDQGBEB4DgSG+jshvSeL3khU7c+kWFcrwhChfhyJE22Gzws/rYe8K2LMcivafmhccCWExxkkgPAbCYx3fY43vYdEQHktkSRbkd4egUAgKh6AwCAyDAB9VaBTsM8pyZAtYgo14auM67RUOgfViDgo3yhTXz+MhSuL3grIqK2v25jNvXC8CAqSaR5hc5UnI/Ab2fAn7voLKE0aC7PMLGH8HWIKgvBDKjxvvFUXGe9EBKC+CqpOnbW4MwOZG9hMYemZiDQozThBRPaHnOOg5AeIGnN1JwmaFwz8ayX7Pl1CUZUyPTjTeayqgphJqysFe0/y2EpLh1m9bH4uLJPF7weq9+VRZ7VwqzTiFWR0/5Liq/xIOfm8kwPBYGHQ5DJwGSVMgxMVGD7YaqDhunATKC9mx6XuGDezrSLAVRoKtqQBrg++1ybem3Ihl27vG9kI7OU4C46DneCP5Bjt5zqaqpN7Ja6URT0AQ9DkfJtwJA6ZCp56NxG5tEFfl6fEGhbfs79pKkvi9YEV6LjERwYxNjPZ1KEK4j9ZGEj4j0TqSWnU5HNlkJMdj6cY6cQNg4q9h4GXQYywEWFq+X0sQRHYxXkDhwRoYltLy2Auz4PAPxtX64Q3Grw8AZYGuw6DXhFO/CqISjPsPe750nLy+A1u1UTXT/1Lj5NX3Agjt6CT2QLB0gJAOLS+3G0ni97C84kq+zcjj8mHdCLRIIyrhRkUHjCvXvStIzjsM+7s0qNpopj45MBRsVfUSdWNXyA0SubXyzHna1nyMKgB6TYRLnoQB07xSf+0SpYxY4vrBqOuNaeVFkL3JcTLYAJvfhh9fM+aFxxrVTQAxfWH87Y6T1zgjmbcz7S/idqSyxsbt72zGrjX/c14fX4cj2ju7HY5shr2Oq868Xcb0uIFUB0cbV8LVpVCWf2bCtlY6337DG5GB9U4ckfFn3jytf1Jpal5MknEjtj0Ij4EBlxgvMH7N5O4wTgJHt0OXQUayj+vv2zjdQBK/h2iteeTjdLb+fILXrh/NgHjf/rQTXmS3Q0mO0brj5GHjarFjAkT1MD635DmO6nLYn2bcONy7EsryjKqI3pPg0qdh4FSISWJHWhopKSnNx1S/KsZa6Uj09X4dtKbaxZ9ZgiBhtPHyMy4lfqXUVOAfgAV4Q2v9TIP5vYGFQGegCLhea53tmPcscDnGw2JfA/dqrbXbStBGLVx7kKWbs7n3wv5MHdrN1+EIT6gqMZJ7YRYU7nN8dnyvKW98ncBQx0kgATr2MN6jepz63DHBSMy1N0L3pxpJOqQj9LvIuOLsf5FRt9wSAQHGDUtnNy2FKThN/EopC/AKcDGQDWxUSn2qtd5Vb7HngEVa67eVUhcATwM3KKUmAZOB4Y7lvgd+AaS5rwhtz3f78nnqi11cOiSeey9s/z8LhcPuz4yWHIWZRpIvzT01TwVAp14Q2x8Sz4PYvsbnTr2M5ognj0DxEeMGYe37/jRjG9re+P6iesHom4wbh70nQ6CMzyzcw5Ur/nFAptZ6P4BSajFwFVA/8Q8G/tfxORX42PFZA6FAMKCAIODY2Yfddh0sKOPu97YyIL4Df7tmpLTb94aM5Uxcdxd0eQEGX+WZffzwGqx4AEKjjJYpfS8wbgzG9jfqfKP7GPXcjepjNBFsjK0GSnJPnQxOZhvT+18MXQa3rFpICBe5kvgTgMP1vmcD4xsssx2YiVEddDXQQSkVq7Ver5RKBY5iJP6Xtda7zz7stqmksoZbFm0iQMHrN44hIkRuoXhc9mZY+iuCbNXw4XyY9W8YcrV797Hx30bSH3QFzH7LqPt1F0uQ0d67sTbfQniIK5mpsUuOhnX09wMvK6VuBtYARwCrUqofcA7Qw7Hc10qp87XWa07bgVK3AbcBxMfHk5aW5nIBGiotLT2r9VvLrjUvbqlif4GN348JJeunDWR5OQZfld1XQiuOMnrLA9gCO7J+0AOMPvwGUR/+il0708nvcp5b9tH16CoG7XmJgtix7OxyM/q7tW7ZrruZ7djXZ+bYeG+iAAAWUElEQVSyQyvLr7Vu9gVMBFbW+/4Q8FAzy0cC2Y7Pvwf+VG/en4E/NLe/5ORkfTZSU1PPav3WenbFbt37gc/12+sO+GT/Wvuu7D5RWqD1P0Zp/UxvrfP3GmWvLNH631O1fqyT1j99ePb72LZY60ejtF40XevqirPfngeZ6tg3YOaya32q/MAm7SSf175ceaJoI9BfKdVHKRUMzAU+rb+AUipOKVW7rYcwWvgA/Az8QikVqJQKwrix63dVPZ9tz+GV1CzmjevJDRN6+zoc/1dTAYvnGfXh8xafalcdEgnXfQi9JsF/b4XtH7R+H+n/hY/vgMRzYe57zdTfC9H+OE38WmsrcDewEiNpL9Fa71RK/UUpdaVjsRRgj1JqLxAPPOWYvhTIAnZg3AfYrrX+zL1F8K30Iyf5/dLtjE2M5vErh0pf+55mt8Oy242HamYsMB6rry8kEq5bYrSCWXY7bHu/5fvY/Rl8dIvRb8u1Hxht3IXwIy7dfdRaLweWN5j253qfl2Ik+Ybr2YDbzzLGNiu/pIrbFm0iJjyYf16fTHCgdMngcV//CXZ9Apc8BUOmN75McARcu8T4VfDxnUa3ArWP5TuzZ4VxkzhhtPHrQdq9Cz8kmaqVqq127nxnM0Xl1Sy4cQxxkTJohMf98BqsfxnG3Q4T72p+2eBwoxooKQU+uRu2LHK+/cxVsOQGiB8C1y31eUdaQniKJP5W0Frz6KfpbDp0nL/OGsFQGVzF83Z/BiseNJpUTn3atfbtQWEw732jzf2n98CmN5tedv9qWHwdxA2EG5ZBWCf3xS5EGyOJvxX+88Mh3t9wmLum9OWXI7r7Ohz/d3ijUeeekAwzXm9ZnzJBYcbN2f6XwOe/NdrkN3RoHbw/13gI68aP20+nYkK0kiT+FlqfVcjjn+3ionO68LuLB/o6HP9XmAXvz4EO3YwbrcGtGKgiKBTmvGMMjvHFfbDh9VPzDm+Ad2cbfeTc+AlExLkvdiHaKEn8LfT/VmTQIzqMv88xYXcMVaXGjVWb1Tv7KyuAd2cZg2Zc/9HZJeXAELhmkdHJ2fL74cd/GWOivjMTIjrDTZ9Ch3j3xS5EGyZ9CrTAyYoafso+wd1T+tEh1I2P7bcHJbnw3jVGv+SDp8PMN9zbdUFDNRVG9UtxDtz0mdHp2dkKDIHZb8PS+fDlHyAoAiJije13lCo7YR5yxd8CP+4vxK5hUj+TVQfkZcAbF0FBJiTfDLs+hqW/MjoY8wS7zajTz95k1On3HOe+bQcGG/3tDJ0JkZ2NpC/95AiTkSv+FliXVUhoUACjepmoxceB74zWLkGhMH85dB9ptHxZ+RB8eDPMetO93QXbbfDlA5DxOUx9BgZf6XydlrIEwayFxsNgAXLtI8xH/tW3wNrMAsYmxhASaJKRin5aAv+5Gjp0hVtWGUkfjMGypz1rJOcPbwJrlXv2V1unv/F1mHg3TLjTPdttiiR9YVLyL99FecWV7MsrZbIZqnm0hjXPGf3d9JoA/7PSGFCkvvG3w2XPGUMCfnDD2Sf/n3+A186Dg2vhl/8wBucWQniEJH4XrcsqBGByXz9P/DYrfHYvfPsEDJtttKZpapi/cbfCFX+HfSvhg+uhxoUBvRvSGta9BG9eZtx8veVr4z6C9HkkhMdIHb+L1mYWEBUWxODuHX0diudUlRj91GR+Def9DqY84rw6ZMyvjGEHP7sXPrgO5rzrek+WFSfg41/Dni+MJ3Knv2qMcCWE8ChJ/C7QWrMuq5CJSbFY/LXtfkmu8SDTsZ1wxQswZr7r6ybfbCT/T39jNMGc977zHi1ztsKSm4whBy992qjPl6t8IbxCqnpccKiwnCMnKpjcL9bXoXhG3m6juWZhltGxWUuSfq3RN8JVrxgDiL83B6rLG19Oa6PbhH9fAnYrzP/SuFksSV8Ir5ErfheszSoA/LT9/oE1sPj605trttao64wr/4/vNB72uvaD07s1rio1+svZ8SH0uwiuXmA8QCWE8CpJ/C5Yl1lI146hJMV5qW92azVkfQPKYlSZBIUbibnus+Pd2ZOzNivUlIO10nivqXC8VxqfC/bA149CTBJcv/TMljutMXKe0YnastvhXUfyD4k0flUsuREKM+GCR+Dc30lzSiF8RBK/E3a7Zl1WAVMGdfHe6FrrXjRa1TijLPVOBGGMr6yCTdqR2MvB7sKTtb3PhbnvNN1ypzWGX2Nc+f/3VuO+wYg5sOIh4+r/ho8h6Rfu25cQosUk8TuxO7eY4+U13mvGWVUK61+BpCnGlXHdFXq9K/bTruBPvYpzjxLWI/H0XwUNfykEhp36HhwOnQe1rJtjVw2bZdTbf3Qr/LzOGApx1kLjYTAhhE9J4ndiXaaj/b636vc3LYSKIpjyR+gxpkWr7k5LIz4lxTNxtcbQmRDSEfIzYPydYJF/bkK0BfI/0Ym1WQUkdY6ga5SLbdPPRk2F8TBTUgr0HOv5/XlD/4uNlxCizZC7a82ottrZcKDIe9U8WxZBWR6c/wfv7E8IYUqS+JuxPfsE5dU277Tft1bB9y9Ar0mQONnz+xNCmJYk/maszSxAKZiQ5IXEv+09KMmBX/ze8/sSQpiaJP5mrMssZGj3KDqFu7G/+cbYauD7v0HCGKM1jxBCeJAk/iaUV1vZevg4k7xRzfPTEjjxM5z/e+m6QAjhcZL4m7DhQBE1Nu35G7t2G3z3PHQdDgMu9ey+hBACSfxNWpdVSLAlgLGJMZ7d0c5lUJQlV/tCCK+RxN+EtZkFjOrVibBgDw6zaLcbI111Psfoj14IIbxAEn8jjpdVs+toseef1s34HPJ3w/n3S4dlQgivkWzTiPX7C9Eaz7bf1xrW/BVi+sKQqz23HyGEaEASfyPWZhYQEWxheI9OntvJ3pWQ+5MxxKEnOkkTQogmSOJvxLqsQiYkxRJk8dCfp/Zqv1MvowtjIYTwIkn8DeScqOBAQZlnR9vanwpHNsG59zkfTEUIIdxMEn8DazONYRY9Wr+/+q/QMQFGXuu5fQghRBMk8TewLquQuMhgBsZ38MwODq41BiaZfC8EhnhmH0II0QxJ/PVorVmbWcDEvnGeG2ZxzbMQ0QVG3+iZ7QshhBOS+OvJyi8lr6SKyX09VM1zeCPsT4NJ9xjDHwohhA9I4q9nraeHWVzzVwiLgTG/8sz2hRDCBZL461mbWUDPmDB6xoS7f+M522DfSph4F4REun/7QgjhIpcSv1JqqlJqj1IqUyn1YCPzeyulvlFK/aSUSlNK9XBMn6KU2lbvVamUmu7uQriD1WZn/f5Cz/XGueavEBoF4271zPaFEMJFThO/UsoCvAJMAwYD85RSgxss9hywSGs9HPgL8DSA1jpVaz1Saz0SuAAoB75yY/xuk55TTEml1TPt94/tMvrlGX+HkfyFEMKHXLniHwdkaq33a62rgcXAVQ2WGQx84/ic2sh8gFnAl1rr8tYG60m17fcnufvGbsVxSH0KgiONxC+EED4W6MIyCcDhet+zgfENltkOzAT+AVwNdFBKxWqtC+stMxf421nE6lHrsgoY1LUDcZGtbFuvNZzMhtwdRh88uTvg6E9w8mdj/nn3Q7iH+/YXQggXKK118wsoNRu4VGt9i+P7DcA4rfU99ZbpDrwM9AHWYJwEhmitTzrmdwN+ArprrWsa2cdtwG0A8fHxyYsXL251gUpLS4mMbNnN02qb5q5vypnSM5Brz3Ge+JXdRljFESJL99Oh5ACRpfuJLD1AkLUEAI2iIqw7JR2SKI3sQ2lkX45HDwfl2XvprSm7vzBz2cHc5Tdz2eFU+adMmbJZaz3GlXVcueLPBnrW+94DyKm/gNY6B5gBoJSKBGbWJn2Ha4BljSV9x/oLgAUAY8aM0SkpKa7E3qi0tDRauv66zAJq7D8yJ2UkKefEN79w6tOw7h9grTC+W0KgyzmQdLUxfGK3EagugwkPiSQccLI1t2pN2f2FmcsO5i6/mcsOrSu/K4l/I9BfKdUHOIJRZXNaJzNKqTigSGttBx4CFjbYxjzH9DZpbVYBlgDFuD5OqmJsNfDja9B1mNEWv9twiBsgHa0JIdoVp3UPWmsrcDewEtgNLNFa71RK/UUpdaVjsRRgj1JqL8ZF7lO16yulEjF+Max2a+RutDazkBE9ougQ6iSBH1oHlSdg8m9g5DyIHyJJXwjR7rhyxY/WejmwvMG0P9f7vBRY2sS6BzFuELdJxZU1/JR9grum9HO+8J7lEBgKfS/wfGBCCOEhpn9y98f9Rdg1THL24JbWkLEckqZAcIR3ghNCCA8wfeJfm1lAaFAAo3s7GWbxWLrRNHPQZd4JTAghPMT0iX9dVgFjE2MICXQy7m3GckDBgKleiUsIITzF1Ik/r6SSvcdKnVfzAOz5AnqOg8gung9MCCE8yNSJf31WbTfMTrppOHEYjm6HQZd7ISohhPAsUyf+1Iw8YiKCGdLdScdpe7403gdK4hdCtH+mTfw2uyZtbz4pAztjCXAyzOKeL4wHteJcaPIphBBtnGkT/9afj3OivIYLBjmps684AQe/h4HSmkcI4R9Mm/i/zcjDEqA4r3/n5hfMXAV2q9TvCyH8hqkT/9jEaKLCnHS5kPE5RHSBBJc6vRNCiDbPlIn/yIkKMnJLnFfzWKtg3yoYOA0CTPmnEkL4IVNms9SMPADnif/gd1BdItU8Qgi/YtrE3zMmjL6dnQzekLEcgiKgzy+8E5gQQniB6RJ/ZY2NtVkFXDgoHqWaacaptdF+v98FEBTqvQCFEMLDTJf412cVUlljZ4qzap6crVCSIw9tCSH8jukS/7cZeYQFWRjvbLStjC9AWWDApd4JTAghvMRUiV9rzbcZeZzbP47QICe9ce5ZDr0nQbiTE4QQQrQzpkr8e4+VcuREhfPWPEUHIG+XPK0rhPBLpkr83zqacU4Z6CTx73GMMimDrggh/JCpEn9qRh6Du3Wka5STVjoZy6HLEIhO9EpcQgjhTaZJ/CfKq9n883EuPMfJ1X55Efy8Tq72hRB+yzSJf/XefGx27bwZ594VoO1Svy+E8FumSfy1g66M6OFkUPWML6BDd+g+yjuBCSGEl5ki8bs86EpNBWR9a1TzNPdUrxBCtGOmSPwuD7qyfzXUlEs1jxDCr5ki8bs86MqeLyCkIySe553AhBDCB0yT+J0OumK3OTpluwgCg70XnBBCeJnfJ/4cVwddyd4EZfnS974Qwu/5feL/1tVBV/Z8AQFB0P9iL0QlhBC+4/eJv0WDriSeC6FR3glMCCF8xK8Tv8uDrhTsg8J9Us0jhDAFv078Lg+6kvGF8T5wmueDEkIIH/PrxN+iQVe6jYCoHt4JTAghfMhvE7/Lg66U5kH2RhliUQhhGn6b+PfluTjoyp4vAS31+0II0/DbxP/N7hYMutKpF8QP8UJUQgjhe36b+F0adKWqFLJSjWoe6ZRNCGESfpn4XRp0JX8PfPE7sFXJoCtCCFMJ9HUAntDkoCs2q/GE7obX4eB3YAmGMb+C3pN9E6gQQviAXyb+MwZdKc2DzW/D5jeh+AhE9YQLH4XRN0JEnG+DFUIIL3Mp8SulpgL/ACzAG1rrZxrM7w0sBDoDRcD1Wutsx7xewBtAT0ADl2mtD7qrAA3ZtWb13nymDOyMJftH4+p+1ydgr4G+F8Blz8GASyGgmSaeQgjhx5wmfqWUBXgFuBjIBjYqpT7VWu+qt9hzwCKt9dtKqQuAp4EbHPMWAU9prb9WSkUCdreWoIFDheVcWvUtD+V8DwszICQKxt5ivOL6eXLXQgjRLrhyxT8OyNRa7wdQSi0GrgLqJ/7BwP86PqcCHzuWHQwEaq2/BtBal7op7jOVFcB3zzNn51uEB5VjCx4CV7wAw6+B4AiP7VYIIdobVxJ/AnC43vdsYHyDZbYDMzGqg64GOiilYoEBwAml1H+BPsAq4EGtta3+ykqp24DbAOLj40lLS2t5QWpKmbDxbdbqEawIuZRfDh4JpQrWbWzxttqr0tLSVv3t/IGZyw7mLr+Zyw6tLL/WutkXMBujXr/2+w3ASw2W6Q78F9iKkfyzgShgFnASSMI4yXwE/E9z+0tOTtatlZOXr3s/8Ln+1+rMVm+jPUtNTfV1CD5j5rJrbe7ym7nsWp8qP7BJO8nntS9X2vFnY9yYrdUDyGlw8sjRWs/QWo8C/uiYdtKx7lat9X6ttRWjCmh0y05NrvsmqwxwYdAVIYQwMVcS/0agv1Kqj1IqGJgLfFp/AaVUnFKqdlsPYbTwqV03WilVO8r5BZx+b8CtUjPy6BymnA+6IoQQJuY08Tuu1O8GVgK7gSVa651Kqb8opa50LJYC7FFK7QXigacc69qA+4FvlFI7AAW87vZScGrQlRGdLc0PuiKEECbnUjt+rfVyYHmDaX+u93kpsLSJdb8Ghp9FjC4prqjhksFdOSe4yNO7EkKIds1v+urp0jGUF+eN4pxYeTBLCCGa4zeJXwghhGsk8QshhMlI4hdCCJORxC+EECYjiV8IIUxGEr8QQpiMJH4hhDAZSfxCCGEyyujUre1QSuUDh85iE3FAgZvCaW+k7OZl5vKbuexwqvy9tdadnS0MbTDxny2l1Cat9Rhfx+ELUnZzlh3MXX4zlx1aV36p6hFCCJORxC+EECbjj4l/ga8D8CEpu3mZufxmLju0ovx+V8cvhBCief54xS+EEKIZfpP4lVJTlVJ7lFKZSqkHfR2PtymlDiqldiiltimlNvk6Hk9SSi1USuUppdLrTYtRSn2tlNrneI/2ZYye1ET5H1NKHXEc/21Kqct8GaOnKKV6KqVSlVK7lVI7lVL3Oqb7/fFvpuwtPvZ+UdWjlLIAe4GLMQZ43wjM01p7bHzftkYpdRAYo7X2+/bMSqnzgVJgkdZ6qGPas0CR1voZx4k/Wmv9gC/j9JQmyv8YUKq1fs6XsXmaUqob0E1rvUUp1QHYDEwHbsbPj38zZb+GFh57f7niHwdkaq33a62rgcXAVT6OSXiI1noN0HCMzauAtx2f38b4D+GXmii/KWitj2qttzg+l2CMA56ACY5/M2VvMX9J/AnA4Xrfs2nlH6Qd08BXSqnNSqnbfB2MD8RrrY+C8R8E6OLjeHzhbqXUT46qIL+r6mhIKZUIjAJ+xGTHv0HZoYXH3l8Sv2pkWvuvw2qZyVrr0cA04C5HdYAwj38CfYGRwFHged+G41lKqUjgI+C3WutiX8fjTY2UvcXH3l8SfzbQs973HkCOj2LxCa11juM9D1iGUf1lJsccdaC1daF5Po7Hq7TWx7TWNq21HXgdPz7+SqkgjMT3rtb6v47Jpjj+jZW9NcfeXxL/RqC/UqqPUioYmAt86uOYvEYpFeG42YNSKgK4BEhvfi2/8ylwk+PzTcAnPozF62qTnsPV+OnxV0op4N/Abq313+rN8vvj31TZW3Ps/aJVD4CjCdMLgAVYqLV+yscheY1SKgnjKh8gEHjPn8uvlHofSMHolfAY8CjwMbAE6AX8DMzWWvvlDdAmyp+C8VNfAweB22vrvP2JUupc4DtgB2B3TH4Yo67br49/M2WfRwuPvd8kfiGEEK7xl6oeIYQQLpLEL4QQJiOJXwghTEYSvxBCmIwkfiGEMBlJ/EIIYTKS+IUQwmQk8QshhMn8fxXgU1stVemEAAAAAElFTkSuQmCC
" />
</div>

</div>

</div>
</div>

</div>
    

</div>



  </div><a class="u-url" href="/TomKealy.github.io/2021/06/21/Neural-Netwroks-From-Scratch.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/TomKealy.github.io/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/TomKealy.github.io/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/TomKealy.github.io/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/TomKealy.github.io/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/TomKealy.github.io/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
